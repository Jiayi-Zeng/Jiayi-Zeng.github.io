[{"title":"3 Linear Programming","url":"/docs/Prescriptive-Analytics-3-Linear-Programming/","content":"\n# **Linear Programming**\n\nLinear programming (LP, also called linear optimization) is a method to achieve the best outcome (such as maximum profit or lowest cost) in a mathematical model whose requirements are represented by linear relationships.\n\nLinear programming is a technique for the optimization of a linear objective function, subject to linear equality and linear inequality constraints.\n\nLinear programs are problems that can be expressed in canonical form\n\n* Find a vector $x$\n* That maximizes $c^Tx$\n* Subject to $Ax \\leq b$\n* And $x\\geq0$\n\nLinear programming is used in business and industry in\n\n* Production planning, \n* transportation and \n* Routing, and \n* Various types of scheduling\n* Airlines use linear programs to schedule their flights, considering both scheduling aircraft and scheduling staff\n\n# **Optimization Problem**\n\n$$\n\\begin{align}\nMaximize :& 10 x_1+5x_2\\\\\nConstraints :& 5x_1+x_2 \\leq90\\\\\n& x_1+10x_2\\leq300\\\\\n& 4x_1+6x_2 \\leq 125\\\\\n& x_1, x_2 \\geq 0\n\\end{align}\n$$\n\n![image-20230307093829700](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230307093829700.png)\n\n## PuLP Package\n\n```python\nfrom pulp import *\n```\n\n**Define variables**\n\n```python\nlp = LpProblem(\"Bakery_Problem\", LpMaximize)\nx1 = LpVariable(name=\"Bowdoin_log\", lowBound=0)\nx2 = LpVariable(name=\"Chocolate_cake\", lowBound=0)\n```\n\n**Add the objective function**\n\n```python\nlp += 10 * x1 + 5 * x2\nprint(lp.objective)\n```\n\n```\n10*Bowdoin_log + 5*Chocolate_cake\n```\n\n**Add the constraints**\n\n```python\nlp += (5 * x1 + x2 <= 90, \"oven\")\nlp += (x1 + 10* x2 <= 300, \"food_processor\")\nlp += (4 * x1 + 6 * x2 <= 125, \"boiler\")\n```\n\n**Solve the LP**\n\n```python\nstatus = lp.solve()\n```\n\n**Print solution**\n\n```py\nfor var in lp.variables():\n  print(var, \"=\", value(var))\nprint(\"OPT=\", value(lp.objective))\n```\n\n**Output:**\n\n```\nBowdoin_log = 15.961538\nChocolate_cake = 10.192308\nOPT= 210.57691999999997\n```\n\n","tags":["DS"],"categories":["Prescriptive Analytics: Digital Decisioning"]},{"title":"2 Digital Decision Making: Theory, Process, & Practice","url":"/docs/Prescriptive-Analytics-2-Digital-Decision-Making-Theory-Process-and-Practice/","content":"\n# **Introduction**\n\nDecision making can be spontaneous (自发的) or systematic. For personal and social matters, spontaneous and haphazard (胡乱的) decisions typically work fine, but for professional and business-related matters, one should follow a scientific and standardized process. Such a process helps in implementing best practices, promoting the use of data and information (as opposed to opinions and gut feelings 直觉), and hence, creating accurate and consistent outcomes. \n\n# **Concept(s) Overview** \n\n## Overview of the Human Decision-Making Process\n\nSimon (1977) said that such **a systematic process involves three major phases: *intelligence*, *design*, and *choice*. He later added a fourth phase (*implementation*). *Monitoring* can be considered a fifth phase (i.e., a form of feedback);** however, we view monitoring as part of the ‘intelligence’ phase as applied to the ‘implementation’ phase as we’ll see below. Simon's model is widely accepted as the most concise (简洁）, and yet most complete, characterization (表征) of rational decision making. A conceptual picture of this decision-making process is shown in Figure 1.\n\n![The decision making/modeling process starts with real world issues or problems. These issues are considered in the first 'intelligence' phase where some kind of problem statement is produced. This statement leads to the 'design' phase or solution phase. Alternatives are considered and eventually a decision is made in the 'choice' stage leading to the 'implementation' phase. Once the decision is implemented the cycle moves back to the real world.](https://lh4.googleusercontent.com/4e-dWnGfX1r9mIs7UcuwLkWlXHdhuP0gPvEnVUpnpyGN0eidcmgn5_Z2_tvOQGAmMX1UbNSn-5eYBoU4b1DPbAUBzEGngOH-hUq1Y8oODFQEjpUn4ymzE-s-bEi4M67yM1oQ1rlz_eQhJr-YVT-vWQ)\n\n*Figure 1: The decision-making/modeling process*\n\n### Intelligence\n\nThe *intelligence* phase in the decision-making process involves **scanning the environment**, either intermittently or continuously (间接性或持续性). It includes several activities aimed at identifying problem situations or opportunities. As mentioned above, it may also include monitoring the results of the implementation phase of a decision-making process.\n\n### Design\n\nThe *design* phase involves finding or developing and analyzing possible courses of action. These include **understanding the problem and testing solutions for feasibility.** A model of the decision-making problem is constructed, tested, and validated. Modeling involves conceptualizing (概念化) a problem and abstracting it into quantitative (定量) and/or qualitative (定性) form. For a mathematical model, the variables are identified, and their mutual (相互的) relationships are established.\n\n### Choice\n\nChoice is the critical act of decision making. The *choice* phase is the one in which the actual decision and the commitment to follow a certain course of action are made. The boundary between the design and choice phases is often unclear because certain activities can be performed during both phases and because the decision maker can return frequently from choice activities to design activities (e.g., generate new alternatives while evaluating existing ones). The choice phase includes **searching for, evaluating, and recommending an appropriate solution to a model**. A solution to a model is a specific set of values for the decision variables in a selected alternative.\n\n### Implementation\n\n*Implementation* is the actual deployment of the choice. The implementation of a proposed solution to a problem is the initiation of a new order of things or the introduction of change. Change must be managed, e.g., user expectations must be managed as part of change management. The definition of implementation is somewhat complicated because implementation is a long and involved process with vague boundaries. Simply put, the implementation phase involves putting a recommended solution to work. Many generic implementation issues, such as resistance to change, degree of support from top management, and the need for user training are important in dealing with managerial decisions.\n\n## CRISP-DM: A Standards Process for Analytics\n\nThe *CRISP-DM* (Cross-Industry Standard Process for Data Mining) methodology consists of a cyclical sequence of phases commonly used by data mining experts for traditional business intelligence (BI) data mining and data strategy development. Figure 2 summarizes this cycle— which is dynamic, features bidirectional movement between the six phases, and entails multiple iterations. In the diagram below, please note that:\n\n- The iterative CRISP-DM process is shown in the outer circle.\n- The most significant dependencies between phases are shown.\n- Subsequent phases depend on results from the preceding phases.\n\n*![A diagram showing the relationship between the 6 different phases of CRISP-DM. It illustrates the recursive nature of a data mining project.](https://lh3.googleusercontent.com/eW3nJAycOY0La88ygPNRaguPEKFv47vJiXq-xkx57F5xV1AP-nLuqrxbVWfCVcMlqXibc4a0bEusCe0GiVv0K6ZXmb7d-C4STlUuzQiLrKcH8Z86PGuQoLNp2sTioAzuqbGFyvemP-rbqLe3BAHYhQ)*\n\n*Figure 2: The CRISP-DM process. (Source: Kenneth Jensen,* [*CC BY-SA 3.0*](https://creativecommons.org/licenses/by-sa/3.0)*, via Wikimedia Commons)* \n\n### The Six-Step Process to Guide Analytics Thinking and Problem Solving\n\nIn the [CRISP-DM 1.0 data mining guide](https://the-modeling-agency.com/crisp-dm.pdf), Chapman et al. (2000) lay out specific activities that can guide analytics thinking for each phase, as highlighted below:\n\n#### **Phase 1: Business/Research Understanding**\n\n- Define project requirements and objectives\n- Translate objectives into a data mining problem definition\n- Prepare a preliminary strategy to meet the objectives\n\n#### **Phase 2: Data Understanding** \n\n- Collect data\n- Perform exploratory data analysis (EDA)\n- Assess data quality\n- Select interesting subsets (*optional*)\n\n#### **Phase 3: Data Preparation** \n\n- Prepare for modeling in subsequent phases\n- Select cases and variables appropriate for analysis\n- Cleanse and prepare data so it is ready for modeling tools\n- Perform transformation of certain variables, if needed\n\n#### **Phase 4: Modeling** \n\n- Select and apply one or more modeling techniques\n- Calibrate model settings to optimize results\n- Prepare additional data, if required, to support a particular technique\n\n#### **Phase 5: Evaluation** \n\n- Evaluate one or more models for effectiveness\n- Determine whether defined objectives are achieved\n- Establish whether any important facet of the problem has not been sufficiently accounted for\n- Make a decision regarding data mining results before deploying to the field\n\n#### **Phase 6: Deployment**\n\n- Make use of models created\n- Generate the report (simple deployment example)\n- Implement a parallel data mining effort in another department (complex deployment example)\n\n*Note*: In some businesses, the customer often carries out the deployment based on the model delivered by the analytics team.\n\n# **Mathematics for Prescriptive Analytics**\n1. Linear Programming\n2. Non-Linear Programming\n3. Genetic Algorithm\n4. Simulation\n\n","tags":["DS"],"categories":["Prescriptive Analytics: Digital Decisioning"]},{"title":"1 Overview of Prescriptive Analytics","url":"/docs/Prescriptive-Analytics-1-Overview-of-Prescriptive-Analytics/","content":"\n# **Introduction**\n\n*Prescriptive analytics* is a part of the business analytics continuum and is also the closest to the decision-making phase. Business analytics is often characterized by three consecutive levels/echelons, i.e., descriptive analytics, predictive analytics, and prescriptive analytics. Prescriptive analytics is the highest level in business analytics and the main focus of this course.\n\nIn general terms, business analytics is the art and science of identifying or discovering novel patterns and insights from large volumes and varieties of data utilizing sophisticated machine learning, mathematical, and statistical models to support more accurate and timely managerial decision making (Delen, 2019). Therefore, business analytics is widely perceived as synonymous with managerial decision making and business problem-solving. Figure 1 shows the process of creating information and knowledge through a systematic and scientific transformation of data which leads to making better decisions and, ultimately, achieving “wisdom.”\n\n![Shows the process of converting data into knowledge and wisdom. Data comes in and is processed or summarized. This processing/summarization makes it \"information.\" As this information is determined to be relevant and actionable, it becomes \"knowledge.\" This knowledge will ultimately lead to better decision making and wisdom.](https://lh6.googleusercontent.com/J4TGR2WkAWGORqjleLPp_dtKqO4s-DLAG0Rurrriv9QQO8PupU76MhfVS4B-WTxYq_SlXpN8QQnL0_a7R0Kd2PUNiQ7JgSJgo5WSAnIu4IR-WS3eH3aIzofQBQ8lbpKsBKRGpGCn3V04qBQL4IvzUA)\n\n*Figure 1: The process of converting data into knowledge and wisdom*\n\nAs seen in this figure, various data sources (both structured and unstructured) are converted into mathematical representations (i.e., knowledge models) through a scientific process we now call business analytics.\n\n# **Key Terms**\n\nBefore we continue, several key terms will provide a foundation for our discussion: \n\n- **Business Analytics**\n  - Descriptive analytics\n  - Predictive analytics \n  - Prescriptive analytics\n- **Nature of Data**\n  - Structured data (numeric and non-numeric)\n  - Unstructured data (textual and multimedia data) \n- **Operations Research**\n  - Optimization\n  - Simulation\n  - Heuristics\n  - Multi-criteria decision making\n\n# **Analytics and a Simple Analytics Taxonomy**\n\nThe three commonly-used types/echelons in business analytics (i.e., descriptive, predictive, and prescriptive analytics) provide a structured and comprehensive depiction of the analytics maturity process for organizations (Figure 2). Some sources include *diagnostic analytics* as a fourth layer/echelon between descriptive and predictive, but most often, this layer of analytics is included as an extension of descriptive analytics. A short description of these analytics echelons, and the questions they are aimed at answering, are given below:\n\n- **Descriptive**: *What happened?* Helps to uncover valuable insight into the data being analyzed via the use of on-demand reporting and data visualization.\n- **Diagnostic**: *Why did it happen?* Helps to identify and understand the causal relationships and related patterns in the data.\n- **Predictive**: *What is likely to happen in the future?* Helps to forecast the future behavior of people and markets using statistical and machine learning methods.\n- **Prescriptive**: *What should I do about it?* Now that I know what happened in the past and what might happen in the future, what decisions should I make? Uses operations research methods (optimization, simulation, heuristics, and multi-criteria decision modeling) to provide guidance and understanding on decision alternatives.\n\n*![An x-y axis chart which depicts the progressive nature of the four types of analytics: descriptive, diagnostic, predictive, and prescriptive. The X-axis shows the progression of computational sophistication, and the Y-axis shows the progression of value proposition. Descriptive, diagnostic, and predictive analytics are information and insight focused, whereas prescriptive analytics is decision focused. ](https://lh3.googleusercontent.com/HcCjvjdeHHqtzl3PwpfUWEJV7VTNwmCT8KTTvCPfwHYV4uFayJVlEh9nRNPRS7l6s3DuNwI1RJHklQytT7QFybPyALd6RQOMbWb3ACArfwW1VGZiQ599CCUICuNVKShiBeVXbK7ovs7TDI4QYPG_lA)*\n\n*Figure 2: The progressive nature of the four types of analytics*\n\n# **The Reason behind Analytics Popularity**\n\nAccording to Delen (2021), the reasons behind the popularity of business analytics can be grouped into three main categories, as follows:\n\n- **Need for better decisions**: Conducting business is no longer “as usual.” Competition has progressively transformed from local to regional, to national, and now, to global. The protections created by tariffs and logistics costs (that sheltered companies into their geographic regions) are no longer as prominent as before. In addition to increased global competition, and perhaps partially because of it, customers have also become more demanding, i.e., asking for the highest quality of products and services, offered at the lowest prices, and delivered quickly. Therefore, data-driven, accurate, optimized, and timely decisions are more critical now than ever before. Analytics offers help with these needs.\n- **Availability and affordability of enablers**: Thanks to recent technological advances and the affordability of software and hardware, organizations are collecting tremendous amounts of data. Internet of Things (IoT)-based data collection systems (which are based on various sensor and RFID [Radio-frequency Identification] technologies, internet, and social media sources) have significantly increased the quantity and quality of organizational data. In addition to the ownership model, cloud-based solutions and software (or hardware) “as-a-service” business models allow small to medium-sized businesses to acquire (i.e., lease and pay only for what they use) analytics capabilities though they have limited financial resources.\n- **Culture change**: There has been a shift from traditional (intuition or “gut-feeling”) decision making to contemporary (fact or evidence-based) decision making at the organizational level. Most successful organizations have made a conscious effort to shift to data- or evidence-driven business practices. Because of the availability of data and supporting information systems infrastructure, such a paradigm shift is taking place faster than many thought possible. As the new generation of quantitatively savvy managers replaces baby boomers, this evidence-based managerial paradigm is expected to accelerate.\n\n# **Prescriptive Analytics and Optimal Decision Making: The Final Frontier**\n\nPrescriptive analytics is the highest level or echelon in the analytics hierarchy (Figure 2). It is where the best alternative among many courses of action (which are usually created/identified by descriptive and predictive analytics) are determined using sophisticated mathematical models, often labeled as “operations research” techniques. Therefore, this type of analytics aims to answer the question, “What should I do?” Prescriptive analytics uses well-established operations research techniques like optimization, simulation, and heuristics-based decision modeling. Even though prescriptive analytics is at the top of the analytics hierarchy, its methods are not new. Most of the optimization, simulation, and heuristic techniques that collectively constitute prescriptive analytics were developed during (and right after) World War II in the 1940s and 1950s when there was a dire need to do “the most and best” with limited available resources. Since then, some businesses have used some very specific problem types, including yield/revenue management, transportation modeling, scheduling, etc. The popularity of business analytics and the new taxonomy of analytics have made them popular again, opening their use to a wide array of organizations for various business problems and situations (Delen, 2019).\n\n## Value of Prescriptive Analytics in Business Operations\n\nThe value proposition of prescriptive analytics is obvious: supporting optimal decision making. Descriptive and predictive analytics layers focus on information creation, domain understanding, and problem definition. Prescriptive analytics focuses on problem-solving via optimal, timely decision making.\n\n## Suitable Business Decisions for Prescriptive Analytics\n\nToday, most businesses use business analytics to search for novel patterns and correlations, to identify and formulate business problems worthy of solving, and to support optimal and timely managerial decision making. Below are some examples that highlight applications of business analytics across various industries.\n\n- **Retail**: Retail companies, both online and brick-and-mortar, analyze their customer purchase data to optimize their product offerings, prices, and promotions. The goal is to maximize revenue and profitability while enhancing and maintaining high levels of customer satisfaction and loyalty.\n- **Finance**: Financial institutions use analytics to identify and prevent fraudulent transactions and specifically use *predictive* and *prescriptive* analytics to evaluate a person’s financial behavior and assign them a risk level for credit card approval.\n- **Healthcare**: The healthcare industry has identified quite a few ways to use analytics to optimize the allocation of their resources: in the fulfillment of its mission to improve the quality of life for chronically ill patients, provide personalized patient treatment, reduce the rate of hospital-acquired infections, assess and identify treatment risk factors more rapidly, and many more applications. In addition, large medical centers have used free public health data to create visualizations that can help speed up identifying and analyzing healthcare information and tracking the spread of diseases.\n- **Insurance**: The insurance industry has numerous opportunities to expand its use of analytics. For instance, insurance carriers can increase the personalization of services and optimal pricing, which allows for the identification of more specific and actionable consumer segmentation. Additionally, analytics can provide opportunities for better fraud detection and greater industry transparency.\n- **Transportation**: The software *Dataiku DSS* (Data Science Studio), when applied to freight, sea freight, road freight, and passenger transport, uses predictive and prescriptive analytics with sensor data to determine optimal maintenance schedules.\n- **Communications, Media, and Entertainment**: Entertainment companies such as YouTube, Amazon, and Netflix analyze their users’ browsing habits and patterns, which allows them to create or curate content tailored for specific target audiences optimally.","tags":["DS"],"categories":["Prescriptive Analytics: Digital Decisioning"]},{"title":"Understanding Namespace & Scope","url":"/docs/Immediate-to-Python-4-Understanding-Namespace-and-Scope/","content":"\n# **Namespaces**\n\n## The Global Namespace\n\nt is a collection of names that map to objects (variables, functions, etc...) that have been created in our code. Anytime we run a python script (or a jupyter notebook), a global namespace is created. We can print the names that are in the global namespace with the `dir()` function.\n\n```python\n# use the dir() function to return the list of names in the namespace and use print() to print that list.\nprint(dir())\n```\n**output:**\n\n```\n['In', 'Out', '_', '__', '___', '__builtin__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__', '__vsc_ipynb_file__', '_dh', '_i', '_i1', '_ih', '_ii', '_iii', '_oh', 'exit', 'get_ipython', 'quit']\n```\n\n**Adding our own names to the global namespace**\n\nWe can add our own names very easily, all we need to do is create a new object.  Everything in python is an object, so if we create a variable or a function or a instance of a class, it will be added to the global namespace. \n\n```python\nmy_var = \"Will\"\nprint(dir())\n```\n**output:**\n\n```\n['In', 'Out', '_', '__', '___', '__builtin__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__', '_dh', '_i', '_i1', '_i2', '_ih', '_ii', '_iii', '_oh', 'exit', 'get_ipython', 'my_var', 'quit']\n```\n\n**Names in a namespace are unique - you can not have duplicates**\n\nIf we assign the variable name `my_var` another value, we will not be able to use `my_var` to access the previous value. In other words, you can not have two variables named `my_var` in the same namespace. You can not have two objects with the same name in the same namespace. \n\n**Let's now create a function and see that added to the global namespace**\n\n```python\ndef reverse_string(string):\n    '''Return the reverse of a string\n    '''\n    reverse_string = string[::-1]\n    return reverse_string\n\nprint(dir())\n```\n\n**output:** \n\n```\n['In', 'Out', '_', '__', '___', '__builtin__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__', '_dh', '_i', '_i1', '_i2', '_i3', '_ih', '_ii', '_iii', '_oh', 'exit', 'get_ipython', 'my_var', 'quit', 'reverse_string']\n```\n\n```python\nmy_string = 'Hello'\nmy_string_reversed = reverse_string(my_string)\nprint(my_string_reversed)\nprint(dir())\n```\n\n**output:**\n\n```\nolleH\n['In', 'Out', '_', '__', '___', '__builtin__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__', '_dh', '_i', '_i1', '_i2', '_i3', '_i4', '_i5', '_ih', '_ii', '_iii', '_oh', 'exit', 'get_ipython', 'my_string', 'my_string_reversed', 'my_var', 'quit', 'reverse_string']\n```\n\n**Let's import a package and see that added to our namespace**\n\n```python\nimport pandas\nprint(dir())\n```\n\n**output:**\n\n```\n['In', 'Out', '_', '__', '___', '__builtin__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__', '_dh', '_i', '_i1', '_i2', '_i3', '_i4', '_i5', '_i6', '_i7', '_ih', '_ii', '_iii', '_oh', 'exit', 'get_ipython', 'my_string', 'my_string_reversed', 'my_var', 'pandas', 'quit', 'reverse_string']\n```\n\n**Each package/module also has its own global namespace.**\n\nThis is important to understand. Each python module (a module is simply a .py file that contains functions, classes, and variables) has their own global namespace.\n\n## The Local Namespaces\n\nA local namespace and a global namespace can exist at the same time.\n\nThe code works because any variable created inside the function is not created in the global namespace, but it is created in what's called a ***\\*local namespace\\****. This namespace is local to the function (each function has its own local namespace).\n\n```python\nmy_text = \"I am outside the function and in the notebook's GLOBAL namespace!\"\n\ndef my_func():\n    my_text = \"I am inside the function and in the function's LOCAL namespace!\"\n    print(my_text)\n    return\n\n# Now run the function\nmy_func()\n# Print my_text\nprint(my_text)\n```\n\n**Output:**\n\n```\nI am inside the function and in the function's LOCAL namespace!\nI am outside the function and in the notebook's GLOBAL namespace!\n```\n\n**Let's now add `dir()` to the body of the function so that we can print the names in the function's local namespace**\n\n```python\nmy_text = 'I am outside the function in the notebooks GLOBAL namespace!'\nprint(\"These are the names in the global namespace:\")\nprint(dir(), '\\n')\n\ndef my_func_b():\n    my_text = \"I am inside the function in the function's LOCAL namespace!\"\n    print(\"These are the names in the function's local namespace:\")\n    print(dir(), '\\n')\n    print(my_text)\n    return\n\n# Now run the function\nmy_func_b()\n# Print my_text\nprint(my_text)\n```\n\n```\nThese are the names in the global namespace:\n['In', 'Out', '_', '__', '___', '__builtin__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__', '_dh', '_i', '_i1', '_i2', '_ih', '_ii', '_iii', '_oh', 'exit', 'get_ipython', 'my_func', 'my_text', 'quit'] \n\nThese are the names in the function's local namespace:\n['my_text'] \n\nI am inside the function in the function's LOCAL namespace!\nI am outside the function in the notebooks GLOBAL namespace!\n```\n\n**There can be multiple local namespaces**\n\nEach function has its own local namespace.\n\n```python\nmy_text = 'I am outside the function in the notebooks GLOBAL namespace!'\nprint(\"These are the names in the global namespace:\")\nprint(dir(), '\\n')\n\ndef my_func_1(my_text_1):\n    print(\"These are the names in the 1st function's local namespace:\")\n    print(dir(), '\\n')\n    print(my_text_1, '\\n')\n    return\n\ndef my_func_2(my_text_2):\n    print(\"These are the names in the 2nd function's local namespace:\")\n    print(dir(), '\\n')\n    print(my_text_2, '\\n')\n    return\n\n# Now run the function\nmy_func_1(\"Hello!\")\n\nmy_func_2(\"Goodbye!\")\n# Print my_text\nprint(my_text)\n```\n\n```\nThese are the names in the global namespace:\n['In', 'Out', '_', '__', '___', '__builtin__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__', '_dh', '_i', '_i1', '_i2', '_i3', '_i4', '_ih', '_ii', '_iii', '_oh', 'exit', 'get_ipython', 'my_func', 'my_func_1', 'my_func_2', 'my_func_b', 'my_text', 'quit'] \n\nThese are the names in the 1st function's local namespace:\n['my_text_1'] \n\nHello! \n\nThese are the names in the 2nd function's local namespace:\n['my_text_2'] \n\nGoodbye! \n\nI am outside the function in the notebooks GLOBAL namespace!\n```\n\n","tags":["Python"],"categories":["Immediate to Python"]},{"title":"10 Clustering","url":"/docs/Predicted-Analytics-8-Clustering/","content":"\n# **Unsupervised Learning**\n\n* Clustering\n  * k-means clustering\n  * Hierarchical Clustering\n* Principal Component Analysis\n\n**Unsupervised Learning in Predictive Analytics**\n\nUnsupervised learning is part of Machine Learning family of methods\n\nAlthough, it may not be as popular as supervised learning, it has a significant footprint in Analytics\n\n**The Challenge of Unsupervised Learning**\n\nModel assessment\n\n* We cannot tell if the model we have built is good\n* Because we do not have the test data with known response variable information\n* We cannot do cross validation\n\n# **Clustering**\n\nCategorize objects into groups (or clusters) so that \n\n* Objects in each group are similar \n* Objects in each group are different from objects in other groups\n\n**Clustering Applications**\n\n* Decrease the size and complexity of problems for other data mining methods\n* Identify outliers in a specific domain\n  * Customer Segmentation\n\n## Clustering Definition\n\n* Suppose ‘n’ observations\n* Let $𝐶1,𝐶2,...,𝐶𝑘$ are sets containing\n* the indices of the observations in each other\n  * $𝐶1 ∪ 𝐶2 ∪ 𝐶3 ...∪ 𝐶𝑘 = 1,...,𝑛$ . Each observation belongs to at least one of the ‘k’ clusters.\n  * $𝐶𝑘 ∩ 𝐶𝑘′ = 0$ for all $𝑘≠𝑘$′. Clusters are non-overlapping: no observation belongs to more than one cluster.\n\n## Compute the Distance between Clusters\n\n![image-20230303111030606](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230303111030606.png)\n\n## Clustering Assessment\n\nA good cluster should have the within-cluster-variation is as small as possible.\n\n* within - cluster - variation = $W(C_K)$\n* Good cluster: $minimize(\\sum_1^kW(C_k))$$\\hat{i}$  \n* $W(C_K) = \\frac{1}{|C_k|} \\sum_{i,\\hat{i}}\\sum_{j=1}^p(x_{ij}-x_{i^ij})^2$\n\n# **K-Means**\n\n## K-means Algorithm \n\n* Given a K, find a partition of K cluster\n* Each cluster is represented by the center of the cluster and the algorithm converges to stable centers of clusters.\n* the K-means algorithm is carried out in three steps:\n  ![image-20230303095557785](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230303095557785.png)\n\n## Example \n\n![image-20230303095421868](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230303095421868.png)\n\n![image-20230303095038636](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230303095038636.png)\n\n![image-20230303095058835](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230303095058835.png)\n\n![image-20230303095357240](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230303095357240.png)\n\n![image-20230303095344768](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230303095344768.png)\n\n![image-20230303095333733](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230303095333733.png)\n\n![image-20230303095510057](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230303095510057.png)\n\n**Difference between kNN Classifier (k Nearest Neighbor) & k-Means Clustering**\n\n![image-20230303092358992](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230303092358992.png)\n\n## Example Code\n\n### Load the Libraries\n\n```python\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn.cluster import KMeans\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use(\"ggplot\") # grammar of graphic\n```\n\n### Read Data and Show the Scatterplot\n\n```python\nX = np.array([[1, 1],\n            [2, 1],\n            [4, 5],\n            [5, 4]])\n\nprint(X)\nplt.scatter(X[:,0], X[:,1], s=10, linewidth=5)\nplt.show()\n```\n\n**Output:**\n\n```\n[[1 1]\n [2 1]\n [4 5]\n [5 4]]\n```\n\n![download](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/download.png)\n\n### Build Clusters\n\n```python\nclf = KMeans(n_clusters=2)\nclf.fit(X)\n\ncentroids = clf.cluster_centers_\nlabels = clf.labels_\nprint(centroids)\nprint(\"labels=\", labels)\n```\n\n**Output:**\n\n```\n[[1.5 1. ]\n [4.5 4.5]]\nlabels= [0 0 1 1]\n```\n\n### Plot the Clusters\n\n```python\ncolors = [\"g.\",\"r.\",\"c.\",\"b.\",\"k.\",\"g.\"]\n\nfor i in range(len(X)):\n    plt.plot(X[i][0], X[i][1], colors[labels[i]], markersize = 10)\n    \nplt.scatter(centroids[:,0], centroids[:,1], marker='x', s=150, linewidth=5)\nplt.show()\n```\n\n![download (1)](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/download%20(1).png)\n\n## Parameter: `nstart`\n\nClustering algorithm will give slightly different results if we start with different initial values\n\nThe `kmeans` algorithm implemented in R has a parameter `nstart` which indicates multiple random initial assignments\n\nSuppose `nstart` = n\n\n* Algorithm builds ‘n’ clusters and only the best cluster is reported\n* Best cluster is the one which has minimum within-\n  cluster-variation\n\n**Disadvantage of K-means clustering** \n\n* You have specify the number of clusters\n\n# Hierarchical Clustering\n\nHierarchical clustering solves this problem – no specification of number of clusters\n\nHierarchical structure also creates a hierarchical structure of data called **Dendrogram**\n\n## Strategy to build Hierarchical Clustering\nBottom-up approach\n\n* Agglomerative clustering\n\nCompute the Euclidean distance between data points\n\n* Shortest distance observations should be in a \n  single cluster\n* Next we compute the distance between cluster \n  that we have created and the next point closets to \n  it\n* Include that point in that cluster\n\n## Hierarchical Clustering Algorithm\n![image-20230303111410608](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230303111410608.png)\n\n## Example code\n\n[Mall_Customers.csv](https://uciunex.instructure.com/courses/16600/files/2324830?wrap=1)\n\n### Load the Libraries\n\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n```\n\n### Read Data\n\n```python\n'''\nSince we are performing clustering\nwe only need X variable\n\nClustering is a unsupervised method, \nthat's why we do NOT need the response variable or the 'y' variable\n'''\ndataset = pd.read_csv(\"Mall_Customers.csv\")\nX = dataset.iloc[:,[3,4]].values\n```\n\n### Plot the Dendrogram\n\n```python\n'''\nPlot the dendrogram\nThe plot will determine how many clusters we should need\n'''\nimport scipy.cluster.hierarchy as sch\n\ndendrogram = sch.dendrogram(sch.linkage(X,method='ward'))\nplt.title('Dendrogram')\nplt.xlabel('Customers')\nplt.ylabel('Eucledian Distance')\n\nplt.show()\n```\n\n![download3](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/download3.png)\n\n```python\n'''\nWe can have 3 clusters as standard\nOr we can have 5 clusters\nFind the longest line which is not crossed by horizontal line\n\nThis shows total number of clusters = 5\n'''\n \nfrom sklearn.cluster import AgglomerativeClustering\nhc = AgglomerativeClustering(n_clusters=5, affinity='euclidean',linkage='average')\n\ny_hc = hc.fit_predict(X)\n```\n\n```python\nplt.scatter(X[y_hc==0,0],X[y_hc==0,1],s=50,c='red',label='Cluster1')\nplt.scatter(X[y_hc==1,0],X[y_hc==1,1],s=50,c='blue',label='Cluster2')\nplt.scatter(X[y_hc==2,0],X[y_hc==2,1],s=50,c='green',label='Cluster3')\nplt.scatter(X[y_hc==3,0],X[y_hc==3,1],s=50,c='cyan',label='Cluster4')\nplt.scatter(X[y_hc==4,0],X[y_hc==4,1],s=50,c='magenta',label='Cluster5')\n\nplt.title('Cluster of the Customers')\nplt.xlabel('Annual Income (K$)')\nplt.ylabel('Spending Score (1-100)')\nplt.legend()\n```\n\n![download4](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/download4.png)\n","tags":["Data Mining","DS"],"categories":["Predicted Analytics: Tools & Techniques"]},{"title":"9 Data Prep: Discretization & 1Hot Encoding","url":"/docs/Predicted-Analytics-7-DataPrep-Discretization+OneHotEncoding/","content":"\n![image-20230307140225758](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230307140225758.png)\n\n# **Discretization**\n\nConverting Numeric Data into Categorical Data\n\n**How to determine the boundaries between classes?**\n\n* Natural boundaries\n\n![image-20230307140432485](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230307140432485.png)\n\n* Equi-width ranges\n\n![image-20230307140445235](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230307140445235.png)\n\n* Equi-log ranges\n\n![image-20230307140458116](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230307140458116.png)\n\n* Equi-depth ranges\n  ​\t\tRange [a,b] is chosen\n  * Each range has an equal number of records\n  * First sort the data\n    * Select the boundaries from the sorted \n      data such that each range contains equal number of observations\n\n\n## Example\n\n**Read Datafile**\n\n```python\nimport pandas as pd\ndf = pd.read_csv('Lung Capacity.csv')\n```\n\n**Discretize Height into 6 Categories : Width Size is Different**\n\n```python\nbins = [0, 50, 55, 60, 65, 70, 100]\ngroup_names = ['A', 'B', 'C', 'D', 'E', 'F']\n\nc1 = pd.cut(df['height'], bins, labels=group_names)\n```\n\n# One Hot Encoding\n\nLabel Encoder\n\n* Converts Categorical variable into Numerical values\n* Starting from 0,1,2,...\n* Code is assigned by alphabetical order\n\n## Example\n\n**Load the libraries**\n\n```python\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn import datasets\n```\n\n**Read the Dataset**\n\n```python\niris = datasets.load_iris()\nprint(type(iris))\nfeatures = iris[\"data\"]\nprint(type(features))\nprint(features[:5,:])\n\n#############################################\nprint('---------------------------')\nlabels = iris[\"target\"]\nprint(type(labels))\nprint(labels)\n```\n\n**Encode the response variable (labels - species) data into one-hot**\n\n```py\nlabels_onehot_dataframe = pd.get_dummies(labels,prefix='species')\none_hot = np.array(labels_onehot_dataframe)\n```\n\n","tags":["Data Mining","DS"],"categories":["Predicted Analytics: Tools & Techniques"]},{"title":"Iterators & Generators","url":"/docs/Immediate-to-Python-3-Iterators-and-Generators/","content":"\n# **Iterators**\n\n## Another look at `range()`\n\nThe `range()` function returns an iterator that will iterate through the values that we specify when we call `range()`.\n\nThe iterator only calculates and yields one value at a time.  It does not calculate or store those numbers in memory up front, it calculates them only when they are needed, one at a time.\n\n```python\nmy_range = range(11, 23)\nfor x in my_range:\n    print(x)\n```\n\n## File Object Are Also Iterators\n\nYou may have remembered from the prerequisite course how we opened files and iterated through the lines one at a time. The main takeaway is that each line is read in (iterated through) one at a time.  The entire file is not read up-front, each line is read into this notebook one at a time, as needed.\n\n```python\nfilepath = os.path.join(os.getcwd(), 'AAA_Fuel_Prices.csv')\ncount = 0\nwith open(filepath, 'r') as my_file:\n    for line in my_file:\n        line = line.strip()\n        print(line)\n        count += 1\n        if count > 10:\n            break\n```\n\n## Iterators Can Also Be Made From Lists (as well as other data types)\n\nIterators can also be made out of lists. This is what happens when we use a list in a for loop.\n\n```python\nmy_list = [1, 2, 3, 4, 5]\nfor x in my_list:\n    print(x)\n```\n\n```python\nmy_list_iterator = iter(my_list)\n\nprint(type(my_list_iterator))\nprint(my_list_iterator.__next__())\nprint(my_list_iterator.__next__())\nprint(my_list_iterator.__next__())\n```\n\n# **Generators**\n\nYou may wonder how can we write functions, like `range()`, that return iterators that we can iterate through. We can!  In order to do so, we must use the keyword `yield` instead of `return`.\n\n## Our Own Version Of Range\n\nLet's write our own version of the `range()` function.  We need to write a function the will yield numbers between a beginning and ending number. Note that when the function reaches the yield keyword, it will return that value (in this case, the value of `i`) and it will cease execution until it is asked for the next value. \n\n```python\ndef my_range(beg, end):\n    \"Generate numbers from start to stop\"\n    i = beg\n    while i < end:\n        yield i\n        i += 1\n```\n\n```python\n# Let's call the function to return a generator that we can iterate through. \nrange_of_nums = my_range(0, 10)\n\n# Now, let's call the __next__() method to get each value is the it is \n# \"yielded\" by the generator\n\n# This executes the code in the generator until it hits the yield statement.  It then stops until __next__() is called again.\nprint(range_of_nums.__next__())\nprint(range_of_nums.__next__())\nprint(range_of_nums.__next__())\n```\n**output:**\n\n```\n0\n1\n2\n```\n\nWe do not usually use the `__next__()` method directly. We are usually looping over the iterator, or passing the iterator to another iterative process (in these case,`__next__()` is still used \"under the hood\", but we are not using it directly as programmers). Below, we simple use `range_of_nums` in a loop, we also use the `my_range()` function directly in a for loop, just like you would use `range()`.\n\n```python\nfor num in my_range(30, 33):\n    print(num)\n```\n\n## **A Fibonacci Series Generator.**\n\nA Fibonacci Series is a series of numbers in which the next number is the sum of the two preceding numbers.  If we start with 0 and 1, then the series is 0, 1, 1, 2, 3, 5, 8, 13, etc.  This is a fun series that is often used in computer science lessons. Let's code a function that will return a generator that iterates through the Fibonacci Series (starting with 0 and 1).\n\n```python\ndef fibonacci_series(N):\n    \"\"\"Generate the Fibonacci series starting at 0 and 1\"\"\"\n    # We start by seeding 0 and 1 as the first two numbers\n    i_prev = 0\n    i = 1\n    yield i_prev  # we yield 0 first\n    # now in the following loop, we yield \"i\" and then calculate i_next by\n    # summing the two previous\n    for _ in range(N-1):\n        yield i \n        i, i_prev = i + i_prev, i\n```\n\n```python\nf_s = fibonacci_series(10)\nfor x in f_s:\n    print(x)\n```\n\n**Iterators Can Only Be Iterated Over Once**\n\nOnce we create an iterator, it can only be iterated over once.  For example, in the above cell we looped over the entirety of `f_s`.  Below, we try to loop over it again, but nothing prints. This is because we have already looped over the iterator to its end.  If we need to iterate again, we will have to create a new iterator.\n\n## File Word Counts\n\nAnother example, which will become more meaningful if you take the course Python Data Structures, Data Mining and Big Data, is producing a word counts from a file, line by line.\n\nLet's write a function that will generate word counts, from a file, line by line.\n\n```python\ndef file_word_count(filepath):\n    \"\"\"Generate word counts from ta file, line by line\"\"\"\n    # First, open the file\n    with open(filepath, 'r') as my_file:\n        # Loop through the lines\n        for line in my_file:\n            line = line.strip()  # strip whitespace from the line\n            words = line.split()  # split the line into words\n            # create a dictionary that we will store the word counts in\n            word_count_dict = {}\n            # loop through the words in the line and tally them in the\n            # dictionary\n            for word in words:\n                if word in word_count_dict:\n                    word_count_dict[word] += 1\n                else:\n                    word_count_dict[word] = 1\n            # now loop through the dictionary and yield up the word counts \n            for word in word_count_dict:\n                yield word, word_count_dict[word]\n```\n\n```python\naesopa10_path = os.path.join(os.getcwd(), 'aesopa10.txt')\ncounter = 0\nfor word, count in file_word_count(aesopa10_path):\n    print(word, count)\n    counter += 1\n    if counter > 2000:\n        break\n```\n\n# **Enumerate and Zip**\n\n## Enumerate\n\nEnumerate takes in an iterable object (like a list, tuple, or some other iterator) and outputs tuples that are enumerated. The first element in the tuple is the number and the second is the value from the original iterable object.\n\n```python\nmy_list = ['a', 'b', 'c']\n\nfor item in enumerate(my_list):\n    print(item)\n```\n\n```\n(0, 'a')\n(1, 'b')\n(2, 'c')\n```\n\n## Zip\n\nZip will take multiple iterable objects as inputs and output tuples that contain items from each iterable, in order. That is the first tuple will contain the first item from each iterable, the second tuple will contain the second items, etc...\n\n```python\ntuple_1 = (2012, 2012, 2012)\ntuple_2 = ('01', '02', '03')\n\nnew_list = []\nfor val1, val2 in zip(tuple_1, tuple_2):\n    print(val1, val2)\n    new_list.append(str(val1) + '_' + val2)\nprint(new_list)\n```\n\n```\n2012 01\n2012 02\n2012 03\n['2012_01', '2012_02', '2012_03']\n```\n\n```py\nfor x in zip(tuple_1, tuple_2):\n    print(x)\n```\n\n```\n(2012, '01')\n(2012, '02')\n(2012, '03')\n```\n\n","tags":["Python"],"categories":["Immediate to Python"]},{"title":"8 kNN Model","url":"/docs/Predicted-Analytics-6-kNN-Model.md/","content":"\n# **Similarity Based Learning**\n\n![image-20230307135237509](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230307135237509.png)\n\n**Compute the distance matrices between objects**\n\n![image-20230307135320379](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230307135320379.png)\n\n# **k Nearest Neighbor (kNN) Model**\n\n## Pros and Cons of kNN\n\n| Pros                                                         | Cons                                                         |\n| ------------------------------------------------------------ | ------------------------------------------------------------ |\n| Simple and Effective                                         | Does not produce a model, limiting the ability to understand how the features are related to the class |\n| Makes no assumption about the underlying data distribution<br/>Non-parametric | Requires selection of an appropriate value of ‘k’            |\n\n## Example\n\n![image-20230307135514058](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230307135514058.png)\n\n![image-20230307135528439](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230307135528439.png)\n\n![image-20230307135551335](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230307135551335.png)\n\n![image-20230307135600672](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230307135600672.png)\n\n![image-20230307135654082](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230307135654082.png)\n\n![image-20230307135706642](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230307135706642.png)\n\n# **kNN Model Assessment**\n\n![image-20230307135731080](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230307135731080.png)\n\n![image-20230307135744961](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230307135744961.png)\n\n# **Data Normalization: Standardization & Scaling**\n\nSuppose we have 2 data items\n\n* Height: varies from 4 – 7 feet\n* Net Worth: $10,000 - $100B\n\nIf we use both the variables in a model\n\n* Net Worth will dominate because it contains large values\n\nSolution\n\n* Standardize\n* Scale\n\n## Data Standardization and Scaling\n\n![image-20230307135919181](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230307135919181.png)\n\n# kNN in Python\n\n[03_kNN_Optimum_k_Iris_Sci_L_ConMat.ipynb](https://colab.research.google.com/drive/1rkJLDzQ-40DtU-MPmGMYTANrwNn_wNeo)\n","tags":["Data Mining","DS"],"categories":["Predicted Analytics: Tools & Techniques"]},{"title":"Functions Positional & Keyword Arguments","url":"/docs/Immediate-to-Python-2-Functions-Positional-and-Keyword-Arguments/","content":"\n# Positional Arguments\n\nThese are argument that are assigned based on their position in the function definition.  A example below will make this clear.\n\n```python\ndef print_greeting(username, date):\n    \"\"\"Print a simple greeting\"\"\"\n    greeting = 'Greetings, {}. The date is {}'.format(username, date)\n    print(greeting)\n```\n\n```python\n# test the function\nprint_greeting('Ronda', '2019-01-01')\n```\n\nWhy did the function treat 'Ronda' as the username argument and '2019-01-01' as the date argument? The answer is simple: it is because 'Ronda' was the first argument passed to the function and, in the function definition, username is the first argument in the function signature.\n\nThe arguments are assigned by *position*.\n\n# Keyword Arguments\n\nKeyword arguments are arguments that are passed to a function by the argument name.  For example, when calling the `print_greeting` function we can pass the arguments as shown in the cell below.\n\n```python\nmy_name = \"Will\"\ntoday = \"2019-07-01\"\nprint_greeting(date=today, username=my_name)\n\nprint_greeting(username=\"Padma\", date=\"2019-03-01\")\n```\n\n**Sometimes you can't use keyword to specify arguments, and sometimes you can only use keywords to specify certain arguments.**\n\nMany built in Python function are implemented in C and use a position-only API for processing argument. That means that there are some function with arguments that can not be specified by keyword. \n\n```python\nhelp(sorted)\n```\n\n![image-20230301180010676](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230301180010676.png)\n\nNotice the `/` in the function signature above?  This indicates that all the argument to the left of it can ONLY be specified by position. That is you can not pass the 'iterable' argument by keyword. (You may notice the `*` in the signature as well, we will discuss that in a moment)\n\n```python\nmy_list = [2, 3, 1]\n\n# We incorrectly try to pass the iterable argument by keyword. This will produce an error\n\n# We correctly pass the iterable argument by position\nsorted_list = sorted(my_list)\nprint(sorted_list)\n```\n\nNow, what about the `*` in the function signature? This means that you can ONLY specify the arguments to the right of it by keyword.  For example, in the sorted function, the arguments `key` and `reverse` can only be specified by keywords. See an example below:\n\n```python\nmy_list = ['a_3', 'b_2', 'c_1']\n\nsorted_list = sorted(my_list, key=lambda x: x.split('_')[-1])\n\nprint(sorted_list)\n```\n\n# Introducing *args\n\nNow, take your mind back to positional arguments.  What if you want a function to accept unlimited positional arguments? This is what the `*args` argument can do. By using the `*args` argument in our function definition, all positional arguments will be collected in a **tuple** named `args`. Let's define a function below using the `*args` argument to see how this works\n\n```python\ndef args_example(*args):\n    '''Print the contents of args.'''\n    print(args)\n    \nargs_example('Miguel', 'Mary', 'Paul')\n# Output: ('Miguel', 'Mary', 'Paul')\n```\n\n## Combining \\*args with other positional arguments.\n\nYou can have positional arguments *before* `*args`. `*args` will collect all the extra position arguments passed to the function.\n\n```python\ndef print_all_the_greetings_2(greeting, *args):\n  '''Print \"{greeting}, {name} for the greeting and all the names passed as arguments\n\t'''\n\tfor name in args:\n\t\tprint(\"{}, {}\".format(greeting, name))\n        \nprint_all_the_greetings_2('Good Morning', 'Winston', 'Aarav', 'Julie')\n```\n\n# Introducing \\*\\*kwargs\n\nMuch like `*args` captures extra positional arguments, `**kwargs` captures extra keyword arguments in a dictionary called `kwargs`.\n\n```python\ndef kwargs_example(**kwargs):\n    \"\"\"Print kwargs\"\"\"\n    print(kwargs)\n    \nkwargs_example(name=\"Paloma\", occupation='teacher')\n```\n\n# Mixing Positional and Keyword Arguments.\n\nWhen using both positional and keyword arguments you must specify the positional arguments **first**.\n\n```PYTHON\nprint_greeting(username=\"Padma\", date=\"2019-03-01\")\n\nprint_greeting(\"Padma\", date=\"2019-03-01\")\n```\n\n## Mixing Positional Arguments, \\*args, Keyword Arguments and \\*\\*kwargs\n\n```python\ndef test_arg_and_kwarg(arg1, arg2, *args, kwarg1, kwarg2, **kwargs):\n    print(arg1)\n    print(arg2)\n    print(args)\n    print(kwarg1)\n    print(kwarg2)\n    print(kwargs)\n    return\n\ntest_arg_and_kwarg(1, 2, 'extra_1', 'extra_2', kwarg1='kwarg 1',\n                   kwarg2='kwarg 2', extra_kwarg1='Bonus!',\n                  extra_kwarg2=\"I'm extra!\")\n```\n\n# Default Arguments\n\nSometimes you want to specify a default argument value to one of the arguments in your function. This means that if no value is passed to that specific argument, it will still have a default value and the function will run successfully.\n\nThis is very common. In fact, let's look at the documentation for the built in function `sorted` again.\n\nSee the 'key=None' and the 'reverse=False' in the function signature? This indicates that the default value for key is None, and the default value for reverse is False. This means that if we use the sorted function, these will be the values for these arguments if we do not specify other values.\n\n```python\nhelp(sorted)\n\n# Output:\nHelp on built-in function sorted in module builtins:\n\nsorted(iterable, /, *, key=None, reverse=False)\n    Return a new list containing all items from the iterable in ascending order.\n    \n    A custom key function can be supplied to customize the sort order, and the\n    reverse flag can be set to request the result in descending order.\n```\n\n## You can use \\* and \\*\\* to pass arguments to functions.\n\nJust as we used the \\* and \\*\\* to collection arguments that are passed to a function, you can also use the to pass arguments to a function from a tuple or dictionary.  Let's walk through two examples below.\n\n```python\ndef my_function(x, y):\n    result = x**2-y**(0.5)\n    return result\n    \n# Here we use the function and pass the values directly\nresult1 = my_function(2.1, 0.6)\n\n# Belww, we define a tuple with the values, and then pass those values to the\n# function by indexing them from the tuple.\nvalues = (2.1, 0.6)\nresult2 = my_function(values[0], values[1])\n\n# Here, we just use the '*' to dump the arguments in the values tuple directly\n# to the function\nresult3 = my_function(*values)\n\n# Finally, print all three results so that we ensure all methods give the same\n#result\nprint(result1, result2, result3)\n```\n\n```python\ndef my_greeting(date, greeting):\n    print('{}: {}'.format(date, greeting))\n    \n# Here we use the function and pass the values directly\nmy_greeting('2018-11-07', \"How are you today?\")\n\n# Below, we define a tuple with the values, and then pass those values to the\n# function by indexing them from the tuple.\nmy_greeting(greeting='How are you today?', date='2018-11-07')\n\n# Here, we just use the '*' to dump the arguments in the values tuple directly\n# to the function\nvalues = {'greeting': 'How are you today?', 'date': '2018-11-07'}\nmy_greeting(**values)\n```\n\n# An example of a built-in function that actually uses `args` and `kwargs`!\n\nNow, let's look at a built-in functions that uses both `*args` and `**kwargs`.\n\nThe function is the `format` method of string objects.  Depending on which version of the prerequisite course you have taken, you may have already seen this, but we will do a quick review anyways.\n\n## The .format() method of a string object.\n\nOne of the preferred methods to format strings in Python is to use the format method of string objects. (The latest preferred method is something called 'f strings'). Observe the example below.  First, we define a string and we put `{}` in the string wherever we would like to fill in the string by a variable. We then call the `.format()` method on the string and pass to it the variables we would like to use to fill in the `{}` portions of the string.  The `{}` are filled in by the order we pass the variables to the `.format()` method.\n\n```python \nmy_string = 'Hi, my name is {}. I live in {} and I work at {}'.format('Will', 'California', 'UCI')\nprint(my_string)\n```\n\nThe point here is that `.format()` can accept *any* number of arguments, it just depend how many `{}` we have to fill in in the string.  How does `.format()` do this? It uses the `*args` argument to capture all of the positional arguments passed, and then it fills in the `{}` in the order of the arguments.\n\n## The .format() method also uses kwargs!\n\nFormat also supports keywords, observe the example below:\n\n```python\nmy_string = 'Hi, my name is {name}. I live in {home} and I work at {work}'. \\\n    format(name='Will', home='California', work='UCI')\nprint(my_string)\n```","tags":["Python"],"categories":["Immediate to Python"]},{"title":"7 Multi Variable Regression","url":"/docs/Predicted-Analytics-5-Multi Variable Regression/","content":"\n# **Multiple Variables Regression**\n\n**Definition**\n\nHow a response variable $y$ changes as the predictor (explanatory) variables $x1$, $x2$, ... $xn$ change\n\nIn a $n$ variable regression,\n\n* There is 1 response variable $y$\n* And $n$ predictor variables $x1$, $x2$, $x3$, ... $xn$, Goal is to find the values of $\\beta1$, $\\beta2$, $\\beta3$, ... $\\beta{n}$\n\n$$\ny = \\beta_1x_1+\\beta_2x_2+\\beta_3x_3+...+\\beta_nx_n+c\n$$\n\n**Regression Strategy**\n\nSame strategy used in the 2-variable regression: The least-squares regression line of y and x is the line that makes \n\n* the sum of the squares of the vertical distances of the data points from the line as small as possible\n\n# **Correlation between Multiple Variables**\n\n## Correlation Matrix\n\n**Definition: Correlation**\n\nA correlation matrix shows the linear correlation between each pair of variables under consideration in a multiple regression model\n\n**Predictor Variables Selection Criteria in Multi Regression**\n\n* Predictor variables should have a high linear correlation with the response variable\n* But do not include variables that are highly correlated among themselves\n\n**Multi-collinearity**\n\nMulti-collinearity exists between 2 explanatory (predictor) variables if they have a high linear correlation\n\n# **Normality of Residuals**\n\n![image-20230307134215162](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230307134215162.png)\n\n![image-20230307134223716](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230307134223716.png)\n\n# **Test Individual Regression Coefficients for Significance**\n\n## Coefficient: p-value\n\nSince both p-values are sufficiently small\n\n* We reject both the NULL hypothesis\n* There is a linear relationship between both the predictor variables and the response variable\n\nIf the p-value for the slope coefficient is large\n\n* We should consider removing it from the model\n\n# Example\n\n[P1_x1x2x3x4y.ipynb](https://colab.research.google.com/drive/1cWNHzXwddnd2cudiIZ9twv3Gxc7jSkNu)\n","tags":["Data Mining","DS"],"categories":["Predicted Analytics: Tools & Techniques"]},{"title":"6 Regression Quality","url":"/docs/Predicted-Analytics-4-Regression-Quality/","content":"\nMetrics to measure the Quality of Regression \n\n* Correlation between the response variable and predictor variables \n* Root Mean Square Error (RMSE) \n* R-square + Adjusted R-Square \n* p-values of the predictor variables \n* Residuals are normally distributed. \n\n# **Measure of Regression**\n![image-20230307111851997](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230307111851997.png)\n\n**Total Deviation = Explained Deviation + Unexplained Deviation**\n\n**SST = SSR + SSE**\n\n* SST = Total Sum of Squares = Total Deviation\n* SSR = Regression Sum of Squares = Explained Deviation\n* SSE = Error Sum of Squares = Unexplained Deviation\n\n![image-20230307112029424](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230307112029424.png)\n\n## Coefficient of Determination $r^2$\n\n**Definition**\n$$\nr^2 = \\frac{explainedVariation}{totalVariation}=\\frac{SSR}{SST}\n$$\n\n\n**How to compute $r^2$**\n$$\nsquareCorrelationValue = r^2 = 1-\\frac{SSE}{SST}\n$$\nThe closer $r^2$ is to $1$, the better the fir. For a perfect fit, $SSE = 0$, $r^2 =1$\n\n# **Standard Error**\n\nStandard error is the standard deviation of **the deviation of actual response variable with the predicted variable (residuals)** using the regression line.\n\nDegree of freedom\n\n* Two variables are estimated: Slope, Intercept\n* Lose 2 degree of freedom: $df=n-2$\n\n$$\ns_e=\\sqrt{\\frac{\\sum(y_i-\\hat{y_i})^2}{n-2}}=\\sqrt{\\frac{\\sum(residuals)^2}{n-2}}=\\sqrt{\\frac{SSE}{n-2}}\n$$\n\n\n\n# **Verify that the residuals are normally distributed**\n\n**If the residuals are not normally distributed, regression is not Valid**\n\n## Histogram\n\nPlot the histogram of the data, see a normal distribution\n\n**Problem with this technique**\n\n* Histograms shape change with different bin sizes\n\n## QQ Plot - Quantile-Quantile plot\n\nData is plotted against a theoretical normal distribution. If you see a straight line, data is normally distributed\n\n**Testing Procedure**\n\n* First Sort the data\n* Plot against appropriate quantiles from the standard normal distribution\n  * Divide the normal distribution curve into (n+1=10) parts, each part represents 10% of the area\n  * Compute the corresponding z-values\n\nQQ plot is\n\n* X axis: z-values taken from the standard normal distribution curve\n* Y-axis: Sorted Data values\n\n![image-20230307133050546](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230307133050546.png)","tags":["Data Mining","DS"],"categories":["Predicted Analytics: Tools & Techniques"]},{"title":"5 Linear Regression","url":"/docs/Predicted-Analytics-3.2-Linear-Regression/","content":"\n*Linear regression* *analysis* is a widely used statistical technique used to explore relationships among continuous variables. In addition, this technique can be applied to categorical variables through the use of dummy variables.\n\nThere are many situations where linear regression is useful. For instance,\n\n- A market researcher can use regression to determine which one of several media markets is the best on which to spend their advertising dollars;\n- In business, regression can be used to predict a competitive salary offer for a programmer with a given number of years of experience;\n- In Major League Baseball, regression can be used to predict the salaries of free agents;\n- In social science research, regression can predict a wide range of phenomena—from economic performance to college enrollment.\n\n# **Simple Linear Regression**\n\n*Simple linear regression* refers to the statistical relationship between a dependent continuous variable, say *y*, and a single independent continuous variable, say *x*. This relationship takes the form of an equation that lets us predict values of *y* given values of *x*. Regression equations are useful for estimating new data values or exploring “what if” questions.\n\nAs an example of simple linear regression, suppose you survey a group of workers and notice that those who have more years of education tend to have received higher starting salaries when they began working. A linear regression analysis allows you to quantify the relationship between starting salary and years of education through an equation that you can then use to make predictions about starting salaries for other individuals. \n\nLinear regression analysis allows you to (1) determine whether one or more continuous (independent) variables can effectively predict the values of an outcome (dependent) variable and (2) quantify the impact that each independent variable has on that outcome variable.\n\nThe relationship between a dependent variable and a single independent variable can be visualized using a scatter plot, as shown in Figure 1, below. The figure shows a set of data points along with the “best-fit” line that results from a regression analysis of that data.\n\n![A simple scatter with Fit Line of Cred card debt in the thousands by Household income in thousands showing the relationship between credit card debt and household income.](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/8z2gseSWiCfo4OvgpV_tyZr16er1sl1k6bsCGtrVhCrH8DrIRl3Jz4zaXHG7FmUiUiGqhjLPxpJXNjleOFdQP-8SW78yLrc_XzCSzZKO7wiBI7OFv7z-PCZ9VCRBgqkjM9weT-_CqN1WgquKbimuQA)\n\n*Figure 1: A scatter plot showing the relationship between credit card debt (in thousands) and household income (also in thousands)*\n\nThe line is represented in general form by the equation \n\n$$\ny = bx+a\n$$\n\nwhere *b* is the slope (the change in *y* per unit change in *x*) and *a* is the intercept (the value of *y* when *x* is zero). People are usually more interested in the slope than in the intercept. Also, note that *a* and *b* depend on the unit of measurement of the variables and, as such, do not necessarily indicate anything about the strength of the relationship.\n\nAs you can see, the data points don’t necessarily fall on this line, so it’s important to evaluate how well the line actually represents the data. To determine this, we start by looking at the *correlation* between the *x* and *y* values, and calculating a *correlation coefficient, r,* to quantify this relationship. Because the correlation coefficient can take on negative values and we prefer to work with only positive values, we typically square it (*r**2*) and refer to it as *R-squared.* This quantity lies on a scale from 0 (no linear association) to 1 (perfect linear association), and can be interpreted as the proportion of variation in one variable that can be predicted from the other. Thus an R-squared of 0.5 indicates that you can account for 50% of the variance in one variable if you knew the values of the other variable. Think of this value as a measure of the improvement in your ability to predict one variable from the other (or others if there are multiple independent variables).\n\nWhile R-squared provides a good starting point for determining how well your regression equation fits your data, a more rigorous way involves performing *statistical tests* to determine the *statistical significance* of the relationship. A relationship that is statistically significant means that your regression equation can reliably make predictions given new values of the independent variable. In other words, the predictions are more likely to be due to some factor of interest rather than random chance.\n\nReferring to Figure 1, notice that many points fall near the line but some are quite a distance from it. For each point, the difference between the value of the dependent variable and the value predicted by the equation (the value on the line) is called the *residual* (also known as the *error*). Points above the line have positive residuals (the equation under-predicted them), those below the line have negative residuals (the equation over-predicted them), and those points falling on the line have a residual of zero (perfect prediction). Points having relatively large residuals are of interest because they represent instances where the prediction performed poorly. Outliers, or points far from the positions of the other points, are of interest in regression because they can exert a considerable influence on the equation (especially if the sample size is small).\n\nLet’s apply linear regression analysis to an example in which household income is the independent (predictor) variable and credit card debt is the dependent variable. A standard linear regression analysis generates three tables depicting the relationship between the two variables. Here are several key points about the information appearing in those tables:\n\n- The *model summary table* provides several measures of how well the model fits the data (see Table 1).\n- As discussed above, R-squared, which can range from 0 to 1, is the correlation coefficient squared. It can be interpreted as the proportion of variance of the dependent measure that can be predicted from the independent variable(s).\n- The *adjusted R-squared* represents a technical improvement over R-squared in that it explicitly adjusts for the number of predictor variables relative to the sample size. If the adjusted R-squared and R-squared differ dramatically, it is a sign that you have used too many predictor variables for the sample size.\n- The *standard error of the estimate* is a measure of the standard deviation of the residuals. It represents the amount of variation that is not accounted for by the regression line on the scale of the dependent variable.\n\n![image-20230223222504355](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230223222504355.png)\n\nWhile the goodness-of-fit measures indicate how well you can expect the regression equation to predict the dependent variable, they do not tell whether there is a statistically significant relationship between the dependent and independent variable(s). For this, we turn to an *analysis of variance* (ANOVA) table, which presents technical summaries (i.e., sums of squares and mean square statistics) of the variation accounted for by the prediction equation (Table 2). The main goal is to determine whether there is a statistically significant (non-zero) linear relation between the dependent variable and the independent variable(s).\n\nThe significance (Sig.) column in the ANOVA table provides the probability that there is no relationship between the dependent and independent variable(s). A zero or nearly zero Sig. value means that the relationship is statistically significant and that you should further investigate the results of the regression coefficients appearing in the Coefficients table (Table 3).\n\n![image-20230223222446131](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230223222446131.png)\n\n*Table 2: ANOVA table showing sums of squares, mean squares, and the probability that there is no relationship between dependent and independent variables*\n\nThe first column contains a list of the independent variables plus the intercept (constant). The intercept is the value of the dependent variable when the independent variable is 0—it is also *a* in the equation $y=bx+a$.\n\n- The column labeled B contains the estimated regression coefficients you would use in a prediction equation. In this example, the coefficient for household income indicates that on average, each additional unit increase in household income is associated with an increase of 0.030 in credit card debt.\n- The Std. Error column contains standard errors of the regression coefficients. The standard errors can be used to calculate a 95% confidence interval above and below the B coefficients. (This means that statistically, the true value of B will fall within this interval 95% of the time.)\n- Betas are standardized regression coefficients used to judge the relative importance of each of several independent variables.\n- The t statistics provide a significance test for each B coefficient, indicating which predictors are statistically significant.\n\n![image-20230223222431472](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230223222431472.png)\n\n*Table 3: A tabulation of statistical information for the coefficients in the regression model relating credit card debt to household income*\n\n# **Multiple Linear Regression**\n\nRegression involving more than one independent variable is called *multiple linear regression* and is a direct extension of simple linear regression. When running a multiple linear regression analysis you are again concerned with fitting a linear model to the data, determining whether any of the variables are significant predictors, and estimating the coefficients of the best-fitting prediction equation. In addition, you are interested in the relative importance of the independent variables in predicting the dependent measure.\n\nContinuing with the previous example, we add age, years with current employer, and having previously defaulted as independent variables to the model, which now explains about 43% of the variance in credit card debt. This is a substantial increase in explanatory power from the 30% we were able to explain with just one predictor variable, namely household income.\n\n![image-20230223222534173](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230223222534173.png)\n\n*Table 4: The model summary table for a multiple linear regression*\n\nNot surprisingly, we still have a statistically significant model as shown in the ANOVA table Table 5).\n\n![image-20230223222555334](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230223222555334.png)\n\n*Table 5: ANOVA table showing sums of squares, mean squares, and the probability that there is no relationship between dependent and independent variables*\n\nFinally, we can see in Table 6 that all of the variables in the model are statistically significant except age, which means that we can remove this variable from the model. We can also see that household income is the most important predictor, followed by previous defaults, and then years with the current employer. We can use the regression equation that resulted from this analysis to predict someone’s credit card debt as follows:\n\n$$\ncreditCardDebt=0.026(householdIncome)+0.067(yearsWith Employer)+1.627(previouslyDefaulted)-0.721\n$$\n\nNote that “previously defaulted” is a categorical variable, which is coded as a dummy variable in which “no” is 0 and “yes” is 1.\n\n![image-20230223222722318](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230223222722318.png)\n\n*Table 6: A tabulation of statistical information for the coefficients in the regression model relating credit card debt to household income, age, years with employer, and previous defaults*\n\n# **Polynomials and Interaction Terms**\n\nThis is an advanced and important topic. It is not true that linear regression can identify only linear relationships. It can handle curvilinear relationships as well if you know how to prepare the data. For instance, if you determined (through visual inspection) that income would predict credit card debt better with a quadratic relationship, you could do the following.\n\n- Create a new variable consisting of income squared\n- Add another coefficient corresponding to the new variable\n\nIt would look like this:\n\n$$\ny=a+b_1x_1+b_2x_1^2\n$$\n\nwhere $x_1$= income and $x_1^2$= income squared.\n\nAnother feature you might uncover through visual inspection is a pair of variables that interact. Here again the solution is to create a new variable. A good indicator that you have an interaction occurring is if the slopes of the regression lines for two groups (e.g., males and females) are different. In our example, this would suggest that the relationship between credit card debt and income is different for people that had previously defaulted on a loan than it is for people that had not previously defaulted on a loan (see Figure 2).\n\nThe resulting regression formula looks like this:\n\n$$\ny=a+b_1x_1+b_2x_2+b_3x_1x_2\n$$\n\nwhere $x_1$= income and $x_2$= previously defaulted. The interaction appears as the product $x_1x_2$.\n\nIf you fail to include the interaction term, the model will mathematically force the lines to be parallel and maintain the same “ gap” over the entire income range. Figure 2 makes it clear that this would not be accurate since the gap is clearly more severe at higher levels of income. Polynomials and interaction terms can seem tricky and abstract at first, but they are critical for understanding neural networks, which will be presented in a later module.\n\n![A simple scatter with Fit Line of Cred card debt in the thousands by Household income in the thousands by Previously defaulted showing differing linear regression line slopes for customers that had and had not previously defaulted on a loan](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/Ej3sVrRFTs-bdKkJtOaIiAGX-PEAf5_Qu3oJrL9rB6FV4Ba9qaasgKCZRvvuOfmXwVbQwHnykt_t91faPELw1vPP7Z8UHXUAWMavQhsVs6EH5eqKCsegJRdqbzd7etezFIwh0fLcwoN5-UqvJjIRbw)\n\n*Figure 2: Differing linear regression line slopes for customers that had and had not previously defaulted on a loan*","tags":["ML","Data Mining","DS"],"categories":["Predicted Analytics: Tools & Techniques"]},{"title":"4 Introduction to Regression","url":"/docs/Predicted-Analytics-3.1-Introduction-to-Regression/","content":"\n# **Definition: Linear Regression**\n\n**2 variable regression** - how a response variable $y$ changes the predictor (explanatory) variable $x$ changes.\n$$\ny = \\beta_1x + c\n$$\n\n**Multiple regression** - how a response variable $y$ changes as the predictor (explanatory) variables $x1$, $x2$, ... $xn$ change\n$$\ny = \\beta_1x_1+\\beta_2x_2+\\beta_3x_3+...+\\beta_nx_n+c\n$$\n\n**Single Variable Polynomial Regression: First degree to Fifth Degree**\n\nThe concept can be extended to polynomial regression\n\n$$\n\\begin{align}\ny &= c + a_1x \\\\\ny &= c + a_1x + a_2x^2 \\\\\ny &= c + a_1x + a_2x^2 + a_3x^3 \\\\\ny &= c + a_1x + a_2x^2 + a_3x^3 + a_4x^4 \\\\\ny &= c + a_1x + a_2x^2 + a_3x^3 + a_4x^4 + a_5x^5 \\\\\ny &= c + a_1x + a_2x^2 + a_3x^3 + a_4x^4 + a_5x^5 +a_nx^n\\\\\n\\end{align}\n$$\n\n# **Regression Strategy: Ordinary Least Squares (OLS)**\n\nThe least-squares regression line of y and x is the \nline that makes the sum of the squares of the vertical \ndistances of the data points from the line as small as \npossible.\n\n![image-20230222092629577](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230222092629577.png)\n\n# Regression: Supervised Learning Method\n\nSingle split model assessment methodology\n\n* The model is tested on hold-out sample\n* Only the hold-out sample accuracy is reported\n\n![image-20230306002607033](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230306002607033.png)\n\n# **Nearest Neighbor Regression**\n\nA method for predicting a numerical variable $y$, given a value of $x$:\n\n* Identify the group of points for which the values of $x$ are close to the given value\n* The prediction is the average of the $y$ values for the group\n\n**Graph of Averages**\n\n* For each value of $x$, the predicted value of $y$ is the average of the $y$ values of the nearest neighbors.\n* Graph these predictions for all the values of $x$, That's the **graph of average**\n* If the association between the two variables is linear, then points on the graph of averages tend to fall on near a straight line. That's the **regression line.**\n\n# Solution for Regression Line\n\n* Residual = Observed value – Computed Value\n\nSuppose regression equation is\n$$\ny = mx+b\n$$\n\n* $y$ is the explanatory variable, $x$ is the predictor variable\n* $m$ is the slope of the line, $b$ is the intercept\n\n$$\n\\begin{align}\nResidual &= y_i-(mx_i+b)\\\\\nResidual^2 &= (y_i-(mx_i+b))^2\\\\\nResidualSumOfSquares &= RSS = \\sum_{i=1}^{N}(y_i-(mx_i+b))^2\n\\end{align}\n$$\n\n* To find the minimum point of this function, we will take the partial derivative of RSS with respect to ‘m’ and ‘b’ and set that to zero.\n\n![image-20230306005620610](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230306005620610.png)\n\n![image-20230306005637510](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230306005637510.png)\n$$\n\\begin{align}\nm &= \\frac{\\sum(y_ix_i)-\\frac{\\sum(y_i)\\sum(x_i)}{N}}{\\sum(x_i^2)-\\frac{\\sum(x_i)^2}{N}}\\\\\nb &= (\\frac{\\sum(y_i)}{N}-m\\frac{\\sum(x_i)}{N})\n\\end{align}\n$$\n\n## Method 1\n\n![image-20230306010108743](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230306010108743.png)\n\n![image-20230306010118579](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230306010118579.png)\n\n![image-20230306010129262](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230306010129262.png)\n\n# Example\n\n## #1 Galton\n\n[galton.ipynb](https://colab.research.google.com/drive/1EsqPlpKab6Ko7YXQzjLxe0kVI88L3341)  [galton.csv](https://drive.google.com/file/d/1R3n46DKTH_jXpkUaE_D0jOQ1mzblXTEk/view?usp=sharing)\n\n## #2 Using skLearn and staysmodel\n\n[P1_Dictionary.ipynb](https://colab.research.google.com/drive/1rfAxSYwvt0VAW7Z6oDrpfGb5j9TP-yZs)\n\n## #3 FULL CODE_Regression_Model\n\n[FULL CODE_Regression_Model_Advertising.ipynb](https://colab.research.google.com/drive/1fLiSvyb-y4cQR8MFPMc7ZytbgOTjRBr7)\n","tags":["ML","Data Mining","DS"],"categories":["Predicted Analytics: Tools & Techniques"]},{"title":"3 Modeling Techniques","url":"/docs/Predicted-Analytics-2.2-ML-Techniques/","content":"\n# **Modeling Methods**\n\n| #    | Modeling Methods               | Response Variable: Numerical /Categorical | Supervised or Unsupervised | Strategy                              |\n| ---- | ------------------------------ | ----------------------------------------- | -------------------------- | ------------------------------------- |\n| 1    | Linear & Polynomial Regression | Numerical                                 | Supervised                 | Error Based<br/>Minimizing Error      |\n| 2    | Logistic Regression            | Categorical (Binary)                      | Supervised                 | Maximizing Likelihood                 |\n| 3    | Discriminant Analysis          | Categorical                               | Supervised                 |                                       |\n| 4    | K Nearest Neighbor             | Categorical                               | Supervised                 | Similarity Based                      |\n| 5    | Decision and Regression Trees  | Categorical + Numerical                   | Supervised                 | Information Based                     |\n| 6    | Naïve Bayes                    | Categorical                               | Supervised                 | Probability Based                     |\n| 7    | Neural Networks                | Numerical + Categorical                   | Supervised                 | Mimicking Human Brain                 |\n| 8    | Clustering                     |                                           | Unsupervised               |                                       |\n| 9    | Principal Component Analysis   |                                           | Unsupervised               |                                       |\n| 10   | Support Vector Machines        | Categorical                               | Supervised                 | Error Based                           |\n| 11   | ARIMA : Time Series            | Numerical                                 | Supervised                 | Auto Regression & Moving <br/>Average |\n\n# **Estimation or Classification**\n\n**Goals of Machine Learning Application: Estimation or Classification**\n\n* **Estimation** – Regression modeling technique is used\n\n  *Output is a number*\n\n  * House price\n  * Product sales for next quarter\n  * GNP growth for the next quarter\n  * Employment\n\n* **Classification** – Naïve Bayes, Decision Trees etc. modeling techniques are used\n\n  *Output is a categorical variable*\n\n  * Sports team will win or lose\n  * Email is junk or not\n  * Which grade student will get\n  * Tweet is positive or negative\n\n# **Classification of Modeling Methods**\n\n**Response Variable**\n\n* Numerical or Categorical\n\n**Supervised or unsupervised**\n\n**Strategy**\n\n* Error based learning\n* Similarity Based Learning\n* Information Based Learning\n* Probability Based Learning\n* Mimicking the Human Brain\n\n# **Supervised vs. Unsupervised**\n\n**Supervisor learning** is the most common learning type where **there is a target/output variable** (which is also called supervisor)\n\n* Supervisor (target variable) teaches the algorithm how to build/learn the pattern model\n* In PA, supervised learning ≈ predictive modeling\n\n**Unsupervised learning has NO target variable**\n\n* No supervisor to teach → algorithm has to learn by itself\n* In PA, unsupervised learning ≈ descriptive modeling\n\n![image-20230221112637145](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230221112637145.png)\n\n# **Classifying Based on Strategy to Build a Model**\n\n## Error based learning\n\n* Linear Multi Variable Regression\n* Support Vector Machine\n\nIn error-based machine learning\n\n* We perform a search for a set of parameters for a parameterized model\n* That minimizes the total error across the predictions made by the model\n* With respect to a set of training instances (training data)\n\n![image-20230221113355334](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230221113355334.png)\n\n## Similarity Based Learning\n\n* K Nearest Neighbor\n\nCompute the distance matrices between objects\n\n![image-20230221113427635](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230221113427635.png)\n\n## Information Based Learning\n\n* Decision Trees\n* Regression Trees\n* Split of decision trees are based on the entropy of the tables\n\nLearn by Asking Questions\n\n* The Socratic approach to questioning is based on the practice of disciplined, thoughtful dialogue.\n* Socrates, the early Greek philosopher/teacher, believed that disciplined practice of thoughtful questioning enabled the student to examine ideas logically and to determine the validity of those ideas.\n\n![image-20230221113520227](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230221113520227.png)\n\n## Probability Based Learning\n\n* Naïve Bayes\n\nProvides a way to compute *reverse* probability. \n\nGiven $P(B|A)$, we can compute $P(A|B)$\n\n$$\nP(A|B) = P(B|A)P(A)/P(B)\n$$\n\nNaïve Assumption: Assuming Variable Independence\n\n## Mimicking the Human Brain: Neural Networks\n\n* Extract linear combinations of the inputs\n* Model the target as the non-linear functions of these features\n\n![image-20230221113814241](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230221113814241.png)\n\n**Deep Learning:** Complex set of Neural Networks with many layers of processing\n\n![image-20230221114146210](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230221114146210.png)\n\n**Main Applications of Deep Learning Neural Networks**\n\n* Image Recognition\n  * Convolution Neural Networks\n* Image Classification\n  * Convolution Neural Networks\n* Hand Writing Identification\n* Speech Recognition\n  * Long Short-Term Memory Networks","tags":["ML","Data Mining","DS"],"categories":["Predicted Analytics: Tools & Techniques"]},{"title":"2 Introduction to Predictive Modeling","url":"/docs/Predicted-Analytics-2.1-Introduction-to-Predictive-Modeling/","content":"\nData analysis involves a number of modeling techniques that can be classified as three main types:\n\n- Predictive\n- Clustering\n- Association\n\n# **Predictive Modeling**\n\n*Predictive modeling*, sometimes called ***supervised learning***, focuses on understanding or making predictions about a given variable based on one or more other variables. **The ultimate goal of predictive analysis is accuracy**. For example, you may want to predict which potential customers are more likely to apply for a credit card based on a knowledge of their demographic and financial information. In this scenario, there are two types of variables:\n\n- **The** **dependent variable** **or** **target**: The variable you are trying to predict or understand (i.e., the likelihood of applying for a credit card)\n- **The** **independent variables** **or predictors**: The variables you are using as a basis for predicting or understanding the target variable (i.e., demographic and financial information)\n\nBelow are several additional applications of predictive modeling:\n\n- Determining which students in a class will pass or fail\n- Predicting which parts of a city will experience increasing crime (predictive policing)\n- Forecasting the number of unfilled hotel rooms three months from now\n- Projecting which patients are likely to have a heart attack\n- Estimating how much each customer will purchase when shopping online\n- Classifying shipping containers according to the likelihood that they carry drugs or weapons\n- Identifying IP addresses that are sending unusual amounts of information\n\n# **Cluster Analysis**\n\nCluster analysis, or segmentation, is a type of unsupervised learning problem that is very different from predictive modeling. Cluster analysis is appropriate when you can associate each case in a dataset with other cases that share similar distinct characteristics. You end up with several groups, in which the members of each group are very similar to each other but very different compared to members of other groups.\n\nCluster analysis is often used in marketing campaigns so that customers receive ads tailored for the group to which they belong rather than generic ads. These groups are identified via cluster analysis. In such a scenario, there is no dependent variable; only independent variables are used to segment the cases.\n\nCases in which the variables have similar values are grouped into *clusters* in an attempt to find homogenous subsets. Determining the number of clusters to create is part of the challenge of such techniques. Since clustering is exploratory in nature, the resulting clusters are not necessarily right or wrong. Instead, cluster solutions should be judged by their ability to address the specific business problem you are trying to solve. Applications of cluster analysis include:\n\n- Market research\n- Plant and animal ecology\n- Medical imaging (e.g., PET Scans)\n- Service/product usage pattern identification\n- Social network analysis\n- Crime analysis\n- Anomaly detection\n\n# **Association Modeling**\n\nCompanies like Amazon and Netflix have made *association modeling* commonplace. As consumers, most of us have encountered recommendation engines where a movie is recommended based upon our prior viewing habits or books are recommended based on our prior purchases (or even just our prior browsing behavior). **Association models use transactional data to predict future transactions.** The idea is that you may be able to suggest additional items that a person may want or need based on their previous buying behavior. This results in statements such as, “People who bought product A and product B might also like product C” appearing on their computer screens.\n\nAssociation modeling is often described primarily as a kind of market basket analysis, but while it is strongly associated with retail data, it can also be applied in other areas. For example, in predictive maintenance, a pair of part failures might frequently be associated with the failure of a third part even though that third part doesn’t show evidence of trouble at the time the first two fail.\n\n**Transactions in association models often occur at the same time.** For instance, many items might be listed on a grocery store receipt, but, there is no indication of which purchases occurred before others. Hot dogs and hot dog buns may be frequently purchased together, but there is no “rule” that indicates which purchase occurred first. In a predictive model, you might say that hot dogs “predict” buns or that buns predict hot dogs. A further refinement involves ***sequence analysis***, which does take into consideration the order in which events, such as purchases, occur. This can be useful in predictive maintenance or in web mining, where it might be beneficial to know the sequence in which website visitors click on links and buttons on a page or move to other pages in the site.\n\nApplications of association modeling include:\n\n- Market basket analysis\n- Retail data analysis\n- Web usage \n- Insurance claim analysis\n- Service usage\n- Medical procedures\n\n# **Overview of Predictive Models**\n\nThis course covers only some of the most popular predictive models. Specifically, we’ll take a look at the following:\n\n- Statistical models\n- Decision Tree models\n- Machine Learning models\n\n## Statistical Models\n\n***Statistical models* produce equations and *statistical tests* guide predictor selection. These models make certain assumptions whereas *rule induction* and *machine learning* models do not.** Here are several characteristics of statistical predictive models:\n\n- Predictions are expressed as equations.\n- Equations allow users to see the effect of a one-unit change on any field and how this change impacts the outcome variable.\n- Predictive models are based on statistical theory, which involves developing hypotheses and assessing statistical significance. This allows you to easily identify the important variables.\n- Predictive models involve assumptions about the data, which may limit the situations in which some models can be used.\n\nBelow is a list of some statistical models.\n\n- Logistic Regression\n- Discriminant Analysis\n- Linear Regression\n- Generalized Linear Models\n- Cox Regression\n- Time Series\n\n## Decision Tree Models\n\nA *decision tree* or *rule induction model* is an important type of predictive model. It derives a set of rules in relation to a dependent variable. The model’s output shows the reasoning for each rule and can therefore be used to understand the decision-making process that drives a particular outcome. Models that produce decision trees belong to this class of models. Generally, decision tree predictive models:\n\n- Create segments that are mutually exclusive and exhaustive (identify homogeneous subgroups)\n- Create rules for making predictions about individual cases\n- Can easily handle a large number of predictors\n- Can account for interaction and non-linear relationships\n- Have few assumptions\n- Can create overly complex models that over-fit data (does not generalize)\n\nBelow is a list of several rule induction models:\n\n- CHAID\n- CART\n- C5.0\n- QUEST\n- Decision List\n- MARS\n\n## Machine Learning Models\n\n*Machine learning models* are optimized for learning complex patterns. Unlike traditional statistical techniques, no assumptions are made about the data. Machine learning models do not produce a set of rules like rule induction models, nor do they produce easy-to-understand equations like statistical models. Thus, machine learning models are often said to be “black box” models. They produce a set of equations, but because there is a hidden layer (possibly several hidden layers), the interpretation of the coefficient weights is not straightforward as it is with traditional statistical models or rule induction models. Machine learning predictive models:\n\n- Are optimized for learning complex patterns\n- Can account for interaction and non-linear relationships\n- Have few assumptions \n- Are essentially “black box” models—their interpretation is not straight-forward\n- Are used for predictive accuracy but not for understanding the mechanics behind a prediction\n\nBelow is a list of several machine learning models:\n\n- Neural Networks\n- Support Vector Machines\n- Random Forest\n- Naïve Bayesian Algorithms\n- Gradient Boosting Algorithms\n- K-Nearest Neighbors\n\n# **Model Validation**\n\nThe process of statistical hypothesis testing, which involves a result’s statistical significance in the context of certain data distribution assumptions (such as having normally distributed errors), helps us determine when we have found a valid and reliable result. However, most data-mining methods do not depend on specific data distribution assumptions for drawing inferences from the sample to the population. So how is validation achieved? Model validation in data mining is usually done by partitioning the data into training and testing datasets. Models are developed from the training data and then the models’ predictions are tested on the testing data. Validity is established by demonstrating that the model applies to data different from what was used to derive the model. Statisticians often recommend such validation for statistical models, but it is crucial for more general (less distribution-bound) data-mining techniques.\n\n# **How to Choose a Model**\n\nChoosing a model is difficult. Obviously, if you have a variable in the data file that you want to predict, then any of the predictive models (depending on the target variable’s level of measurement) will perform the task albeit with varying degrees of success. If you want to find groups of individuals that behave similarly on a number of fields in the data, then any of the clustering methods are appropriate. The use of association rules, while not directly giving you the ability to make predictions, are extremely useful as a tool for understanding the various patterns within the data.\n\nHowever, determining which particular prediction technique will work best depends specifically on how the variables you want to predict are related to the predictors. There are suggested guidelines as to when one technique may work better than another, but these are only suggestions and not rules.\n\nFrom the previous discussion, it follows that more than one prediction model can be used to predict an outcome. The business context provides the first deciding factor in selecting a model. For example, if your goal is to extract a set of rules from the model, a rul*e induction model* is the only choice. Alternatively, if the model itself is of no interest but must nevertheless be as accurate as possible, then any of the models could be a candidate for the task. When one class of model is preferred but there are many models within that class, how do you choose a specific model?\n\nEach model has different characteristics when it comes to they way in which:\n\n- Missing values are handled\n- Continuous predictors are handled\n- Categorical predictors are handled\n- Outliers are handled\n- The number of predictors impacts prediction\n- The model scores data\n\nThere are many subtle differences between the models. In the end, however, it is always the business users who balance the pros and cons, and decide which model or combination of models should be used. There is a wide range of possibilities and it is only the business user who can decide what to do.\n\nData analytics and reporting tools such as KNIME provide for simplicity in building models. Machine learning models, rule induction models (decision trees), and statistical models can be built with great ease and speed, and their results compared. You must remember that data mining is an iterative process: models will be built, broken down, and often even combined before the user is satisfied with the results.\n\nOne final yet important point to keep in mind when building models is that software will only find rules or patterns in data if they actually exist. You cannot extract a model with high predictive accuracy if there are no associations between the predictors and dependent variables.\n\n# **Reference** \n\n**Course text:** UCI. (2020). [Introduction to Predictive Modeling. ](https://docs.google.com/document/d/1Hzpxlu7ypOIbBC8IjuNPG_Mx3ndYB9RFPcMYawpTUJU/edit?usp=sharing)\n\n","tags":["ML","Data Mining","DS"],"categories":["Predicted Analytics: Tools & Techniques"]},{"title":"Sysargv, Argparse & Function Docstring","url":"/docs/Immediate-to-Python-1-Sysargv-Argparse-and-Function-Docstring/","content":"\n# **Sysargv**\n\n```python\n'''\nThis is a simple script.\n'''\nimport sys\n\nprint(__doc__)\t# output: docstring\n\nprint(sys.argv)\t# output: System argument vector\n```\n\n# **Argparse**\n\n## Example 1\n\n```python\n'''\nThis is a simple script.\n'''\nimport argparse\n\nparser = argparse.ArgumentParser(description=__doc__)\nparser.add_argument('echo', help=\"the string you want to write to the file\")\nargs = parser.parse_args()\nprint(args.echo)\n```\n\n**Output:**\n\n![image-20230226181135214](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230226181135214.png)\n\n## Example 2\n\n```python\nimport argparse\n\nparser = argparse.ArgumentParser(description=__doc__)\nparser.add_argument('x', help=\"the value for x\", type=int)\nparser.add_argument('y', help=\"the value for y\", type=int)\nparser.add_argument('-f', '--formula', help=\"the formula you'd like to run\",\n                    choices=[\"power\", \"subtract\"], default=\"power\")\n\nargs = parser.parse_args()\n\nif args.formula == \"power\":\n    print(args.x ** args.y)\nelif args.formula == \"subtract\":\n    print(args.x - args.y)\n\n```\n\n**Output:**\n\n![image-20230226183445409](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230226183445409.png)\n\n# **Function Docstring**\n\n## One-line Docstring\n\nThis is a simple function with a simple one-line docstring. Notice how the docstring is written as a command, \"Return the value...\" not a description \"This function returns the value...\"\n\n```py\ndef add_2(num):\n    \"\"\"Return the value of num + 2.\"\"\"\n    return_num = num + 2\n    return num\n```\n\n### Printing Docstrings\n\nWe can print doc strings by printing the **__doc__** attribute of the function.  This is a built-in attribute and all objects have it (even if the value is None).\n\nMany editors and IDEs have special functionality / commands to print docstrings. For example, if you write the '?' key after a function name, in Jupyter Notebook, and then evaluate the cell, it will open a window with the docstrings.  See the example below.\n\n```py\n# Below is an example of printing the docstring directly\nprint(add_2.__doc__)\n```\n\n![image-20230226185514333](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230226185514333.png)\n\n```py\nhelp(add_2)\n```\n\n![image-20230226185537947](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230226185537947.png)\n\n```python\n# Below is an example of using Jupyter Notebooks '?' command\nadd_2?\n```\n\n## Multi-line Docstrings\n\nA multi-line docstring provide more information but it still starts with single line description, followed by a blank line, and then a more detailed description. The more detailed description includes a description of the arguments, the return value(s), exceptions that the function raises, and any side effects.  It may also included references to similar functions and other helpful information.\n\n```py\ndef circle(radius):\n    \"\"\"Return the circumference and area of a circle, given the radius.\n    \n    Parameters\n    ----------\n    radius : float, int\n        the radius of the circle.\n        \n    Returns\n    -------\n    circumference : float\n        the circumference of the circle.\n    \n    area : float\n        the area of the circle.\n    \n    \"\"\"\n    circumference = 2*math.pi*radius\n    area = math.pi*(radius**2)\n    return circumference, area\n```\n\n### Printing Docstrings\n\n```python\n# method 1\nprint(circle.__doc__)\n\n# method 2\ncircle?\n```\n\n![image-20230226191611138](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230226191611138.png)","tags":["Python"],"categories":["Immediate to Python"]},{"title":"1 Introduction to Predictive Analytics","url":"/docs/Predicted-Analytics-1-Introduction-to-Predictive-Analytics/","content":"\n# **The Data Mining Process**\n\n***Data mining* is a general term that encompasses a number of data analysis techniques used to extract meaningful information from (large) data** files without necessarily having preconceived notions about what will be discovered. The useful information often consists of patterns and relationships in the data that were **previously unknown or even unsuspected**.\n\nA common misconception is that data mining involves passing huge amounts of data through intelligent technologies that find patterns and give magical solutions to business problems. This is not true, although data mining does involve more automation than one typically finds in traditional statistical analyses.\n\n**Data mining is an interactive and iterative process.** Business expertise must be used together with advanced technologies to identify underlying relationships and features in the data. A seemingly useless pattern in data discovered by data mining can often be transformed into a valuable piece of actionable information using business experience and expertise.\n\nMany of the techniques used in data mining are referred to as ***modeling*** and require a different approach for model generation and testing compared to standard, traditional statistics. While **traditional statistics focuses on probabilities and hypothesis** testing using data-specific research, **data mining focuses on using historical data** accumulated during the normal course of business. It is then the responsibility of the data miner to select, prepare, and analyze the data to determine whether it is acceptable and likely to generalize to the population of interest. Due to the typically large files involved and the weak assumptions made about the distribution of the data, **data mining tends to be less focused on statistical significance tests and more focused on practical importance.**\n\nData mining has been used in hundreds of applications, including:\n\n- Detecting fraudulent financial activity;\n- Identifying specific purchases that are more likely to lead to additional purchases;\n- Classifying customers into groups based on distinct purchase or usage patterns; and\n- Predicting which page a website visitor will visit next.\n\n# **The Art and Practice of Data Mining**\n\nThis is the definition that Keith McCormick has previously used in books and presentations. \n\n> *Data mining is the selection and analysis of data accumulated during the normal course of business. The goal is to find (and confirm) previously unknown relationships that can be used to develop predictive models that, when applied to new data, can produce valuable insight for making business decisions. Several points are worth emphasizing:*\n>\n> - *The data is not new.*\n> - *The data is not collected solely to perform data mining.*\n> - *The data miner is not testing known relationships (neither hypotheses nor hunches) against the data.*\n> - *The patterns must be verifiable.*\n> - *The resulting models must be capable of something useful.*\n> - *The resulting models must actually work when deployed on new data.*\t\n\nFor additional information on data mining, please visit [https://keithmccormick.com/data mining-defined/](https://keithmccormick.com/data-mining-defined/)\n\n# **Introducing CRISP-DM**\n\nThe typical data mining process can become complicated very quickly. There is much to keep track of—complex business problems, multiple data sources, varying data quality across data sources, an array of data mining techniques, different ways of measuring data mining success, and so on. To stay on track, it helps to have an explicitly defined process model for data mining that can guide you through critical issues and ensure that important points are addressed. This process model can serve as a data mining road map that helps you stay on course as you dig into the complexities of the data.\n\nThe data mining process model we recommend is the ***CRoss-Industry Standard Process for Data Mining* (CRISP-DM)**, which is considered the de facto standard for conducting a data mining project. As you can tell from the name, this model is designed as a general model that can be applied to a wide variety of business problems in just about any industry. The model includes six phases starting with *business understanding*.\n\n![image-20230223173559821](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230223173559821.png)\n\n![image-20230223173655160](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230223173655160.png)\n\n## Business Understanding\n\nThe business objectives and question(s) to be answered, and formulating a concrete plan for proceeding through the data mining process. You need to:\n\n- Identify business objectives and success criteria;\n- Perform a situational assessment (resources, constraints, assumptions, risks, costs, and benefits);\n- Determine the goals of the data mining project;\n- Identify success criteria; and\n- Produce a project plan.\n\n## Data Understanding\n\nOnce the business understanding phase is complete, you’re ready to begin collecting data and associated relevant information (such as the source of the data and the manner in which it was collected) for use in the project. It is also crucial to meet with subject matter experts (SMEs) to review what you have been collecting to ensure completeness (i.e., that no necessary data is missing) and verify your understanding of the data. It is also important to discuss the data in the context of the business problem you’re addressing. Sometimes, it may be necessary to return to the business understanding phase before proceeding.\n\nWith data in hand, you can begin exploring it and becoming thoroughly familiar with its characteristics. For each field in your dataset, you should review **the distribution, range (for continuous fields), outliers, anomalies, and missing values (type and amount)**. You can also begin looking for obvious, interesting patterns in the data such as relationships between a predictor and a target field. You’ll need to:\n\n- Understand your data resources;\n- Know the characteristics of the data;\n- Describe the data;\n- Explore the data; and\n- Verify data quality.\n\n## Data Preparation\n\nAfter cataloging your data resources, it’s time to prepare your data for mining. Data preparation is by far the most time-consuming step in the data mining process. **Various estimates suggest that 70% to 90% of the time spent on a data mining project is allocated to data preparation;** this is because you are using data that was collected for other reasons (for normal business operations, not for data mining). Preparations include:\n\n- Selecting data;\n- Cleaning data;\n- Constructing data;\n- Integrating data; and\n- Formatting data.\n\nThese tasks will likely be performed multiple times and not in any prescribed order. They can be very time-consuming but are critical for the success of the data mining project. In particular, data construction is a critical aspect of data preparation. Models work much better when the variables have been adjusted (e.g., by creating ratios, determining change scores, and calculating total scores) to make patterns appear more clearly. \n\n## Modeling\n\n![image-20230223173722789](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230223173722789.png)\n\nOnce you have prepared the data, you are ready for modeling, which involves sophisticated analytical methods that can extract information from the data. This phase involves:\n\n- Selecting the modeling technique;\n- Generating a test design;\n- Building the model; and\n- Assessing the model.\n- Validation\n  - Split the data\n    - Training: Build the model\n    - Testing: Test the model with testing dataset and compare the results with observed data\n\nDeveloping a model is an iterative process and you can expect to try several models and modeling techniques before finding the best one. **One feature that separates data mining from other approaches is the use of multiple models to make predictions, building on the strengths of each technique.**\n\nAn important part of model development is ***validating*** the model’s predictive capability. Briefly, the process involves dividing your dataset into two parts: **a *training dataset* and a *testing dataset***. You develop your model using the training data and then test it by making predictions using the testing data. If the predictions made using the two datasets are in agreement, you can begin applying the model to new data. In summary, you conclude that the model is valid by demonstrating that it applies to (fits) a dataset that is independent of the one used in the model’s derivation. Statisticians often recommend such validation for statistical models, generally, but it is especially important when employing data mining techniques. \n\n## Evaluation\n\nOnce you have chosen your models, you are ready to **evaluate how the data mining results can help you achieve your business objectives**. At this stage of the project, you have built one or more models that appear to be of high quality from a data analysis perspective. Before writing final reports and deploying the model, however, it is important to more thoroughly evaluate the model and review the steps taken in constructing the model so you can be certain it properly achieves your business objectives. A key aim is to determine if there are any important business issues that have not been sufficiently considered. At the end of this phase, a decision will be made on the use of the data mining results. The evaluation phase tasks are:\n\n- Evaluate results;\n- Review the process; and\n- Determine the next steps.\n\nEvaluation is frequently confused with *model **assessment***—the last task of the modeling phase. **Assessing the model focuses on the “data analysis perspective”** **and includes metrics like model accuracy and stability.** The authors of CRISP-DM considered calling this phase *business evaluation* because it has to be conducted in the language of the business using the metrics of the business as indicators of success. The value of a predictive model arises in two ways (Khabaza, 2010):\n\n1. The model’s predictions lead to improved (more effective) action; and\n2. The model delivers insight (new knowledge), which leads to improved strategy.\n\nKeep in mind that the value of a predictive model is not determined by any technical measure. Data miners should *not* focus on predictive accuracy, model stability, or any other technical metric for predictive models at the expense of business insight and business fit.\n\n## Deployment\n\nDepending on the business requirements, deployment can be as simple as generating a report or as complex as implementing a repeatable data mining process. **Keep in mind that creating the model is generally not the end of the project.** Even if the purpose of the model is only to increase one’s knowledge of the data, the knowledge gained will need to be organized and presented in a way that the organization can use for decision-making. So in essentially all projects, a final report will need to be produced and distributed.\n\n**Most critical is the deployment of the model to make predictions or create scores against new data.** This might be relatively simple if done within the data mining software you used, or more complex if the model is to be applied directly against an existing database. Whatever the case, a plan should be developed to monitor the model’s predictions and success in order to verify that the model is still valid.\n\nIn many projects, It is not unusual for the deployment team to be different from the modeling team; in some situations deployment may be the responsibility of team members having more of an IT focus. The tasks in the deployment phase are:\n\n- Plan the deployment;\n- Plan monitoring and maintenance activities;\n- Produce a final report; and\n- Review the project.\n\nSee also[ https://keithmccormick.com/crispdm](https://keithmccormick.com/crispdm) for additional information about CRISP-DM.\n\n# **Reference**\n\n**Course text:** UCI. (2020). [Introduction to Predictive Analytics.](https://docs.google.com/document/d/11zk4wIP0Hb_TeOo24fdTR8IX-lbGvWVCUHRvK563RUk/edit?usp=sharing)\n\n","tags":["Data Mining","DS"],"categories":["Predicted Analytics: Tools & Techniques"]},{"title":"Descriptive Analytics Data Visualization & Storytelling With Data","url":"/docs/Descriptive-Analytics-Data-Visualization-and-Storytelling-with-Data/","content":"\n**Readings:**\n\n1. [Introduction to Data Analytics and Data-Driven Decision Making.](https://docs.google.com/document/d/1dThO2qtamnsh5sB9aYIwUUvnMRMOpDGhuuYR8KKYw1Q/edit?usp=sharing)\n2. [Analysis and Data Exploration.](https://docs.google.com/document/d/1ItWcrCbE9BVJRNiAq3_UddVZ4lAmW7W_F7p_czgT4hA/edit?usp=sharing)\n\n3. [Creating Business Value through Data Presentation.](https://docs.google.com/document/d/1DcSZldqFKi_VVJD1lV1NyyMb-ta49ZvDewUsOJN_QxA/edit?usp=sharing)\n4. [Introduction to Descriptive Analytics.](https://docs.google.com/document/d/1tJR-OEVltVD8fHzJLEoJ-Ncg4F84y-T3t3ldtaPTS-c/edit?usp=sharing)\n5. [Introduction to Diagnostics Analytics.](https://docs.google.com/document/d/12veeK_yI_KUGKtCCCol9yGBV9GLsYALcj-YMLF2Vflw/edit?usp=sharing)\n6. [Introduction to Predictive Analytics.](https://docs.google.com/document/d/1sgn-PHP4qKIkl_NO36iYCbg50wLfKgo3e1NtH16twSk/edit?usp=sharing)\n7. [Introduction to Prescriptive Analytics.](https://docs.google.com/document/d/1hEeO0PHrxzVpCeQm8yRq3iPQhjDjJXzepJQ6kHZh_Ng/edit?usp=sharing)\n8. [The Future of Data Analytics.](https://docs.google.com/document/d/17edECjc4KAuY22e9v5pRwhdcpHjvqqgdziqCpVH1MWg/edit?usp=sharing)\n\n","tags":["DS"],"categories":["Data Analytics"]},{"title":"Intro to Analyzing Data for Business","url":"/docs/Intro-to-Analyzing-Data-for-Business/","content":"\n**Readings:**\n\n1. [Descriptive Analytics and Data Visualization](https://docs.google.com/document/d/11EGIvW8j3YHpxcWA-jxTk2qbQm6yGudVBZShg51VLYI/edit?usp=sharing)\n2. [Univariate Descriptive Analytics. ](https://docs.google.com/document/d/19vZW66el7k71v4t_NeMW7yPioi8ggiHLUDFfAe9qq8U/edit?usp=sharing)\n3. [Multivariate Descriptive Analytics.](https://docs.google.com/document/d/1EIPnOk-VsfoC9tsvyiDjVlZGNofTpaYzXXz2uJ9-MV8/edit?usp=sharing)\n4. [Network and Spatial Analysis.](https://docs.google.com/document/d/1SJn7Kkfsurw7Wy7zSmbK-yttgzKhykK5K2-2QZwBYYg/edit?usp=sharing)\n\n5. [Data Through Time.](https://docs.google.com/document/d/13M5CpLT56B-2K8pceZjOjkSNQVHjZ3SvPpllO5Jv6yo/edit?usp=sharing)\n\n6. [From Data to Visual Understanding.](https://docs.google.com/document/d/1btDfrphVZVQoMrelD1JXcLjWAeHmU1tHGoeNeCRQJV0/edit?usp=sharing) \n7. [Perception and Communication.](https://docs.google.com/document/d/1bYj9dpGDHz6VL55xcCLx7bbloQqu9lasGIeiLl4sLjE/edit?usp=sharing)\n8. [Going Beyond.](https://docs.google.com/document/d/1IltLJgC79jAuX0tT1RObB9cKQ_sviXwwLYR6b3IyafE/edit?usp=sharing)\n","tags":["DS"],"categories":["Data Analytics"]},{"title":"TO-DO List","url":"/docs/Other-TO-DO-list/","content":"\n# **To Be Continued**\n\n- [ ] Hung-yi Lee's Machine Learning\n\n- [ ] Intermediate to Python\n\n- [x] Predictive Analytics\n\n- [x] Introduce to Data Analytics for Business\n\n- [x] Descriptive Analytics: Visualization  \n\n# **About Website**\n\n- [ ] 目录二级标题缩进\n\n- [x] Motify `Archives` and `Tags`\n\n- [x] 公式超出卡片范围 [Reference]( https://docs.mathjax.org/en/latest/options/output/index.html#options-common-to-all-output-processors)\n\n- [x] 实现目录分级显示 \n\n- [x] 调整图片大小\n\n- [x] 图片点击预览 Fansy Box\n\n- [x] 搜索优化\n\n- [x] <mark>修改高亮颜色</mark>\n\n- [x] 修改代码块高亮\n\n- [x] 实现table居中显示\n\n- [x] 文章置顶\n\n- [x] 实现home页面宽屏大图片\n\n- [x] 表格宽度根据内容宽度进行调整\n\n- [x] 实现Search\n\n- [x] Emoji :thinking:\n\n- [x] 字体自适应窗口大小\n\n- [x] 设计滑动栏\n\n\n\n\n\n\n\n","categories":["Other"]},{"title":"1 Introduction of Deep Learning","url":"/docs/Machine-Learning-1-Introduction-of-Deep-Learning/","content":"\n# **Machine Learning ≈ Looking for function**\n\n![image-20221206053236150](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221206053236150.png)\n\n# **Different Types of functions**\n\n- **Regression**: The function outputs a scalar\n- **Classification**: Given options (classes), the function outputs the correct one.\n- **Structured Learning:** create something with structure (image, document)\n\n![image-20221206053348699](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221206053348699.png)\n\n# **How to find a function**\n\nA case study\n\n## The function we want to find …\n\n![image-20221206053424557](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221206053424557.png)\n\n![image-20221208221245405](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221208221245405.png)\n\n![image-20221208223257254](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221208223257254.png)\n\n![image-20221208223309541](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221208223309541.png)\n\n![image-20221208223318275](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221208223318275.png)\n\n![image-20221208223349650](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221208223349650.png)\n\n![image-20221208223401234](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221208223401234.png)\n\n![image-20221208223418427](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221208223418427.png)\n\n# **ML Framework**\n\n![image-20221206053857892](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221206053857892.png)\n\n## Step1. Model\n\ndepend on domain knowledge\n\n### Linear Models\n\nhave model bias (limitation), \n\n$$\ny = b + \\sum_{j=1}^{n}w_jx_j\n$$\n\n\n![image-20221206054003950](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221206054003950.png)\n\n![image-20221206054205148](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221206054205148.png)\n\n![image-20221206054210957](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221206054210957.png)\n\n### Sigmoid Function\n\n![image-20221206054229272](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221206054229272.png)\n\n![image-20221206054237612](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221206054237612.png)\n\n$$\ny=b+\\sum_{i}c_isigmoid(b_i+w_ix_i)\n$$\n\n### New Model: More Features\n\n![image-20221206054403705](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221206054403705.png)\n\nhyperparameter: \n\n* $i$ : no. of features\n* $j$ : no. of sigmoid\n\n![image-20221206054613829](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221206054613829.png)\n\n![image-20221206054619581](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221206054619581.png)\n\n### ReLu\n\n![image-20221206054710081](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221206054710081.png)\n\n### Deeper Model\n\n![image-20221206054747992](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221206054747992.png)\n\n## Step2. Loss\n\n![image-20221206054758334](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221206054758334.png)\n\n## Step 3. optimization\n\n![image-20221206054825545](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221206054825545.png)\n\n![image-20221206054848495](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221206054848495.png)\n\n![image-20221206054855172](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221206054855172.png)\n\n# **Deep Learning**\n\n![image-20221206054927207](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221206054927207.png)\n\n![image-20221208223439982](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221208223439982.png)\n\n\n\n","tags":["ML"],"categories":["HUNG-YI LEE | MACHINE LEARNING"]},{"title":"Class Intro","url":"/docs/Machine-Learning-0-Course-Intro/","content":"\n# **How to find a function**\n\n## Lecture 1-5: Supervised Learning\n\n![image-20221208230201056](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221208230201056.png)\n\n**Limit**: it is not easy to label for every assignments\n\n## Lecture 7: Self-Supervised Learning\n\n![image-20221208230316476](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221208230316476.png)\n\n![image-20221208230321680](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221208230321680.png)\n\n## Lecture 6: Generative Adversarial Network\n\n![image-20221208230347313](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221208230347313.png)\n\n## Lecture 12: Reinforcement Learning\n\n![image-20221208230407336](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221208230407336.png)\n\n## Lecture 8: Anomaly Detection\n\n![image-20221208230425964](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221208230425964.png)\n\n## Lecture 9: Explainable AI\n\n![image-20221208230443881](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221208230443881.png)\n\n## Lecture 10: Model Attack\n\n![image-20221208230457959](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221208230457959.png)\n\n## Lecture 11: Domain Adaption\n\n![image-20221208230520016](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221208230520016.png)\n\n## Lecture 13: Network Compression\n\n![image-20221208230533994](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221208230533994.png)\n\n## Lecture 14: Life-Long Learning\n\n![image-20221208230551887](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221208230551887.png)\n\n## Lecture 15: Met Learning\n\n![image-20221208230612831](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221208230612831.png)\n\n","tags":["ML"],"categories":["HUNG-YI LEE | MACHINE LEARNING"]},{"title":"Image Classification | Paper","url":"/docs/Computer-Vision-Image-Classification-Paper/","content":"\n- LeNet http://yann.lecun.com/exdb/lenet/index.html\n\n- AlexNet http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n\n- ZFNet Visualizing and Understanding Convolutional Networks https://arxiv.org/abs/1311.2901\n\n- VGG https://arxiv.org/abs/1409.1556\n\n- GoogLeNet, Inceptionv1(Going deeper with convolutions) https://arxiv.org/abs/1409.4842\n\n- Batch Normalization https://arxiv.org/abs/1502.03167\n\n- Inceptionv3(Rethinking the Inception Architecture for Computer Vision) https://arxiv.org/abs/1512.00567\n\n- Inceptionv4, Inception-ResNet https://arxiv.org/abs/1602.07261\n\n- Xception(Deep Learning with Depthwise Separable Convolutions) https://arxiv.org/abs/1610.02357\n\n- ResNet https://arxiv.org/abs/1512.03385\n\n- ResNeXt https://arxiv.org/abs/1611.05431\n\n- DenseNet https://arxiv.org/abs/1608.06993\n\n- NASNet-A(Learning Transferable Architectures for Scalable Image Recognition) https://arxiv.org/abs/1707.07012\n\n- SENet(Squeeze-and-Excitation Networks) https://arxiv.org/abs/1709.01507\n\n- MobileNet(v1) https://arxiv.org/abs/1704.04861\n\n- MobileNet(v2) https://arxiv.org/abs/1801.04381\n\n- MobileNet(v3) https://arxiv.org/abs/1905.02244\n\n- ShuffleNet(v1) https://arxiv.org/abs/1707.01083\n\n- ShuffleNet(v2) https://arxiv.org/abs/1807.11164\n\n- Bag of Tricks for Image Classification with Convolutional Neural Networks https://arxiv.org/abs/1812.01187\n\n- EfficientNet(v1) https://arxiv.org/abs/1905.11946\n\n- EfficientNet(v2) https://arxiv.org/abs/2104.00298\n\n- CSPNet https://arxiv.org/abs/1911.11929\n\n- RegNet https://arxiv.org/abs/2003.13678\n\n- NFNets(High-Performance Large-Scale Image Recognition Without Normalization) https://arxiv.org/abs/2102.06171\n\n- Vision Transformer https://arxiv.org/abs/2010.11929\n\n- DeiT(Training data-efficient image transformers ) https://arxiv.org/abs/2012.12877\n\n- Swin Transformer https://arxiv.org/abs/2103.14030\n\n- Swin Transformer V2: Scaling Up Capacity and Resolution https://arxiv.org/abs/2111.09883\n\n- BEiT: BERT Pre-Training of Image Transformers https://arxiv.org/abs/2106.08254\n\n- MAE(Masked Autoencoders Are Scalable Vision Learners) https://arxiv.org/abs/2111.06377\n\n- ConvNeXt(A ConvNet for the 2020s) https://arxiv.org/abs/2201.03545","tags":["CV"],"categories":["Computer Vision"]},{"title":"Cross Entropy","url":"/docs/Other-ML-Cross-Entropy/","content":"\n交叉熵 (Cross Entropy) 是深度学习中常用的一个概念，一般用来求目标与预测值之间的差距。\n\n<!--MORE-->\n\n交叉熵是信息论中的一个概念，要想了解交叉熵的本质，需要先从最基本的概念讲起。\n\n# **1 信息量**\n首先是信息量。假设我们听到了两件事，分别如下：\n\n> 事件A：巴西队进入了2018世界杯决赛圈。\n> 事件B：中国队进入了2018世界杯决赛圈。\n\n仅凭直觉来说，显而易见事件B的信息量比事件A的信息量要大。究其原因，是因为事件A发生的概率很大，事件B发生的概率很小。所以当越不可能的事件发生了，我们获取到的信息量就越大。越可能发生的事件发生了，我们获取到的信息量就越小。那么信息量应该和事件发生的概率有关。\n\n假设$X$是一个离散型随机变量，其取值集合为$x$,概率分布函数 $p(x)=Pr(X=x)$, $x∈χ$则定义事件$X=x_0$的信息量为：\n\n$$\nI(x_0)=−log(p(x_0))\n$$\n\n由于是概率所以$p(x_0)$的取值范围是 $[0,1]$,绘制为图形如下，可见该函数符合我们对信息量的直觉。\n\n![image-20221111062251336](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221111062251336.png)\n\n# **2 熵**\n\n考虑另一个问题，对于某个事件，有$n$种可能性，每一种可能性都有一个概率$p(xi)$\n这样就可以计算出某一种可能性的信息量。举一个例子，假设你拿出了你的电脑，按下开关，会有三种可能性，下表列出了每一种可能的概率及其对应的信息量\n\n| 序号 | 事件         | 概率 $p$ | 信息量 $I$         |\n| ---- | ------------ | ----- | ------------------- |\n| A    | 电脑正常开机 | 0.7   | $-log(p(A))=0.36$   |\n| B    | 电脑无法开机 | 0.2   | $-log(p(B))=1.61$ |\n| C    | 电脑爆炸了   | 0.1   | $-log(p(C))=2.30$ |\n\n> 注：文中的对数均为自然对数\n\n我们现在有了信息量的定义，而熵用来表示所有信息量的期望，即：\n\n$$\nH(X)=−\\sum_{i=1}^np(x_i)log(p(x_i))\n$$\n\n其中$n$代表所有的$n$种可能性，所以上面的问题结果就是\n\n$$\n\\begin{aligned}\nH(X) &= −[p(A)log(p(A))+p(B)log(p(B))+p(C))log(p(C))]\n\\\\&= 0.7×0.36+0.2×1.61+0.1×2.30\n\\\\&= 0.804\n\\end{aligned}\n$$\n\n然而有一类比较特殊的问题，比如投掷硬币只有两种可能，字朝上或花朝上。买彩票只有两种可能，中奖或不中奖。我们称之为0-1分布问题（二项分布的特例），对于这类问题，熵的计算方法可以简化为如下算式：\n\n$$\n\\begin{aligned}\nH(X)&=−\\sum_{i=1}^np(xi)log(p(xi))\\\\\n&=−p(x)log(p(x))−(1−p(x))log(1−p(x))\n\\end{aligned}\n$$\n\n# **3 相对熵（KL散度）**\n相对熵又称KL散度,如果我们对于同一个随机变量 $ x $ 有两个单独的概率分布 $ P(x) $ 和 $ Q(x)$ ，我们可以使用 KL 散度（Kullback-Leibler divergence）来衡量这两个分布的差异\n\n> 维基百科对相对熵的定义\n> In the context of machine learning, $D_{KL}(p‖q) $ is often called the information gain achieved if $P$ is used instead of $Q$.\n\n即如果用 $P$ 来描述目标问题，而不是用  $ Q$  来描述目标问题，得到的信息增量。\n\n在机器学习中，$P$ 往往用来表示样本的真实分布，比如 $[1,0,0]$ 表示当前样本属于第一类。$Q$ 用来表示模型所预测的分布，比如 $[0.7,0.2,0.1]$\n\n直观的理解就是如果用 $P$ 来描述样本，那么就非常完美。而用 $Q$ 来描述样本，虽然可以大致描述，但是不是那么的完美，信息量不足，需要额外的一些“信息增量”才能达到和 $P$ 一样完美的描述。如果我们的 $Q$ 通过反复训练，也能完美的描述样本，那么就不再需要额外的“信息增量”， $Q$ 等价于$P$。\n\nKL散度的计算公式：\n\n$$\nD_{KL}(p||q)=\\sum_{i=1}^np(x_i)log(\\frac{p(x_i)}{q(x_i)})\n$$\n\n$n$ 为事件的所有可能性。\n\n$D_{KL}$的值越小，表示 $P$ 分布和 $Q$ 分布越接近\n\n# **4 交叉熵**\n对式*KL散度的计算公式*变形可以得到：\n\n$$\n\\begin{aligned}\nD_{KL}(p||q)&=\\sum_{i=1}^np(xi)log(p(xi))−\\sum_{i=1}^np(xi)log(q(xi))\\\\\n&=−H(p(x))+[−\\sum_{i=1}^np(xi)log(q(xi))]\n\\end{aligned}\n$$\n\n等式的前一部分恰巧就是 $p$ 的熵，等式的后一部分，就是交叉熵：\n\n$$\nH(p,q)=−\\sum_{i=1}^np(xi)log(q(xi))\n$$\n\n在机器学习中，我们需要评估 `label` 和 `predicts` 之间的差距，使用KL散度刚刚好，即 $D_{KL}(y||\\hat{y})$，由于KL散度中的前一部分 $−H(y)$ 不变，故在优化过程中，只需要关注交叉熵就可以了。所以一般在机器学习中直接用用交叉熵做`loss`，评估模型。\n\n# **5 机器学习中交叉熵的应用**\n\n## 5.1 为什么要用交叉熵做loss函数？\n\n在线性回归问题中，常常使用MSE（Mean Squared Error）作为loss函数，比如：\n\n$$\nloss=\\frac{1}{2m}\\sum_{i=1}^m(y_i−\\hat{y_i})^2\n$$\n\n这里的 $ m $  表示  $ m $ 个样本的， $ loss $ 为 $ m $ 个样本的 $ loss $ 均值。\n\nMSE在线性回归问题中比较好用，那么在逻辑分类问题中还是如此么？\n\n## 5.2 交叉熵在单分类问题中的使用\n这里的单类别是指，每一张图像样本只能有一个类别，比如只能是狗或只能是猫。\n\n交叉熵在单分类问题上基本是标配的方法\n\n$$\nloss=−\\sum_{i=1}^ny_ilog(\\hat{y_i})\n$$\n\n上式为一张样本的  $ loss  $ 计算方法。式中 $ n  $ 代表着 $ n $ 种类别。\n\n举例说明，比如有如下样本：\n\n![SouthEast-16489642743263](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/SouthEast-16489642743263.png)\n\n对应的标签和预测值\n\n|       | 猫   | 青蛙 | 老鼠 |\n| ----- | ---- | ---- | ---- |\n| Label | 0    | 1    | 0    |\n| Pred  | 0.3  | 0.6  | 0.1  |\n\n那么\n\n$$\n\\begin{aligned}\nloss&=−(0×log(0.3)+1×log(0.6)+0×log(0.1)\\\\&=−log(0.6)\n\\end{aligned}\n$$\n\n对应一个batch的 $ loss $ 就是\n\n$$\nloss=−\\frac1m\\sum_{j=1}^m\\sum_{i=1}^ny_{ji}log(\\hat{y_{ji}})\n$$\n\n $ m $ 为当前 $ batch $ 的样本数\n\n## 5.3 交叉熵在多分类问题中的使用\n\n这里的多类别是指，每一张图像样本可以有多个类别，比如同时包含一只猫和一只狗\n\n和单分类问题的标签不同，多分类的标签是**n-hot**。\n\n比如下面这张样本图，即有青蛙，又有老鼠，所以是一个多分类问题。\n\n![SouthEast-16489643649396](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/SouthEast-16489643649396.png)\n\n对应的标签和预测值：\n\n|       | 猫   | 青蛙 | 老鼠 |\n| ----- | ---- | ---- | ---- |\n| Label | 0    | 1    | 1    |\n| Pred  | 0.1  | 0.7  | 0.8  |\n\n\n值得注意的是，这里的 `Pred` 不再是通过 `softmax` 计算的了，这里采用的是**sigmoid**。将每一个节点的输出归一化到 $ [0,1] $ 之间。所有 `Pred` 值的和也不再为1。换句话说，就是每一个 `Label` 都是独立分布的，相互之间没有影响。所以交叉熵在这里是单独对每一个节点进行计算，每一个节点只有两种可能值，所以是一个二项分布。前面说过对于二项分布这种特殊的分布，熵的计算可以进行简化。\n\n同样的，交叉熵的计算也可以简化，即\n\n$$\nloss=−ylog(\\hat{y})−(1−y)log(1−\\hat{y})\n$$\n\n注意，上式只是针对一个节点的计算公式。这一点一定要和单分类 $ loss $ 区分开来。\n\n例子中可以计算为：\n\n$$\n\\begin{aligned}\nloss_{cat}&=−0×log(0.1)−(1−0)log(1−0.1)=−log(0.9)\\\\\nloss_{frog}&=−1×log(0.7)−(1−1)log(1−0.7)=−log(0.7)\\\\\nloss_{mouse}&=−1×log(0.8)−(1−1)log(1−0.8)=−log(0.8)\n\\end{aligned}\n$$\n\n单张样本的 $ loss $ 即为$loss=loss_{cat}+loss_{frog}+loss_{mouse}$\n\n每一个batch的loss就是：\n\n$$\nloss=\\sum_{j=1}^m\\sum_{i=1}^n−y_{ji}log(\\hat{y_{ji}})−(1−y_{ji})log(1−\\hat{y_{ji}})\n$$\n\n式中 $ m $ 为当前batch中的样本量， $ n $ 为类别数。\n\n# **Reference**\n\nhttps://blog.csdn.net/tsyccnh/article/details/79163834\n","tags":["ML"],"categories":["Other"]},{"title":"Basic Knowledge of CNN","url":"/docs/Computer-Vision-The-Basic-Info-of-CNN/","content":"\n简单介绍卷积神经网络中常见的全连接层，卷积层、池化层以及误差反向传播过程和训练优化器的原理。\n\n<!--more-->\n\n# **1 卷积神经网络 CNN**\n\n* 卷积神经网络，即包含卷积层的神经网络。\n* 第一个卷积神经网路：LeCun的LeNet（1998）网络结构\n\n![Net_LeNet](https://github.com/Jiayi-Zeng/Jiayi-Zeng.github.io/blob/pic/img/Net_LeNet_covoer.png?raw=true)\n\n## 1.1 CNN的发展\n\n![](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220327213127062.png)\n\n## 1.2 CNN的应用\n\n图像检测、图像检索、图像检测、图像分割、无人驾驶、GPU、图像迁移\n\n![image-20220327213228684](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220327213228684.png)\n\n![image-20220327213244394](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220327213244394.png)\n\n![image-20220327213324662](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220327213324662.png)\n\n# **2 神经元**\n\n![image-20220403112023201](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220403112023201.png)\n\n$$\ny=f(x_1·w_1+x_2·w_2+x_3·w_3-1)\n$$\n\n其中，$x$为激励，$w$为神经元连接权值，$-1$为偏置，$f(x)$为激活函数。\n\n# **3 全连接层**\n\n![image-20220403113627551](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220403113627551.png)\n\n# **4 BP神经网络**\n\n## 4.1 BP神经网络简介\n\n**BP（Back Propagation）算法包括信号的前向传播和误差的反向传播两个过程**，即计算误差输出时按从输入到输出的方向进行，而调整权值和阈值则从输出到输入的方向进行。\n\n## 4.2 BP神经网络实例\n\n### 1.输入层\n\n1. 彩色RGB图像 --> 灰度化--> 灰度图像 --> 二值化--> 二值图像\n\n![image-20220327214232774](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220327214232774.png)\n\n2. 窗口滑动：计算白色像素占整个框像素的比例\n\n![image-20220327214413416](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220327214413416.png)\n\n3. 按行展开，拼接成**行向量（神经网络输入层）**\n\n![image-20220327214508874](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220327214508874.png)\n\n### 2. 输出层\n\n**one-hot编码**\n\n![image-20220327214704028](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220327214704028.png)\n\n### 3. 神经网络\n\n![image-20220327214728420](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220327214728420.png)\n\n# **5 卷积层**\n\n**目的：**进行图像特征提取\n\n**特性：**拥有局部感知机制，权值共享（减少参数）\n\n> **权值共享**\n>\n> ![image-20220327215001204](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220327215001204.png)\n\n## 5.1 卷积过程\n\n![SouthEast](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/SouthEast.gif)\n\n![image-20220327215117151](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220327215117151.png)\n\n通过观察可以发现：\n\n* **`卷积核channel`与`输入层的channel`相同**\n* **输出的`特征矩阵channel`与`卷积核个数`相同**\n\n## 5.2 激活函数\n\n目的：**引入非线性因素，使其具备解决非线性问题的能力。**\n\n1. **`Sigmoid`激活函数**\n\n* 饱和时梯度非常小，故网络层数较深时易出现梯度消失。\n* 计算多类损失最后使用`softmax`激活函数，经过`softmax`处理后所有输出节点概率和为1。\n\n$$\no_1=\\frac{e^{y_1}}{e^{y_1}+e^{y_2}}\\\\\no_2=\\frac{e^{y_2}}{e^{y_1}+e^{y_2}}\n$$\n\n2. **ReLU激活函数**\n\n* 缺点在于当反向传播的过程中有一个非常大的梯度经过时，反向传播更新后可能导致权重分布中心小于0，导致该处的倒数始终为0，反向传播无法更新权重，即进入失活状态。\n* 失活后无法“复活”。建议一开始不使用较大学习率，否则大多数神经元容易失活。\n\n![image-20220327215434274](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220327215434274.png)\n\n## 5.3 越界情况\n\n利用**padding**补零，经卷积后的矩阵尺寸大小计算公式为：\n\n$$\nN=(W-F+2P)/S+1\n$$\n\n其中，输入图片大小为$W×W$，`Filter`大小为$F×F$，步长为$S$，`padding`的像素数为$P$.\n\n# **6 池化层**\n\n目的：对特征图进行稀疏处理，减少数据运算量\n\n* 没有训练参数\n* 只改变特征矩阵`W`和`h`，不改变`channel`\n* 一般`pool size`和`stride`相同\n\n1. **MaxPooling 下采样层**\n\n![image-20220403114548879](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220403114548879.png)\n\n2. **AveragePooling 下采样层**\n\n![image-20220403114623086](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220403114623086.png)\n\n# **7 反向传播**\n\n## 7.1 误差计算\n\n**Cross Entropy Loss 交叉熵损失**\n\n1. 针对**多分类**问题（最后一层为**softmax**输出，所有输出概率和为1）\n\n$$\nH=-\\sum_io^*log(o_i)\n$$\n\n2. 针对**二分类**问题（最后一层为**sigmoid**输出，输出节点之间相互不相干）\n\n$$\nH=-\\frac{1}{N}\\sum_{i=1}^N[o_i^*log(o_i)+(1-o_i^*)log(1-[o_i)]\n$$\n\n其中，$o_i^*$为真实标签值，$o_i$为预测值，默认$log$=$ln$\n\n## 7.2 误差的反向传播\n\n![image-20220328221513797](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220328221513797.png)\n\n![image-20220328221543152](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220328221543152.png)\n\n![image-20220328221622680](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220328221622680.png)\n\n## 7.3 权重的更新\n\n![image-20220328221732785](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220328221732785.png)\n\n![image-20220328221848547](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220328221848547.png)\n\n# **8 优化器 optimizer**\n\n![image-20220328222050506](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220328222050506.png)\n\n## 8.1 SGD\n\n![image-20220328222130228](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220328222130228.png)\n\n## 8.2 SGD+Momentum优化器\n\n![image-20220328222335145](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220328222335145.png)\n\n## 8.3 Adagrad优化器（自适应学习率）\n\n![image-20220328222425119](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220328222425119.png)\n\n## 8.4 RMSProp优化器\n\n![image-20220328222528324](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220328222528324.png)\n\n## 8.5 Adam优化器（自适应学习率）\n\n![image-20220328222551419](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220328222551419.png)","tags":["CV"],"categories":["Computer Vision"]},{"title":"Real-Time Rendering & DLSS 2.0","url":"/docs/Other-Real-time-Rendering-and-DLSS-2.0/","content":"\n本文是在计算机图形学期末考察的背景下，通过一周的调查并按个人理解整理得出的。虽说文章也是总结的精华，但个人以为还颇有粗糙之处（如有错误欢迎指正）。在此，笔者把参考资源放于文章之前——相比于本文，参考资料在学术上更准确，内容深刻，表达得体；另一方面也是希望大家优先从参考资料下手，从中得出自己的思考，再来笑看这篇多方借鉴的总结。\n\n<!--more-->\n\n# **Reference**\n\n* 如果想要入门或者快速了解一下可以先看B站的科普视频，可以大致了解DLSS是什么，为什么，怎么做。看完视频方便你进一步提出问题，查找资料。\n\t*  [B站：DLSS到底是什么技术？为何能提升游戏性能？有代价吗？](https://www.bilibili.com/video/BV1Dy4y117BM?spm_id_from=333.999.0.0),\n\t* [B站：【硬件科普】免费提升画质和帧数？详解DLSS2.0的工作原理与作用](https://www.bilibili.com/video/BV1PA41187g2?spm_id_from=333.999.0.0)\n* NVIDIA官网介绍：[NVIDIA：DLSS 2.0](https://www.nvidia.com/en-us/geforce/news/nvidia-dlss-2-0-a-big-leap-in-ai-rendering/)\n此外，NVIDA还在GTC上给出了相应的Talk，其中介绍了DLSS 2.0、针对于游戏的图像超分辨率的挑战以及DLSS 2.0引擎集成：NVIDA's Talk：[GTC 2020: DLSS 2.0 - Image Reconstruction for Real-time Rendering with Deep Learning](https://www.youtube.com/watch?v=d5knHzv0IQE)（这是油管上面的，英语听力ok的可以直接冲，英语不太行的可以开中文字母。好像b站也上线了，但还没点进去过，不知道有没有翻译）本文有关于DLSS 2.0的介绍大抵上也出自这个Talk.\n* 同时，DLSS团队成员[文刀秋二](https://www.zhihu.com/people/edliu/posts)也是这个Talk的汇报人在知乎上也对Talk进行了总结，详见：[DLSS 2.0 - 基于深度学习的实时渲染图像重建](https://zhuanlan.zhihu.com/p/123642175)\n* [Beyond3D: Diving into Anti-Aliasing](https://www.beyond3d.com/content/news/798)个人觉得是很全面很有逻辑的抗锯齿的介绍。有需要的朋友可以自取！\n* 书籍：[Real time rendering](https://www.taylorfrancis.com/books/mono/10.1201/9781315365459/real-time-rendering-tomas-akenine-mo%CC%88ller-eric-haines-naty-hoffman) 在实时渲染和计算机图形学领域，《Real-Time Rendering》这本书一直备受推崇。有人说，它实时渲染的圣经。也有人说，它是绝世武功的目录。这次调查主要阅读了本书关于抗锯齿方面的介绍。如有机会可以进军相关领域，还是很期待可以把这本书读一下的。当然毛星云也在CSDN上发布了这本书第三版提炼总结的专栏：[【《Real-Time Rendering 3rd》提炼总结】](https://blog.csdn.net/poem_qianmo/category_9269285.html?spm=1001.2014.3001.5482)，两者可以配合食用。\n* 一篇2021年7月的期刊：[An overview of current deep learned rendering technologies](https://www.webofscience.com/wos/alldb/full-record/INSPEC:20799965) 着重讨论实时渲染和深度学习渲染。其中介绍了实时渲染技术中的抗锯齿和超分辨率，深度学习渲染技术中的DLSS和NSS模型，并且介绍了DLSR技术面临的挑战。本文的思路也从这篇期刊而来。\n\n# **1 前言**\n\n通俗讲，**渲染**（Render)是处理器将需要计算的画面信息，计算并“绘制”在显示屏幕上的过程。随着显示技术的进步，渲染技术也慢慢地出现了两条主流分支：一种用于视频游戏技术，另一种则是用于影视技术。这两类需求对应的渲染技术分别为：**实时渲染与离线渲染**。\n\n**实时渲染**（Real-time rendering）指的是在计算机上快速生成图像。它是计算机图形学中最具交互性的领域。首先一幅图像显示在屏幕上，然后观察者做出动作与反应，并且其动作反馈会影响接下来的生成内容。由于这种反馈、渲染的循环速度足够快，观察者就不会只看到独立的图像，而是会沉浸在这种动态过程中。\n\n由于追求高分辨率和高帧率的真实性体验，RTR技术难度呈指数型上升。显示设备的更新换代以及物理着色、光线追踪、精确物理引擎、更高质量的纹理模型的实现使最新一代 GPU 也难以在不影响帧率的情况下以原始分辨率渲染图像。此时，**低分辨率的性能开销实现高分辨率的画面**成为大势所趋。\n\n<center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/f5622c6950d64a19b46d2bd8a9a10748.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_11,color_FFFFFF,t_70,g_se,x_16#pic_center\"><br>\n    <div style=\"color: #999;\">图1 RTX</div>\n</center>\n\n借助于深度学习超采样，NVIDIA推出DLSS 2.0。其实现了通过渲染更少的像素，使用 AI 构建清晰、分辨率更高的图像。 DLSS2.0由GeForce RTX GPU上的专用 AI 处理器Tensor Cores 提供支持，是一种经过改进的全新深度学习神经网络，可在生成精美、清晰的游戏图像的同时提高帧速。它为游戏玩家提供了最大化光线追踪设置和提高输出分辨率的性能空间。 \n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/809cd174ff5a4637bbe8dfa58f3c55fa.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_20,color_FFFFFF,t_70,g_se,x_16\"> <br>\n    <div style=\"color: #999\">图2 NVIDIA推出DLSS技术</div>\n</center>\n\n本文旨在介绍实时渲染图像重建技术部分基础，同时着重于讨论新兴的基于深度学习的实时渲染重建DLSS 2.0。文章结构如下：**第一部分讨论了实时渲染重建技术基础，包括抗锯齿和超分辨率采样两个方向；第二部分着重讨论DLSS 2.0技术的理论、工作原理和实现效果；第三部分分析DLSS 2.0的优点和缺点，并讨论了DLSR技术面临的挑战和展望。**\n\n# **2 实时渲染图像重建技术基础**\n\n## 2.1 抗锯齿技术\n\n**Aliasing**（锯齿）这个术语最早出现在信号处理这门学科中，指的是当一个连续信号被采样后和其他非一致信号混淆的现象。在3D渲染中，这个术语有着更特殊的意思——它涵盖了所有3D渲染光栅化后产生的画面瑕疵。3D场景渲染在光栅化之前是连续信号，但在进行像素渲染（对每个像素生成相应的色彩值）的时候就不得不对连续信号进行采样以获得能够输出到显示器的结果。反锯齿的目标就在于让最终输出的画面和原生场景尽可能接近，修复渲染瑕疵。\n\n**所有的渲染失真都可以归因于采样问题**（用有限的像素展示无限的细节），使用哪种反锯齿手段与锯齿成因息息相关。因此，为了探讨不同反锯齿手段的优势和劣势，我们先将3D渲染的瑕疵根据其成因简单归纳为6个类别：**几何失真、透明失真、子像素失真、纹理失真、渲染失真、闪烁情形（时间性锯齿）**。\n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/6501b6475d9644f79dc48c0d99094aec.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_12,color_FFFFFF,t_70,g_se,x_16\"><br>\n    <div style=\"color: #999;\">图 3 各种类型的失真。分别为：<br>透明失真、几何失真（2D）、子像素失真、<br>几何失真（3D）、纹理失真、渲染失真</div>\n</center>\n\n现如今的抗锯齿技术可以分为两类：**一类是通过提高采样质量来减少渲染时锯齿，另一类则是通过对已渲染好的图片分析和后处理来减少锯齿。**\n\n### 2.1.1 基于采样的抗锯齿技术\n\n首先讨论基于采样的反锯齿技术，其实质则是通过渲染比屏幕分辨率更高的画面而后再降采样至屏幕空间分辨率。样本数量，样本位置、采样模式和样本融合方式都会影响最终的画面质量。\n\n#### 样本数量\n\n显而易见，倘若生成一个像素的采样点趋近于无穷多，那么最终的效果就会无限趋近“完美”的光栅化效果。因此，抗锯齿的效果和样本数量密切相关。当然样本数量也关系到设备性能：通常在游戏中每个像素会使用2个或4个采样点，而在高端显示器中可能会使用到8个及以上的采样点。\n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/3bbef153efc74eeebc26a7bf68b36446.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_20,color_FFFFFF,t_70,g_se,x_16\"><br>\n    <div style=\"color: #999;\">图 4 三角形光栅化，每个像素有4个有序样本</div>\n</center>\n\n#### 样本位置\n\n1. **顺序栅格超级采样 (Ordered Grid Super-Sampling，OGSS)**\n\n样本位置的选取对最终画面质量有着至关重要的影响。特别是在游戏渲染中，由于采样点数量少，样本位置就更为重要。由于样本位置呈有序点排列，这种抗锯齿也被称作顺序栅格超级采样。\n\n2. **旋转栅格超级采样 (Rotated Grid Super-Sampling，RGSS)**\n\n然而，对于接近垂直或接近水平的线而言，使用排列有序的采样网格效果往往不佳。此时，采用旋转栅格超级采样可以获得更好的结果。\n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/349d5e3fd91d4f06bbb93e08706fdb10.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_15,color_FFFFFF,t_70,g_se,x_16\"><br>\n    <div style=\"color: #999;\">图 5 近垂直线场景、完美抗锯齿光栅化、4个有序样本的光栅化、4个稀疏栅格样本的抗锯齿</div>\n</center> \n\n为了缓解这个问题，我们也可以将采样点稀疏摆放在不同的列。对于抗锯齿来说，理想的摆放应当是稀疏的。换句话说，对于N个采样点，任意两个采样点不会在一个$N\\times N$网格的同一列、行以及对角线上。通过对N皇后问题求解可得到满足这种条件的采样点摆放方式，在此不再赘述。这种稀疏摆放采样点的抗锯齿也被称作**稀疏栅格抗锯齿（Sparse Grid Anti-aliasing，SGAA）**。\n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/2d6763203ca54651992b293a2164e1ed.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_18,color_FFFFFF,t_70,g_se,x_16\"><br>\n    <div style=\"color: #999;\">图 6  左图：有序采样点 右图：N皇后采样</div>\n</center> \n\n#### 采样类型\n\n1. **超采样抗锯齿（Super-sampling Anti-aliasing，SSAA）**\n\n   基于采样的抗锯齿方法对每个采样点都进行了运算。虽然这样的采样方式可以消除各类渲染失真，但也非常耗费资源。举个例子，N倍采样将会给像素渲染、光栅单元、内存带宽以及内存容量施加N倍的计算压力。这种对每个采样点都进行独立计算的采样也被称为超采样抗锯齿。\n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/778a6c9f6e7947c9afff4a06b76f34b4.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_12,color_FFFFFF,t_70,g_se,x_16\"><br>\n    <div style=\"color: #999;\">图 7 2 x SGSSAA</div>\n</center> \n\n\n2. **多重采样抗锯齿（multi-sample anti-aliasing，MSAA）**\n\n   在进入21世纪后，多重采样抗锯齿开始作为SSAA的一种优化解被广泛应用。MSAA实质是只对 Z 缓存（Z-Buffer）和模板缓存 （Stencil Buffer）中的数据进行超级采样抗锯齿的处理。可以理解为只对边缘进行抗锯齿处理。当硬件支持Z缓存和模板缓存时（而现今大部分GPU都已经支持这些特性），MSAA带来的内存带宽开销会进一步缩小。相比SSAA对画面中所有数据进行处理，MSAA大大减弱对资源的消耗。但由于MSAA仅针对几何体边缘进行抗锯齿，其他类别的失真（透明失真、纹理失真和渲染失真等）都无法被消除。\n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/81d2caa0a8c64832b54dd7ce81f1c227.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_19,color_FFFFFF,t_70,g_se,x_16\">\n    <br><div style=\"color: #999;\">图 8 MSAA信息存储和相应的EQAA</div>\n</center> \n\n\n<center>\n   <img style=\"border-radius: 0.3125em;\n   box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n   src=\"https://img-blog.csdnimg.cn/b7046a4f52b44e188e241173ba2c5966.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_12,color_FFFFFF,t_70,g_se,x_16\">\n   <br><div style=\"color: #999;\">\n   图 9 2xMSAA</div>\n</center> \n\n3. **覆盖采样抗锯齿（coverage sampling anti-aliasing，CSAA）**\n\n   第三种采样类型则是NVIDIA在2006年引入的覆盖采样抗锯齿。CSAA在MSAA的基础上还增加了覆盖采样(Coverage Sample)。简单说 CSAA 就是将边缘多边形里需要取样的子像素坐标覆盖掉，把原像素坐标强制安置在硬件和驱动程序预先算好的坐标中。这就好比取样标准统一的MSAA，能够最高效率的执行边缘取样，效能提升非常的显著。比方说16xCSAA取样性能下降幅度仅比4xMSAA略高一点，处理效果却几乎和 8xMSAA一样。8xCSAA有着4xMSAA的处理效果，性能消耗却和2xMSAA相同。\n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/81052231830a4a5f9159826f0fb92178.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_12,color_FFFFFF,t_70,g_se,x_16\">\n    <br><div style=\"color: #999;\">\n    图 10  8x MSAA + alpha-to-coverage</div>\n</center> \n\n#### 样本融合方式\n\n影响采样抗锯齿质量的最后一个要素就是样本融合模式，即如何将采样点加权计算出一个像素值。\n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/34d90a7c43744fd5aae46040bd69f5c1.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_20,color_FFFFFF,t_70,g_se,x_16\">\n    <br><div style=\"color: #999;\">\n    图 11 左图：盒式过滤器，中间：高分辨率抗锯齿方法 右图：Tent filter抗锯齿. 最常见的混合方法是以相同权重混合每个采样点，这被称作Box Filter，也是所有传统MSAA所采用的模式。</div>\n</center> \n\n1. **高分辨率抗锯齿方法 (High Resolution Anti-Aliasing，HRAA)**\n\n一种改进的融合方法被称作高分辨率抗锯齿方法也称 Quincunx 方法，出自 NVIDIA 公司。Quincunx指的是5个物体的排列方式：其中 4 个在正方形角上，第五个在正方形中心，也就是梅花形，很像六边模型上的五点图案模式。这使得采样点个数不多的情况下的抗锯齿效果明显增强，但由于一个像素点过多地混合了周边的采样点信息而使图像边缘变得模糊，带来了画面锐度降低的问题。\n\n2. **可编程过滤抗锯齿（Custom Filter Anti-Aliasing，CFAA）**\n\n一种更灵活的方法则出现于2007年AMD的HD2900系列显卡中，其称为**Tent Filter**。HD2900系列提供了可编程的混合能力，也称为可编程过滤抗锯齿，并借这种工具实现了Narrow Tent和Wide Tent两种模式。如图9所示，这两种新型采样模式在混合采样点时并没有使用相同权重，而是根据采样点离像素中心的距离决定相应的混合权重。Narrow和Wide两种混合模式的区别仅在于其使用的过滤核心(Filter Kernel)的大小上。这种混合模式可以根据需要使用不同样本数量。相较于Quincunx方法，这种抗锯齿模式可以说是平衡了画面锐度和抗锯齿力度。\n\n<center>\n   <img style=\"border-radius: 0.3125em;\n   box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n   src=\"https://img-blog.csdnimg.cn/f9c56223eb1f490b9521a50c7dfc8697.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_20,color_FFFFFF,t_70,g_se,x_16\">\n   <br><div style=\"color: #999;\">\n   图 12 6x Narrow Tent | 6x Wide Tent</div>\n</center> \n\n### 2.1.2 后处理抗锯齿技术\n\n虽然基于采样的抗锯齿算法不仅原理简单，在采样点足够的情况下也有很优秀的效果，但在性能方面仍然会带来巨大的开销。此外基于采样的抗锯齿在近期流行的渲染模式中（例如延迟渲染）基于各种原因更难被实现。由此诞生了另一种非基于采样的抗锯齿方法——后处理抗锯齿。这个方法渲染出未使用抗锯齿的原生画面（无任何采样和缩放），随后尝试通过对成品画面的分析来减少锯齿和失真。\n\n总体而言，所有的后处理抗锯齿都包含了以下三个步骤，而不同的后处理抗锯齿主要区别就在这三个步骤的具体实现方法上。\n\n1. **检测图像中不连续的部分，即检测边缘信息**\n2. **通过这些不连续部分的信息重建原始边缘信息**\n3. **对估测边缘上的像素进行重着色**\n\n**形态抗锯齿（Morphological Anti-Aliasing，简称 MLAA）**，是 AMD 推出的完全基于 CPU 处理的抗锯齿解决方案。例如图10展示了MLAA对边缘的识别和重建方式。左侧是走样图像。我们的目的是确定边缘的可能方向。中间图展示了算法通过检查相邻像素来记录其为边缘的可能性，图中显示了两个可能的边缘位置。右侧图则展示使用了最佳的推测边缘后，将相邻的颜色与估计的覆盖率成比例地混合到中心像素中。\n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/083100ab8f5d45f99fc4717e34186570.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_20,color_FFFFFF,t_70,g_se,x_16\">\n    <br><div style=\"color: #999;\">\n    图 13 形态学抗锯齿</div>\n</center> \n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/ffee10ce178c4057b5427684ed580f76.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_16,color_FFFFFF,t_70,g_se,x_16\">\n<br><div style=\"color: #999;\">图 14 左图：MLAA 模式及其重建边缘，右图：MLAA 中使用的不连续模式</div></center> \n\n### 2.1.3 混合抗锯齿算法\n\n了解了两大类抗锯齿技术方向后，我们会去思考现有流行的抗锯齿技术是如何弥补传统算法的遗憾。\n\n**一种可行的解决方案是将基于采样的抗锯齿算法和后期处理抗锯齿技术结合起来。**这种新的混合抗锯齿算法在对画面进行多次采样的同时，会结合后期处理抗锯齿算法以输出最终画面。这样做的好处是显而易见的：新算法既避免了纯后期处理抗锯齿的种种缺点(比如说不能处理子像素失真和造成边缘闪烁的问题)，在同样的性能损失下，对于几何失真的处理结果又比纯粹基于采样的抗锯齿算法好得多。\n\n1. **快速近似抗锯齿（Fast Approximate Anti-Aliasing，FXAA）**\n\n  快速近似抗锯齿是传统 MSAA效果的一种高性能近似。它是一种单程像素着色器，和 MLAA一样运行于目标游戏渲染管线的后期处理阶段，但不像后者那样使用 DirectCompute，而只是单纯的后期处理着色器，不依赖于任何GPU计算 API。正因为如此，FXAA技术对显卡没有特殊要求，完全兼容 NVIDIA、AMD 的不同显卡(MLAA仅支持AMD)和 DirectX 9.0、DirectX 10、DirectX 11。\n\n2. **时间性抗锯齿（Temporal Anti-Aliasing，TXAA）**\n\n  时间性抗锯齿将MSAA、时间滤波以及后期处理相结合，用于呈现更高的视觉保真度。与CG电影中所采用的技术类似，TXAA集MSAA的强大功能与复杂的解析滤镜于一身，可呈现出更加平滑的图像效果。此外，TXAA还能够对帧之间的整个场景进行抖动采样，以减少闪烁情形（时间性锯齿）。目前，TXAA有两种模式：TXAA 2X和 TXAA 4X。TXAA 2X可提供堪比8X MSAA的视觉保真度，然而所需性能却与 2X MSAA相类似；TXAA4X的图像保真度胜过8XMSAA，所需性能仅仅与4X MSAA相当。\n\n3. **多帧采样抗锯齿（Multi-Frame Sampled Anti-Aliasing，MFAA）**\n\n  多帧采样抗锯齿（Multi-Frame Sampled Anti-Aliasing，MFAA）是NVIDIA公司根据MSAA 改进出的一种抗锯齿技术。目前仅搭载Maxwell架构GPU的显卡才能使用。可以将MFAA理解为MSAA的优化版，能够在得到几乎相同效果的同时提升性能上的表现。MFAA与MSAA最大的差别就在于在同样开启4倍效果的时候MSAA是真正的针对每个边缘像素周围的 4 个像素进行采样，MFAA则是仅仅只是采用交错的方式采样边缘某个像素周围的两个像素。\n\n另一种可行的方案还未被广泛应用：**在渲染时记录额外的几何信息，以供后处理抗锯齿使用。**目前的实现有GPAA (Geometric Post-process Anti-Aliasing) 以及GBAA(Geometry Buffer Anti-Aliasing)等。\n\n## 2.2 超分辨率技术\n\n图像分辨率体现了系统实际所能反映物体细节信息的能力。相较于低分辨率图像，高分辨率图像通常包含更大的像素密度、更丰富的纹理细节及更高的可信赖度。由此，从软件和算法的角度着手，实现图像超分辨率重建的技术成为了图像处理和计算机视觉等多个领域的热点研究课题。\n\n图像的超分辨率重建技术指的是将给定的低分辨率图像通过特定的算法恢复成相应的高分辨率图像。超分辨率方法通常分为**单帧超分辨率（Single Image Superresolution，SISR）和多帧超分辨率（Multi-image Superresolution，MISR）、时域超采样（Temporal Super Sampling）**。\n\n### 2.2.1 单帧超分辨率\n\n单帧超分辨率是和DLSS非常相关的一个研究方向。尤其是跟着这两年深度学习的应用的热度，这个问题的state of the art也提高了很多，这个方向的研究也经常上国内公众号的头条，例如SRCNN，SRGAN，ESRGAN等等。\n\n可是单帧超分辨率其实是个非常困难的问题，因为本质上需要生成低分辨率图像中完全不存在的信息。用神经网络解决这一类问题，**本质上就是在训练集中学习到各种低分辨率的像素和高分辨率像素的一个对应关系。**有了这个映射后，神经网络能做到比一般的基于插值（interpolation）的方法更好的效果。\n\n<center>\n   <img style=\"border-radius: 0.3125em;\n   box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n   src=\"https://img-blog.csdnimg.cn/a89f7614853a4ff7aa2ecff3c609bc9b.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_20,color_FFFFFF,t_70,g_se,x_16\">\n   <br><div style=\"color: #999;\">图 15 单帧超分辨率</div></center> \n\n**但尽管如此，这样生成出来的信息其实是完全基于训练集图片中的数据分布，而并不是对我们实际正在渲染的场景的采样。**所以单帧超分辨率的结果经常会和原生分辨率渲染在风格和样式上不一致。对于DLSS来说，我们的目标是重建出和高分辨率渲染一模一样的结果，所以单帧超分辨率一类的工作对实时渲染来说很难适用。\n\n### 2.2.2 多帧超分辨率\n\n另一类超采样的工作则是针对视频，或者手机摄影的多帧超分辨率。多帧超分辨率并不像单帧超分辨率那么的困难，因为我们不完全需要填补原本不存在的信息。有多个低分辨率图片的情况下，这个问题会可控很多，多帧合成的高分辨率图片往往在光学细节上的还原的质量上会比单帧超分辨率的高许多。\n\n然而针对视频或者摄影的算法也并不太能直接搬过来用于实时渲染，原因有许多。\n\n1. 在渲染中，我们可以用到的数据是比视频多很多的，我们可以计算每个像素精确的运动向量，我们也可以有场景函数的精确采样，有HDR颜色，甚至像素的精确深度。**不利用这些信息，设计出来的算法在效率和质量上都不会是最优的。**\n\n2. 许多视频超分辨率的工作是需要**用时序上未来的图片来重建当前帧的图片的**。因为实时渲染对延迟的要求，这也显然不适用。\n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/c7ba067031a4440baefb6e0d02c61e5e.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_20,color_FFFFFF,t_70,g_se,x_16\">\n    <br><div style=\"color: #999;\">图 16 多帧超分辨率</div></center> \n\n### 2.2.3 时域超采样\n\n利用把渲染的样本分布在多个帧上，并且用这些多帧复合的样本重建出最终渲染的图片，这在实时渲染领域太司空见惯了。几乎所有引擎都在用的Temporal Antialiasing(TAA)，或者游戏主机上非常流行的Checkerboard Rendering都是这么一个思路。\n\n<center>\n   <img style=\"border-radius: 0.3125em;\n   box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n   src=\"https://img-blog.csdnimg.cn/3dae0dd648e44c07b6eeaa4da296d74f.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_20,color_FFFFFF,t_70,g_se,x_16\">\n   <br><div style=\"color: #999;\"> 图 17 棋盘渲染</div>\n</center> \n\n这一类算法利用了渲染图片的**时域连贯性（Temporal coherency）**，既渲染结果的帧与帧之间大体是连续的，发生高频变化的概率不高。也就是说，我们可以假设需要渲染的场景在上一帧和当前帧几乎一样。如果这个假设成立的话，我们大可以之间复用过去帧上对场景采样的样本来重建当前帧。\n\n<center>\n   <img style=\"border-radius: 0.3125em;\n   box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n   src=\"https://img-blog.csdnimg.cn/50ae89e4ff2f4e0d86364167110377af.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_17,color_FFFFFF,t_70,g_se,x_16\">\n   <br><div style=\"color: #999;\">图 18 TAA</div>\n</center> \n\n这样做的好处是，每一帧的采样率非常低，所以渲染性能会有很大的提升，然而重建图像时的样本还都确实是对于场景函数的无偏采样，所以最终重建的图像质量也会和原生分辨率渲染非常一致。\n\n然而天下哪有这等好事，实时渲染或者游戏的图片序列中几乎每一帧都有或多或少的变化，从角色动画，到动态光影，到粒子特效。直接复用过去帧的样本来重建当前帧的图片会使重建的结果中产生很大的错误。**这种错误在渲染图片中会以延迟，或者鬼影（ghosting）的形式呈现出来。**这也是为什么，所有实时渲染中的时域超采样算法，例如TAA，都有非常重要的一步去“纠正”过去帧中样本的错误。\n\n<center>\n   <img style=\"border-radius: 0.3125em;\n   box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n   src=\"https://img-blog.csdnimg.cn/8395216147f04632a142ba5f02cfcb52.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_14,color_FFFFFF,t_70,g_se,x_16\">\n   <br><div style=\"color: #999;\">图 19 鬼影现象</div>\n</center> \n\n这一类算法需要首先检测过去帧和当前帧因为场景的变化导致的样本错误，然后在不影响画质太多的情况下，“合理”的纠正那些错误的样本。乍一看这简直是个计算机视觉问题，然而在实时渲染中这一步需要非常高效的完成。所以过去十几年，游戏开发者绞尽脑汁的发明了各种**“启发式”的方法（Heuristics）**。\n\n目前解决这个问题效果最好，也最常用的Heuristic，叫做**Neighborhood Clamping**，是Epic的Brian Karis在14年的SIGGRAPH的一个实时渲染讲座里最先提到的。思路其实很简单，就是把过去帧采样的样本的值的范围，限制在当前帧像素周围3x3大小的Local neighborhood的所有样本的值的范围内。\n\n# **3 DLSS 2.0技术介绍**\n\n## 3.1 DLSS 2.0原理与思路\n\nDLSS 2.0首先也是一个基于多帧的图像重建技术。因为我们的目标是重建出和原生渲染一致的画面，所以基于用单帧的算法去“想象”不存在的信息是不适用的。\n\n那DLSS 2.0和现有的实时渲染中的时域超采样有什么区别呢？DLSS 2.0抛弃了人肉手调的启发式算法，用一个在超级计算机上数万张超高质量图片训练的神经网络来代替这些Heuristics。就像深度学习在几年前在计算机视觉领域超越了各种手调的特征提取算法一样，深度学习第一次在实时渲染中也非常合理的跑赢了图形领域的手调算法。\n\n用DLSS 2.0重建的渲染图像序列达到了非常高的多帧样本利用率，这也是为什么只用四分之一的样本就可以重建出媲美原生分辨率渲染的图像质量的原因。\n\n<center>\n   <img style=\"border-radius: 0.3125em;\n   box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n   src=\"https://img-blog.csdnimg.cn/02404b8cfec84daab16a1f45208ebbd8.png\">\n   <br><div style=\"color: #999;\">图 20 DLSS重建方法</div>\n</center> \n\n下图是DLSS 2.0的粗略架构：\n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/cbb1df0e4c1c4ce1a9eef04f984b5a88.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_20,color_FFFFFF,t_70,g_se,x_16\">\n    <br><div style=\"color: #999;\">图 21 DLSS 2.0架构图</div></center> \n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/ea9e3becdfcc4a36aab12011303e20c4.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_17,color_FFFFFF,t_70,g_se,x_16\">\n    <br><div style=\"color: #999;\">图 22 DLSS 2.0 神经网络 输入及输出</div></center> \n\n\n## 3.2 DLSS 2.0引擎集成\n\nDLSS 2.0并不是一个单纯的图像超分辨率算法。它是一个专门针对实时渲染应用的算法。所以引擎要集成DLSS 2.0，需要配合的作出相应的改动。但所幸改动的幅度远比类似Checkerboard rendering简单。\n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/d9402c34f41d477583a316ff71201788.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_20,color_FFFFFF,t_70,g_se,x_16\">\n    <br><div style=\"color: #999;\">图 23 DLSS引擎集成</div></center> \n\n\n**首先当然是引擎要把所有的像素着色工作在低分辨率执行，**通常这些包括GBuffer渲染，动态光影，屏幕特效，光线追踪等。\n\n**其次，DLSS 2.0是一个融合了抗锯齿和超采样的算法**。引擎现有的抗锯齿解决方案例如TAA需要被移除，然后DLSS需要被插入在后处理（post processing）之前，这样后处理可以处理抗锯齿后的平滑图片以避免各种artifact。\n\nDLSS的输出会是一个高分辨率的图片，所以**引擎需要超采样后的分辨率下计算各种后处理特效**，例如景深，动态模糊，tonemapping以及渲染UI。\n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/da4bfb6c94204f45bf40e13979f0e309.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_18,color_FFFFFF,t_70,g_se,x_16\">\n    <br><div style=\"color: #999;\">图 24 DLSS 2.0 渲染步骤</div></center> \n\n## 3.3 DLSS 2.0渲染加速\n\nDLSS 2.0加速渲染的原理很简单。开启DLSS后，引擎的渲染会在1/2到1/4像素的低分辨率下运行。这意味着，一大半的像素级别的计算直接被粗暴的砍掉了。像素级别的计算通常包括GBuffer的渲染，动态光源、阴影的计算，屏幕空间的特效例如屏幕空间环境遮挡（SSAO）、屏幕空间反射（SSR），甚至实时光线追踪。这些计算通常也是一帧里面最耗费性能的部分、毕竟大部分的画面出色的游戏，像素计算是绝对的瓶颈（pixel bound）。\n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/fc5f475f3afa49e6b7a553aa3b5f58c1.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_20,color_FFFFFF,t_70,g_se,x_16\">\n    <br><div style=\"color: #999;\"> 图 25 DLSS 渲染加速</div></center> \n\n\n所以DLSS 2.0的加速多少，也**直接取决于像素计算在多大程度上是性能瓶颈。**通常来说，画面越好的3A大作，越会用更加耗费性能的渲染技术，像素也就会更大程度的成为瓶颈，而DLSS则会提供更大的加速！\n\n在省掉的渲染计算之上，运行DLSS 2.0这个算法本身会引入一定的开销，这个开销通常是完全取决于分辨率大小的，不随场景内容而改变。下面这个表格展示了DLSS 2.0在不同GPU和不同分辨率下的开销。相比于DLSS 1.0，我们把这个开销减小了两倍以上。在2080Ti上，4K分辨率下也只有1.5毫秒，因为有Tensor Core的加速，这已经和普通的TAA非常接近了。\n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/1a58dfc0348741b0861f2ebbec493c72.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_20,color_FFFFFF,t_70,g_se,x_16\">\n    <br><div style=\"color: #999;\">图 26 DLSS cost</div></center> \n\n# **4 DLSS挑战与总结**\n\n## 4.1 DLSS效果对比\n\nDLSS 2.0可以将540p的渲染图像直接放大到1080p，或者720p到1440p，1080p到4K。并且放大的画面在质量以及细节程度完全不输原生分辨率渲染，网上的许多测评也都反映了这一点。\n\n下面放几组例子，第一个是一个几何非常密集的森林场景，开启实时光线追踪后540p原生分辨率渲染大概有89fps，但是因为分辨率太低，画面非常模糊。\n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/b571906c1f2a4e6c895085427ef1cae0.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_14,color_FFFFFF,t_70,g_se,x_16\">\n    <br><div style=\"color: #999;\"> 图 27 540p - 89fps原生渲染效果图</div></center> \n\n如果1080p渲染，画面则清晰了很多，但是帧率也降低到48fps\n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/df3b877917694856b7f2ef7a16362116.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_14,color_FFFFFF,t_70,g_se,x_16\">\n    <br><div style=\"color: #999;\">图 28 1080p – 48fps原生渲染效果图</div></center> \n\n使用DLSS2.0, 用540p分辨率渲染的画面作为输入，通过深度学习超采样至1080p，帧率提升到86fps，并且画质和原生十分接近。\n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/8b12156c24f64cd69ead4edacf8a299e.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_14,color_FFFFFF,t_70,g_se,x_16\">\n    <br><div style=\"color: #999;\">图 29 DLSS 2.0，540p渲染输入超采样至1080p，86fps</div></center> \n\n但如果放大看的话，可以发现DLSS2.0的结果和原生1080p还是有一些差别，那么为了验证正确性，下面这个对比的左下角是每个像素用32个样本渲染的ground truth。很明显DLSS 2.0用在540p下渲染的结果，比1080p的原生渲染更接近ground truth！\n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/b7e0d43a924e45fe91b6bbb6810a381c.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_17,color_FFFFFF,t_70,g_se,x_16\">\n    <br><div style=\"color: #999;\">图 30 200%对比</div></center> \n\n## 4.2 DLSS 2.0的优缺点\n\n### 4.2.1  DLSS 2.0 四大特性\n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/a743d2987f5f431e84c11123fceaf66a.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_20,color_FFFFFF,t_70,g_se,x_16\">\n    <br><div style=\"color: #999;\">图 31 DLSS 2.0四大特性</div></center>\n\n1. 画质极大提升，细节和锐度媲美、甚至超越原生分辨率\n2. 4倍像素超采样（540p到1080p，1080p到4K，每4个像素中有3个是通过超采样生成）\n3. 通用模型，一个神经网络适用于所有游戏（不同引擎，着色风格，分辨率都有很强的通用性）\n4. Inference开销减半\n\n### 4.2.2 DLSS缺点\n\n1. DLSS似乎不能很好地与某些AA技术(如TSAA)一起工作，当启用这些技术时，DLSS性能会受到严重影响。\n2. 此外，由于DLSS只能工作在张量核的GPU上，所以CUDA-only和Stream处理器的gpu不能实现DLSS。\n\n## 4.2.3 DLSS技术的展望\n\n1. **更简单的 SR 网络架构**\n\n虽然 DLSR 模型在图像重建方面取得了很高的准确率和保真度，但在本地部署仍然是极其困难且耗时的。由于拥有大量的计算成本和网络训练相关参数，DLSR需要大型数据中心或超级计算机。为了解决这个问题，需要降低空间和时间复杂度，降低计算成本，减少参数数量，还需要将图像质量保持在可接受范围内。\n\n2. **更有效的算法来补偿信息丢失**\n\nDLSR 模型的主要工作原理是从低分辨率输入重建高分辨率图像。但是，以非常低的内部分辨率（例如 540p）进行渲染，或者运行非常高的放大操作时（例如从 1080p 缩放到 8K），对于重建而言，此时的缺失数据量变得太大。这通常会导致错误表达或不准确的视觉数据。为了解决这个问题，需要努力使预测算法从更少的像素中更有效地提取视觉信息。\n\n3. **专注于更广泛的实施和支持**\n\nDLSR 是一项非常新的技术，仍处于起步阶段。因此，目前只有少数应用程序提供对 DLSR 的实现支持。需要完成工作并需要构建相关的 API 以扩展对更多应用程序的支持，并且必须为开发人员创建软件工具以实现更快的增长。\n\n# **Postscript：总结**\n\n调查和攥写报告前前后后花了一周时间。前期也是随心所欲的查文献看资料写md，后期三天狂肝出小20页的报告还是相当痛苦的一件事……确实时间安排相当不合理（拖延症你又来啦），但更多的时间我认为是花费在捋思路上。在查阅资料过程中，我一直在尝试摸清内在的逻辑链——可惜思路一直被推翻重建，文章结构也一改再改，直到ddl前的最后一天的我基本满意。\n\n总体来说，收获还是相当大的，调研调查本身就是一件令人兴奋的事——可以自己思考内在联系，提出问题，再去解决……这也是我第一次看这么多的外文材料（原先对英文文献有些畏惧心理，一直没有迈出这一步，或者说没迈几步）。\n\n我最感兴趣或者最想从事的方向之一就是游戏开发（一个爱玩游戏的人也想去做游戏）。原先我多多少少低估了这个领域的难度，但现在更多地我对这个方向产生了敬畏之情。当然打动我的还有很多这个领域的前辈们。当我研究《Real time rendering》，翻看毛星云的总结提炼时，我佩服着他的热爱，也惋惜着他的离开；当我浏览文刀秋二的[知乎回答](https://www.zhihu.com/question/29504480/answer/44764493)时，我也是深深佩服的——我认为我对coding是很感兴趣的，以至于每次课设和项目我很多时间都花在学习新东西加上去，以做得好些，更好些。但当我发现大神的\"兴趣\"后，我也意识到，我还有很长的一段路要走：\n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/41c45a32be944723964575606a6fb82d.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_20,color_FFFFFF,t_70,g_se,x_16\"\n    width=\"50%\">\n</center>\n\n最后再来谈谈这篇文章，个人感觉更多的还是在学习摘抄借鉴。写绪论的时候还在手敲，附引用；写抗锯齿部分的时候还能整合多份资料，翻译网站文献，再做校对；等写到DLSS时就开始复制粘贴了……总体而言，更像是多家资料的大合集，勉勉强强一个优点就是有自己的思考在内。当然，如有错误和问题，欢迎探讨。道阻且长，这是一次把报告整理至CSDN上，相信不会是最后一次！\n","tags":["DL","CG"],"categories":["Other"]}]