[{"title":"3 Linear Programming","url":"/docs/Prescriptive-Analytics-3-Linear-Programming/","content":"\n# **Linear Programming**\n\nLinear programming (LP, also called linear optimization) is a method to achieve the best outcome (such as maximum profit or lowest cost) in a mathematical model whose requirements are represented by linear relationships.\n\nLinear programming is a technique for the optimization of a linear objective function, subject to linear equality and linear inequality constraints.\n\nLinear programs are problems that can be expressed in canonical form\n\n* Find a vector $x$\n* That maximizes $c^Tx$\n* Subject to $Ax \\leq b$\n* And $x\\geq0$\n\nLinear programming is used in business and industry in\n\n* Production planning, \n* transportation and \n* Routing, and \n* Various types of scheduling\n* Airlines use linear programs to schedule their flights, considering both scheduling aircraft and scheduling staff\n\n# **Optimization Problem**\n\n$$\n\\begin{align}\nMaximize :& 10 x_1+5x_2\\\\\nConstraints :& 5x_1+x_2 \\leq90\\\\\n& x_1+10x_2\\leq300\\\\\n& 4x_1+6x_2 \\leq 125\\\\\n& x_1, x_2 \\geq 0\n\\end{align}\n$$\n\n![image-20230307093829700](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230307093829700.png)\n\n## PuLP Package\n\n```python\nfrom pulp import *\n```\n\n**Define variables**\n\n```python\nlp = LpProblem(\"Bakery_Problem\", LpMaximize)\nx1 = LpVariable(name=\"Bowdoin_log\", lowBound=0)\nx2 = LpVariable(name=\"Chocolate_cake\", lowBound=0)\n```\n\n**Add the objective function**\n\n```python\nlp += 10 * x1 + 5 * x2\nprint(lp.objective)\n```\n\n```\n10*Bowdoin_log + 5*Chocolate_cake\n```\n\n**Add the constraints**\n\n```python\nlp += (5 * x1 + x2 <= 90, \"oven\")\nlp += (x1 + 10* x2 <= 300, \"food_processor\")\nlp += (4 * x1 + 6 * x2 <= 125, \"boiler\")\n```\n\n**Solve the LP**\n\n```python\nstatus = lp.solve()\n```\n\n**Print solution**\n\n```py\nfor var in lp.variables():\n  print(var, \"=\", value(var))\nprint(\"OPT=\", value(lp.objective))\n```\n\n**Output:**\n\n```\nBowdoin_log = 15.961538\nChocolate_cake = 10.192308\nOPT= 210.57691999999997\n```\n\n","tags":["DS"],"categories":["Prescriptive Analytics: Digital Decisioning"]},{"title":"2 Digital Decision Making: Theory, Process, & Practice","url":"/docs/Prescriptive-Analytics-2-Digital-Decision-Making-Theory-Process-and-Practice/","content":"\n# **Introduction**\n\nDecision making can be spontaneous (è‡ªå‘çš„) or systematic. For personal and social matters, spontaneous and haphazard (èƒ¡ä¹±çš„) decisions typically work fine, but for professional and business-related matters, one should follow a scientific and standardized process. Such a process helps in implementing best practices, promoting the use of data and information (as opposed to opinions and gut feelings ç›´è§‰), and hence, creating accurate and consistent outcomes. \n\n# **Concept(s) Overview** \n\n## Overview of the Human Decision-Making Process\n\nSimon (1977) said that such **a systematic process involves three major phases: *intelligence*, *design*, and *choice*. He later added a fourth phase (*implementation*). *Monitoring* can be considered a fifth phase (i.e., a form of feedback);** however, we view monitoring as part of the â€˜intelligenceâ€™ phase as applied to the â€˜implementationâ€™ phase as weâ€™ll see below. Simon's model is widely accepted as the most concise (ç®€æ´ï¼‰, and yet most complete, characterization (è¡¨å¾) of rational decision making. A conceptual picture of this decision-making process is shown in Figure 1.\n\n![The decision making/modeling process starts with real world issues or problems. These issues are considered in the first 'intelligence' phase where some kind of problem statement is produced. This statement leads to the 'design' phase or solution phase. Alternatives are considered and eventually a decision is made in the 'choice' stage leading to the 'implementation' phase. Once the decision is implemented the cycle moves back to the real world.](https://lh4.googleusercontent.com/4e-dWnGfX1r9mIs7UcuwLkWlXHdhuP0gPvEnVUpnpyGN0eidcmgn5_Z2_tvOQGAmMX1UbNSn-5eYBoU4b1DPbAUBzEGngOH-hUq1Y8oODFQEjpUn4ymzE-s-bEi4M67yM1oQ1rlz_eQhJr-YVT-vWQ)\n\n*Figure 1: The decision-making/modeling process*\n\n### Intelligence\n\nThe *intelligence* phase in the decision-making process involves **scanning the environment**, either intermittently or continuously (é—´æŽ¥æ€§æˆ–æŒç»­æ€§). It includes several activities aimed at identifying problem situations or opportunities. As mentioned above, it may also include monitoring the results of the implementation phase of a decision-making process.\n\n### Design\n\nThe *design* phase involves finding or developing and analyzing possible courses of action. These include **understanding the problem and testing solutions for feasibility.** A model of the decision-making problem is constructed, tested, and validated. Modeling involves conceptualizing (æ¦‚å¿µåŒ–) a problem and abstracting it into quantitative (å®šé‡) and/or qualitative (å®šæ€§) form. For a mathematical model, the variables are identified, and their mutual (ç›¸äº’çš„) relationships are established.\n\n### Choice\n\nChoice is the critical act of decision making. The *choice* phase is the one in which the actual decision and the commitment to follow a certain course of action are made. The boundary between the design and choice phases is often unclear because certain activities can be performed during both phases and because the decision maker can return frequently from choice activities to design activities (e.g., generate new alternatives while evaluating existing ones). The choice phase includes **searching for, evaluating, and recommending an appropriate solution to a model**. A solution to a model is a specific set of values for the decision variables in a selected alternative.\n\n### Implementation\n\n*Implementation* is the actual deployment of the choice. The implementation of a proposed solution to a problem is the initiation of a new order of things or the introduction of change. Change must be managed, e.g., user expectations must be managed as part of change management. The definition of implementation is somewhat complicated because implementation is a long and involved process with vague boundaries. Simply put, the implementation phase involves putting a recommended solution to work. Many generic implementation issues, such as resistance to change, degree of support from top management, and the need for user training are important in dealing with managerial decisions.\n\n## CRISP-DM: A Standards Process for Analytics\n\nThe *CRISP-DM* (Cross-Industry Standard Process for Data Mining) methodology consists of a cyclical sequence of phases commonly used by data mining experts for traditional business intelligence (BI) data mining and data strategy development. Figure 2 summarizes this cycleâ€” which is dynamic, features bidirectional movement between the six phases, and entails multiple iterations. In the diagram below, please note that:\n\n- The iterative CRISP-DM process is shown in the outer circle.\n- The most significant dependencies between phases are shown.\n- Subsequent phases depend on results from the preceding phases.\n\n*![A diagram showing the relationship between the 6 different phases of CRISP-DM. It illustrates the recursive nature of a data mining project.](https://lh3.googleusercontent.com/eW3nJAycOY0La88ygPNRaguPEKFv47vJiXq-xkx57F5xV1AP-nLuqrxbVWfCVcMlqXibc4a0bEusCe0GiVv0K6ZXmb7d-C4STlUuzQiLrKcH8Z86PGuQoLNp2sTioAzuqbGFyvemP-rbqLe3BAHYhQ)*\n\n*Figure 2: The CRISP-DM process. (Source: Kenneth Jensen,* [*CC BY-SA 3.0*](https://creativecommons.org/licenses/by-sa/3.0)*, via Wikimedia Commons)* \n\n### The Six-Step Process to Guide Analytics Thinking and Problem Solving\n\nIn the [CRISP-DM 1.0 data mining guide](https://the-modeling-agency.com/crisp-dm.pdf), Chapman et al. (2000) lay out specific activities that can guide analytics thinking for each phase, as highlighted below:\n\n#### **Phase 1: Business/Research Understanding**\n\n- Define project requirements and objectives\n- Translate objectives into a data mining problem definition\n- Prepare a preliminary strategy to meet the objectives\n\n#### **Phase 2: Data Understanding** \n\n- Collect data\n- Perform exploratory data analysis (EDA)\n- Assess data quality\n- Select interesting subsets (*optional*)\n\n#### **Phase 3: Data Preparation** \n\n- Prepare for modeling in subsequent phases\n- Select cases and variables appropriate for analysis\n- Cleanse and prepare data so it is ready for modeling tools\n- Perform transformation of certain variables, if needed\n\n#### **Phase 4: Modeling** \n\n- Select and apply one or more modeling techniques\n- Calibrate model settings to optimize results\n- Prepare additional data, if required, to support a particular technique\n\n#### **Phase 5: Evaluation** \n\n- Evaluate one or more models for effectiveness\n- Determine whether defined objectives are achieved\n- Establish whether any important facet of the problem has not been sufficiently accounted for\n- Make a decision regarding data mining results before deploying to the field\n\n#### **Phase 6: Deployment**\n\n- Make use of models created\n- Generate the report (simple deployment example)\n- Implement a parallel data mining effort in another department (complex deployment example)\n\n*Note*: In some businesses, the customer often carries out the deployment based on the model delivered by the analytics team.\n\n# **Mathematics for Prescriptive Analytics**\n1. Linear Programming\n2. Non-Linear Programming\n3. Genetic Algorithm\n4. Simulation\n\n","tags":["DS"],"categories":["Prescriptive Analytics: Digital Decisioning"]},{"title":"1 Overview of Prescriptive Analytics","url":"/docs/Prescriptive-Analytics-1-Overview-of-Prescriptive-Analytics/","content":"\n# **Introduction**\n\n*Prescriptive analytics* is a part of the business analytics continuum and is also the closest to the decision-making phase. Business analytics is often characterized by three consecutive levels/echelons, i.e., descriptive analytics, predictive analytics, and prescriptive analytics. Prescriptive analytics is the highest level in business analytics and the main focus of this course.\n\nIn general terms, business analytics is the art and science of identifying or discovering novel patterns and insights from large volumes and varieties of data utilizing sophisticated machine learning, mathematical, and statistical models to support more accurate and timely managerial decision making (Delen, 2019). Therefore, business analytics is widely perceived as synonymous with managerial decision making and business problem-solving. Figure 1 shows the process of creating information and knowledge through a systematic and scientific transformation of data which leads to making better decisions and, ultimately, achieving â€œwisdom.â€\n\n![Shows the process of converting data into knowledge and wisdom. Data comes in and is processed or summarized. This processing/summarization makes it \"information.\" As this information is determined to be relevant and actionable, it becomes \"knowledge.\" This knowledge will ultimately lead to better decision making and wisdom.](https://lh6.googleusercontent.com/J4TGR2WkAWGORqjleLPp_dtKqO4s-DLAG0Rurrriv9QQO8PupU76MhfVS4B-WTxYq_SlXpN8QQnL0_a7R0Kd2PUNiQ7JgSJgo5WSAnIu4IR-WS3eH3aIzofQBQ8lbpKsBKRGpGCn3V04qBQL4IvzUA)\n\n*Figure 1: The process of converting data into knowledge and wisdom*\n\nAs seen in this figure, various data sources (both structured and unstructured) are converted into mathematical representations (i.e., knowledge models) through a scientific process we now call business analytics.\n\n# **Key Terms**\n\nBefore we continue, several key terms will provide a foundation for our discussion: \n\n- **Business Analytics**\n  - Descriptive analytics\n  - Predictive analytics \n  - Prescriptive analytics\n- **Nature of Data**\n  - Structured data (numeric and non-numeric)\n  - Unstructured data (textual and multimedia data) \n- **Operations Research**\n  - Optimization\n  - Simulation\n  - Heuristics\n  - Multi-criteria decision making\n\n# **Analytics and a Simple Analytics Taxonomy**\n\nThe three commonly-used types/echelons in business analytics (i.e., descriptive, predictive, and prescriptive analytics) provide a structured and comprehensive depiction of the analytics maturity process for organizations (Figure 2). Some sources include *diagnostic analytics* as a fourth layer/echelon between descriptive and predictive, but most often, this layer of analytics is included as an extension of descriptive analytics. A short description of these analytics echelons, and the questions they are aimed at answering, are given below:\n\n- **Descriptive**: *What happened?* Helps to uncover valuable insight into the data being analyzed via the use of on-demand reporting and data visualization.\n- **Diagnostic**: *Why did it happen?* Helps to identify and understand the causal relationships and related patterns in the data.\n- **Predictive**: *What is likely to happen in the future?* Helps to forecast the future behavior of people and markets using statistical and machine learning methods.\n- **Prescriptive**: *What should I do about it?* Now that I know what happened in the past and what might happen in the future, what decisions should I make? Uses operations research methods (optimization, simulation, heuristics, and multi-criteria decision modeling) to provide guidance and understanding on decision alternatives.\n\n*![An x-y axis chart which depicts the progressive nature of the four types of analytics: descriptive, diagnostic, predictive, and prescriptive. The X-axis shows the progression of computational sophistication, and the Y-axis shows the progression of value proposition. Descriptive, diagnostic, and predictive analytics are information and insight focused, whereas prescriptive analytics is decision focused. ](https://lh3.googleusercontent.com/HcCjvjdeHHqtzl3PwpfUWEJV7VTNwmCT8KTTvCPfwHYV4uFayJVlEh9nRNPRS7l6s3DuNwI1RJHklQytT7QFybPyALd6RQOMbWb3ACArfwW1VGZiQ599CCUICuNVKShiBeVXbK7ovs7TDI4QYPG_lA)*\n\n*Figure 2: The progressive nature of the four types of analytics*\n\n# **The Reason behind Analytics Popularity**\n\nAccording to Delen (2021), the reasons behind the popularity of business analytics can be grouped into three main categories, as follows:\n\n- **Need for better decisions**: Conducting business is no longer â€œas usual.â€ Competition has progressively transformed from local to regional, to national, and now, to global. The protections created by tariffs and logistics costs (that sheltered companies into their geographic regions) are no longer as prominent as before. In addition to increased global competition, and perhaps partially because of it, customers have also become more demanding, i.e., asking for the highest quality of products and services, offered at the lowest prices, and delivered quickly. Therefore, data-driven, accurate, optimized, and timely decisions are more critical now than ever before. Analytics offers help with these needs.\n- **Availability and affordability of enablers**: Thanks to recent technological advances and the affordability of software and hardware, organizations are collecting tremendous amounts of data. Internet of Things (IoT)-based data collection systems (which are based on various sensor and RFID [Radio-frequency Identification] technologies, internet, and social media sources) have significantly increased the quantity and quality of organizational data. In addition to the ownership model, cloud-based solutions and software (or hardware) â€œas-a-serviceâ€ business models allow small to medium-sized businesses to acquire (i.e., lease and pay only for what they use) analytics capabilities though they have limited financial resources.\n- **Culture change**: There has been a shift from traditional (intuition or â€œgut-feelingâ€) decision making to contemporary (fact or evidence-based) decision making at the organizational level. Most successful organizations have made a conscious effort to shift to data- or evidence-driven business practices. Because of the availability of data and supporting information systems infrastructure, such a paradigm shift is taking place faster than many thought possible. As the new generation of quantitatively savvy managers replaces baby boomers, this evidence-based managerial paradigm is expected to accelerate.\n\n# **Prescriptive Analytics and Optimal Decision Making: The Final Frontier**\n\nPrescriptive analytics is the highest level or echelon in the analytics hierarchy (Figure 2). It is where the best alternative among many courses of action (which are usually created/identified by descriptive and predictive analytics) are determined using sophisticated mathematical models, often labeled as â€œoperations researchâ€ techniques. Therefore, this type of analytics aims to answer the question, â€œWhat should I do?â€ Prescriptive analytics uses well-established operations research techniques like optimization, simulation, and heuristics-based decision modeling. Even though prescriptive analytics is at the top of the analytics hierarchy, its methods are not new. Most of the optimization, simulation, and heuristic techniques that collectively constitute prescriptive analytics were developed during (and right after) World War II in the 1940s and 1950s when there was a dire need to do â€œthe most and bestâ€ with limited available resources. Since then, some businesses have used some very specific problem types, including yield/revenue management, transportation modeling, scheduling, etc. The popularity of business analytics and the new taxonomy of analytics have made them popular again, opening their use to a wide array of organizations for various business problems and situations (Delen, 2019).\n\n## Value of Prescriptive Analytics in Business Operations\n\nThe value proposition of prescriptive analytics is obvious: supporting optimal decision making. Descriptive and predictive analytics layers focus on information creation, domain understanding, and problem definition. Prescriptive analytics focuses on problem-solving via optimal, timely decision making.\n\n## Suitable Business Decisions for Prescriptive Analytics\n\nToday, most businesses use business analytics to search for novel patterns and correlations, to identify and formulate business problems worthy of solving, and to support optimal and timely managerial decision making. Below are some examples that highlight applications of business analytics across various industries.\n\n- **Retail**: Retail companies, both online and brick-and-mortar, analyze their customer purchase data to optimize their product offerings, prices, and promotions. The goal is to maximize revenue and profitability while enhancing and maintaining high levels of customer satisfaction and loyalty.\n- **Finance**: Financial institutions use analytics to identify and prevent fraudulent transactions and specifically use *predictive* and *prescriptive* analytics to evaluate a personâ€™s financial behavior and assign them a risk level for credit card approval.\n- **Healthcare**: The healthcare industry has identified quite a few ways to use analytics to optimize the allocation of their resources: in the fulfillment of its mission to improve the quality of life for chronically ill patients, provide personalized patient treatment, reduce the rate of hospital-acquired infections, assess and identify treatment risk factors more rapidly, and many more applications. In addition, large medical centers have used free public health data to create visualizations that can help speed up identifying and analyzing healthcare information and tracking the spread of diseases.\n- **Insurance**: The insurance industry has numerous opportunities to expand its use of analytics. For instance, insurance carriers can increase the personalization of services and optimal pricing, which allows for the identification of more specific and actionable consumer segmentation. Additionally, analytics can provide opportunities for better fraud detection and greater industry transparency.\n- **Transportation**: The software *Dataiku DSS* (Data Science Studio), when applied to freight, sea freight, road freight, and passenger transport, uses predictive and prescriptive analytics with sensor data to determine optimal maintenance schedules.\n- **Communications, Media, and Entertainment**: Entertainment companies such as YouTube, Amazon, and Netflix analyze their usersâ€™ browsing habits and patterns, which allows them to create or curate content tailored for specific target audiences optimally.","tags":["DS"],"categories":["Prescriptive Analytics: Digital Decisioning"]},{"title":"Understanding Namespace & Scope","url":"/docs/Immediate-to-Python-4-Understanding-Namespace-and-Scope/","content":"\n# **Namespaces**\n\n## The Global Namespace\n\nt is a collection of names that map to objects (variables, functions, etc...) that have been created in our code. Anytime we run a python script (or a jupyter notebook), a global namespace is created. We can print the names that are in the global namespace with the `dir()` function.\n\n```python\n# use the dir() function to return the list of names in the namespace and use print() to print that list.\nprint(dir())\n```\n**output:**\n\n```\n['In', 'Out', '_', '__', '___', '__builtin__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__', '__vsc_ipynb_file__', '_dh', '_i', '_i1', '_ih', '_ii', '_iii', '_oh', 'exit', 'get_ipython', 'quit']\n```\n\n**Adding our own names to the global namespace**\n\nWe can add our own names very easily, all we need to do is create a new object.  Everything in python is an object, so if we create a variable or a function or a instance of a class, it will be added to the global namespace. \n\n```python\nmy_var = \"Will\"\nprint(dir())\n```\n**output:**\n\n```\n['In', 'Out', '_', '__', '___', '__builtin__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__', '_dh', '_i', '_i1', '_i2', '_ih', '_ii', '_iii', '_oh', 'exit', 'get_ipython', 'my_var', 'quit']\n```\n\n**Names in a namespace are unique - you can not have duplicates**\n\nIf we assign the variable name `my_var` another value, we will not be able to use `my_var` to access the previous value. In other words, you can not have two variables named `my_var` in the same namespace. You can not have two objects with the same name in the same namespace. \n\n**Let's now create a function and see that added to the global namespace**\n\n```python\ndef reverse_string(string):\n    '''Return the reverse of a string\n    '''\n    reverse_string = string[::-1]\n    return reverse_string\n\nprint(dir())\n```\n\n**output:** \n\n```\n['In', 'Out', '_', '__', '___', '__builtin__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__', '_dh', '_i', '_i1', '_i2', '_i3', '_ih', '_ii', '_iii', '_oh', 'exit', 'get_ipython', 'my_var', 'quit', 'reverse_string']\n```\n\n```python\nmy_string = 'Hello'\nmy_string_reversed = reverse_string(my_string)\nprint(my_string_reversed)\nprint(dir())\n```\n\n**output:**\n\n```\nolleH\n['In', 'Out', '_', '__', '___', '__builtin__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__', '_dh', '_i', '_i1', '_i2', '_i3', '_i4', '_i5', '_ih', '_ii', '_iii', '_oh', 'exit', 'get_ipython', 'my_string', 'my_string_reversed', 'my_var', 'quit', 'reverse_string']\n```\n\n**Let's import a package and see that added to our namespace**\n\n```python\nimport pandas\nprint(dir())\n```\n\n**output:**\n\n```\n['In', 'Out', '_', '__', '___', '__builtin__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__', '_dh', '_i', '_i1', '_i2', '_i3', '_i4', '_i5', '_i6', '_i7', '_ih', '_ii', '_iii', '_oh', 'exit', 'get_ipython', 'my_string', 'my_string_reversed', 'my_var', 'pandas', 'quit', 'reverse_string']\n```\n\n**Each package/module also has its own global namespace.**\n\nThis is important to understand. Each python module (a module is simply a .py file that contains functions, classes, and variables) has their own global namespace.\n\n## The Local Namespaces\n\nA local namespace and a global namespace can exist at the same time.\n\nThe code works because any variable created inside the function is not created in the global namespace, but it is created in what's called a ***\\*local namespace\\****. This namespace is local to the function (each function has its own local namespace).\n\n```python\nmy_text = \"I am outside the function and in the notebook's GLOBAL namespace!\"\n\ndef my_func():\n    my_text = \"I am inside the function and in the function's LOCAL namespace!\"\n    print(my_text)\n    return\n\n# Now run the function\nmy_func()\n# Print my_text\nprint(my_text)\n```\n\n**Output:**\n\n```\nI am inside the function and in the function's LOCAL namespace!\nI am outside the function and in the notebook's GLOBAL namespace!\n```\n\n**Let's now add `dir()` to the body of the function so that we can print the names in the function's local namespace**\n\n```python\nmy_text = 'I am outside the function in the notebooks GLOBAL namespace!'\nprint(\"These are the names in the global namespace:\")\nprint(dir(), '\\n')\n\ndef my_func_b():\n    my_text = \"I am inside the function in the function's LOCAL namespace!\"\n    print(\"These are the names in the function's local namespace:\")\n    print(dir(), '\\n')\n    print(my_text)\n    return\n\n# Now run the function\nmy_func_b()\n# Print my_text\nprint(my_text)\n```\n\n```\nThese are the names in the global namespace:\n['In', 'Out', '_', '__', '___', '__builtin__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__', '_dh', '_i', '_i1', '_i2', '_ih', '_ii', '_iii', '_oh', 'exit', 'get_ipython', 'my_func', 'my_text', 'quit'] \n\nThese are the names in the function's local namespace:\n['my_text'] \n\nI am inside the function in the function's LOCAL namespace!\nI am outside the function in the notebooks GLOBAL namespace!\n```\n\n**There can be multiple local namespaces**\n\nEach function has its own local namespace.\n\n```python\nmy_text = 'I am outside the function in the notebooks GLOBAL namespace!'\nprint(\"These are the names in the global namespace:\")\nprint(dir(), '\\n')\n\ndef my_func_1(my_text_1):\n    print(\"These are the names in the 1st function's local namespace:\")\n    print(dir(), '\\n')\n    print(my_text_1, '\\n')\n    return\n\ndef my_func_2(my_text_2):\n    print(\"These are the names in the 2nd function's local namespace:\")\n    print(dir(), '\\n')\n    print(my_text_2, '\\n')\n    return\n\n# Now run the function\nmy_func_1(\"Hello!\")\n\nmy_func_2(\"Goodbye!\")\n# Print my_text\nprint(my_text)\n```\n\n```\nThese are the names in the global namespace:\n['In', 'Out', '_', '__', '___', '__builtin__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__', '_dh', '_i', '_i1', '_i2', '_i3', '_i4', '_ih', '_ii', '_iii', '_oh', 'exit', 'get_ipython', 'my_func', 'my_func_1', 'my_func_2', 'my_func_b', 'my_text', 'quit'] \n\nThese are the names in the 1st function's local namespace:\n['my_text_1'] \n\nHello! \n\nThese are the names in the 2nd function's local namespace:\n['my_text_2'] \n\nGoodbye! \n\nI am outside the function in the notebooks GLOBAL namespace!\n```\n\n","tags":["Python"],"categories":["Immediate to Python"]},{"title":"10 Clustering","url":"/docs/Predicted-Analytics-8-Clustering/","content":"\n# **Unsupervised Learning**\n\n* Clustering\n  * k-means clustering\n  * Hierarchical Clustering\n* Principal Component Analysis\n\n**Unsupervised Learning in Predictive Analytics**\n\nUnsupervised learning is part of Machine Learning family of methods\n\nAlthough, it may not be as popular as supervised learning, it has a significant footprint in Analytics\n\n**The Challenge of Unsupervised Learning**\n\nModel assessment\n\n* We cannot tell if the model we have built is good\n* Because we do not have the test data with known response variable information\n* We cannot do cross validation\n\n# **Clustering**\n\nCategorize objects into groups (or clusters) so that \n\n* Objects in each group are similar \n* Objects in each group are different from objects in other groups\n\n**Clustering Applications**\n\n* Decrease the size and complexity of problems for other data mining methods\n* Identify outliers in a specific domain\n  * Customer Segmentation\n\n## Clustering Definition\n\n* Suppose â€˜nâ€™ observations\n* Let $ð¶1,ð¶2,...,ð¶ð‘˜$ are sets containing\n* the indices of the observations in each other\n  * $ð¶1 âˆª ð¶2 âˆª ð¶3 ...âˆª ð¶ð‘˜ = 1,...,ð‘›$ . Each observation belongs to at least one of the â€˜kâ€™ clusters.\n  * $ð¶ð‘˜ âˆ© ð¶ð‘˜â€² = 0$ for all $ð‘˜â‰ ð‘˜$â€². Clusters are non-overlapping: no observation belongs to more than one cluster.\n\n## Compute the Distance between Clusters\n\n![image-20230303111030606](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230303111030606.png)\n\n## Clustering Assessment\n\nA good cluster should have the within-cluster-variation is as small as possible.\n\n* within - cluster - variation = $W(C_K)$\n* Good cluster: $minimize(\\sum_1^kW(C_k))$$\\hat{i}$  \n* $W(C_K) = \\frac{1}{|C_k|} \\sum_{i,\\hat{i}}\\sum_{j=1}^p(x_{ij}-x_{i^ij})^2$\n\n# **K-Means**\n\n## K-means Algorithm \n\n* Given a K, find a partition of K cluster\n* Each cluster is represented by the center of the cluster and the algorithm converges to stable centers of clusters.\n* the K-means algorithm is carried out in three steps:\n  ![image-20230303095557785](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230303095557785.png)\n\n## Example \n\n![image-20230303095421868](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230303095421868.png)\n\n![image-20230303095038636](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230303095038636.png)\n\n![image-20230303095058835](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230303095058835.png)\n\n![image-20230303095357240](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230303095357240.png)\n\n![image-20230303095344768](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230303095344768.png)\n\n![image-20230303095333733](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230303095333733.png)\n\n![image-20230303095510057](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230303095510057.png)\n\n**Difference between kNN Classifier (k Nearest Neighbor) & k-Means Clustering**\n\n![image-20230303092358992](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230303092358992.png)\n\n## Example Code\n\n### Load the Libraries\n\n```python\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn.cluster import KMeans\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use(\"ggplot\") # grammar of graphic\n```\n\n### Read Data and Show the Scatterplot\n\n```python\nX = np.array([[1, 1],\n            [2, 1],\n            [4, 5],\n            [5, 4]])\n\nprint(X)\nplt.scatter(X[:,0], X[:,1], s=10, linewidth=5)\nplt.show()\n```\n\n**Output:**\n\n```\n[[1 1]\n [2 1]\n [4 5]\n [5 4]]\n```\n\n![download](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/download.png)\n\n### Build Clusters\n\n```python\nclf = KMeans(n_clusters=2)\nclf.fit(X)\n\ncentroids = clf.cluster_centers_\nlabels = clf.labels_\nprint(centroids)\nprint(\"labels=\", labels)\n```\n\n**Output:**\n\n```\n[[1.5 1. ]\n [4.5 4.5]]\nlabels= [0 0 1 1]\n```\n\n### Plot the Clusters\n\n```python\ncolors = [\"g.\",\"r.\",\"c.\",\"b.\",\"k.\",\"g.\"]\n\nfor i in range(len(X)):\n    plt.plot(X[i][0], X[i][1], colors[labels[i]], markersize = 10)\n    \nplt.scatter(centroids[:,0], centroids[:,1], marker='x', s=150, linewidth=5)\nplt.show()\n```\n\n![download (1)](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/download%20(1).png)\n\n## Parameter: `nstart`\n\nClustering algorithm will give slightly different results if we start with different initial values\n\nThe `kmeans` algorithm implemented in R has a parameter `nstart` which indicates multiple random initial assignments\n\nSuppose `nstart` = n\n\n* Algorithm builds â€˜nâ€™ clusters and only the best cluster is reported\n* Best cluster is the one which has minimum within-\n  cluster-variation\n\n**Disadvantage of K-means clustering** \n\n* You have specify the number of clusters\n\n# Hierarchical Clustering\n\nHierarchical clustering solves this problem â€“ no specification of number of clusters\n\nHierarchical structure also creates a hierarchical structure of data called **Dendrogram**\n\n## Strategy to build Hierarchical Clustering\nBottom-up approach\n\n* Agglomerative clustering\n\nCompute the Euclidean distance between data points\n\n* Shortest distance observations should be in a \n  single cluster\n* Next we compute the distance between cluster \n  that we have created and the next point closets to \n  it\n* Include that point in that cluster\n\n## Hierarchical Clustering Algorithm\n![image-20230303111410608](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230303111410608.png)\n\n## Example code\n\n[Mall_Customers.csv](https://uciunex.instructure.com/courses/16600/files/2324830?wrap=1)\n\n### Load the Libraries\n\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n```\n\n### Read Data\n\n```python\n'''\nSince we are performing clustering\nwe only need X variable\n\nClustering is a unsupervised method, \nthat's why we do NOT need the response variable or the 'y' variable\n'''\ndataset = pd.read_csv(\"Mall_Customers.csv\")\nX = dataset.iloc[:,[3,4]].values\n```\n\n### Plot the Dendrogram\n\n```python\n'''\nPlot the dendrogram\nThe plot will determine how many clusters we should need\n'''\nimport scipy.cluster.hierarchy as sch\n\ndendrogram = sch.dendrogram(sch.linkage(X,method='ward'))\nplt.title('Dendrogram')\nplt.xlabel('Customers')\nplt.ylabel('Eucledian Distance')\n\nplt.show()\n```\n\n![download3](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/download3.png)\n\n```python\n'''\nWe can have 3 clusters as standard\nOr we can have 5 clusters\nFind the longest line which is not crossed by horizontal line\n\nThis shows total number of clusters = 5\n'''\n \nfrom sklearn.cluster import AgglomerativeClustering\nhc = AgglomerativeClustering(n_clusters=5, affinity='euclidean',linkage='average')\n\ny_hc = hc.fit_predict(X)\n```\n\n```python\nplt.scatter(X[y_hc==0,0],X[y_hc==0,1],s=50,c='red',label='Cluster1')\nplt.scatter(X[y_hc==1,0],X[y_hc==1,1],s=50,c='blue',label='Cluster2')\nplt.scatter(X[y_hc==2,0],X[y_hc==2,1],s=50,c='green',label='Cluster3')\nplt.scatter(X[y_hc==3,0],X[y_hc==3,1],s=50,c='cyan',label='Cluster4')\nplt.scatter(X[y_hc==4,0],X[y_hc==4,1],s=50,c='magenta',label='Cluster5')\n\nplt.title('Cluster of the Customers')\nplt.xlabel('Annual Income (K$)')\nplt.ylabel('Spending Score (1-100)')\nplt.legend()\n```\n\n![download4](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/download4.png)\n","tags":["Data Mining","DS"],"categories":["Predicted Analytics: Tools & Techniques"]},{"title":"9 Data Prep: Discretization & 1Hot Encoding","url":"/docs/Predicted-Analytics-7-DataPrep-Discretization+OneHotEncoding/","content":"\n![image-20230307140225758](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230307140225758.png)\n\n# **Discretization**\n\nConverting Numeric Data into Categorical Data\n\n**How to determine the boundaries between classes?**\n\n* Natural boundaries\n\n![image-20230307140432485](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230307140432485.png)\n\n* Equi-width ranges\n\n![image-20230307140445235](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230307140445235.png)\n\n* Equi-log ranges\n\n![image-20230307140458116](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230307140458116.png)\n\n* Equi-depth ranges\n  â€‹\t\tRange [a,b] is chosen\n  * Each range has an equal number of records\n  * First sort the data\n    * Select the boundaries from the sorted \n      data such that each range contains equal number of observations\n\n\n## Example\n\n**Read Datafile**\n\n```python\nimport pandas as pd\ndf = pd.read_csv('Lung Capacity.csv')\n```\n\n**Discretize Height into 6 Categories : Width Size is Different**\n\n```python\nbins = [0, 50, 55, 60, 65, 70, 100]\ngroup_names = ['A', 'B', 'C', 'D', 'E', 'F']\n\nc1 = pd.cut(df['height'], bins, labels=group_names)\n```\n\n# One Hot Encoding\n\nLabel Encoder\n\n* Converts Categorical variable into Numerical values\n* Starting from 0,1,2,...\n* Code is assigned by alphabetical order\n\n## Example\n\n**Load the libraries**\n\n```python\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn import datasets\n```\n\n**Read the Dataset**\n\n```python\niris = datasets.load_iris()\nprint(type(iris))\nfeatures = iris[\"data\"]\nprint(type(features))\nprint(features[:5,:])\n\n#############################################\nprint('---------------------------')\nlabels = iris[\"target\"]\nprint(type(labels))\nprint(labels)\n```\n\n**Encode the response variable (labels - species) data into one-hot**\n\n```py\nlabels_onehot_dataframe = pd.get_dummies(labels,prefix='species')\none_hot = np.array(labels_onehot_dataframe)\n```\n\n","tags":["Data Mining","DS"],"categories":["Predicted Analytics: Tools & Techniques"]},{"title":"Iterators & Generators","url":"/docs/Immediate-to-Python-3-Iterators-and-Generators/","content":"\n# **Iterators**\n\n## Another look at `range()`\n\nThe `range()` function returns an iterator that will iterate through the values that we specify when we call `range()`.\n\nThe iterator only calculates and yields one value at a time.  It does not calculate or store those numbers in memory up front, it calculates them only when they are needed, one at a time.\n\n```python\nmy_range = range(11, 23)\nfor x in my_range:\n    print(x)\n```\n\n## File Object Are Also Iterators\n\nYou may have remembered from the prerequisite course how we opened files and iterated through the lines one at a time. The main takeaway is that each line is read in (iterated through) one at a time.  The entire file is not read up-front, each line is read into this notebook one at a time, as needed.\n\n```python\nfilepath = os.path.join(os.getcwd(), 'AAA_Fuel_Prices.csv')\ncount = 0\nwith open(filepath, 'r') as my_file:\n    for line in my_file:\n        line = line.strip()\n        print(line)\n        count += 1\n        if count > 10:\n            break\n```\n\n## Iterators Can Also Be Made From Lists (as well as other data types)\n\nIterators can also be made out of lists. This is what happens when we use a list in a for loop.\n\n```python\nmy_list = [1, 2, 3, 4, 5]\nfor x in my_list:\n    print(x)\n```\n\n```python\nmy_list_iterator = iter(my_list)\n\nprint(type(my_list_iterator))\nprint(my_list_iterator.__next__())\nprint(my_list_iterator.__next__())\nprint(my_list_iterator.__next__())\n```\n\n# **Generators**\n\nYou may wonder how can we write functions, like `range()`, that return iterators that we can iterate through. We can!  In order to do so, we must use the keyword `yield` instead of `return`.\n\n## Our Own Version Of Range\n\nLet's write our own version of the `range()` function.  We need to write a function the will yield numbers between a beginning and ending number. Note that when the function reaches the yield keyword, it will return that value (in this case, the value of `i`) and it will cease execution until it is asked for the next value. \n\n```python\ndef my_range(beg, end):\n    \"Generate numbers from start to stop\"\n    i = beg\n    while i < end:\n        yield i\n        i += 1\n```\n\n```python\n# Let's call the function to return a generator that we can iterate through. \nrange_of_nums = my_range(0, 10)\n\n# Now, let's call the __next__() method to get each value is the it is \n# \"yielded\" by the generator\n\n# This executes the code in the generator until it hits the yield statement.  It then stops until __next__() is called again.\nprint(range_of_nums.__next__())\nprint(range_of_nums.__next__())\nprint(range_of_nums.__next__())\n```\n**output:**\n\n```\n0\n1\n2\n```\n\nWe do not usually use the `__next__()` method directly. We are usually looping over the iterator, or passing the iterator to another iterative process (in these case,`__next__()` is still used \"under the hood\", but we are not using it directly as programmers). Below, we simple use `range_of_nums` in a loop, we also use the `my_range()` function directly in a for loop, just like you would use `range()`.\n\n```python\nfor num in my_range(30, 33):\n    print(num)\n```\n\n## **A Fibonacci Series Generator.**\n\nA Fibonacci Series is a series of numbers in which the next number is the sum of the two preceding numbers.  If we start with 0 and 1, then the series is 0, 1, 1, 2, 3, 5, 8, 13, etc.  This is a fun series that is often used in computer science lessons. Let's code a function that will return a generator that iterates through the Fibonacci Series (starting with 0 and 1).\n\n```python\ndef fibonacci_series(N):\n    \"\"\"Generate the Fibonacci series starting at 0 and 1\"\"\"\n    # We start by seeding 0 and 1 as the first two numbers\n    i_prev = 0\n    i = 1\n    yield i_prev  # we yield 0 first\n    # now in the following loop, we yield \"i\" and then calculate i_next by\n    # summing the two previous\n    for _ in range(N-1):\n        yield i \n        i, i_prev = i + i_prev, i\n```\n\n```python\nf_s = fibonacci_series(10)\nfor x in f_s:\n    print(x)\n```\n\n**Iterators Can Only Be Iterated Over Once**\n\nOnce we create an iterator, it can only be iterated over once.  For example, in the above cell we looped over the entirety of `f_s`.  Below, we try to loop over it again, but nothing prints. This is because we have already looped over the iterator to its end.  If we need to iterate again, we will have to create a new iterator.\n\n## File Word Counts\n\nAnother example, which will become more meaningful if you take the course Python Data Structures, Data Mining and Big Data, is producing a word counts from a file, line by line.\n\nLet's write a function that will generate word counts, from a file, line by line.\n\n```python\ndef file_word_count(filepath):\n    \"\"\"Generate word counts from ta file, line by line\"\"\"\n    # First, open the file\n    with open(filepath, 'r') as my_file:\n        # Loop through the lines\n        for line in my_file:\n            line = line.strip()  # strip whitespace from the line\n            words = line.split()  # split the line into words\n            # create a dictionary that we will store the word counts in\n            word_count_dict = {}\n            # loop through the words in the line and tally them in the\n            # dictionary\n            for word in words:\n                if word in word_count_dict:\n                    word_count_dict[word] += 1\n                else:\n                    word_count_dict[word] = 1\n            # now loop through the dictionary and yield up the word counts \n            for word in word_count_dict:\n                yield word, word_count_dict[word]\n```\n\n```python\naesopa10_path = os.path.join(os.getcwd(), 'aesopa10.txt')\ncounter = 0\nfor word, count in file_word_count(aesopa10_path):\n    print(word, count)\n    counter += 1\n    if counter > 2000:\n        break\n```\n\n# **Enumerate and Zip**\n\n## Enumerate\n\nEnumerate takes in an iterable object (like a list, tuple, or some other iterator) and outputs tuples that are enumerated. The first element in the tuple is the number and the second is the value from the original iterable object.\n\n```python\nmy_list = ['a', 'b', 'c']\n\nfor item in enumerate(my_list):\n    print(item)\n```\n\n```\n(0, 'a')\n(1, 'b')\n(2, 'c')\n```\n\n## Zip\n\nZip will take multiple iterable objects as inputs and output tuples that contain items from each iterable, in order. That is the first tuple will contain the first item from each iterable, the second tuple will contain the second items, etc...\n\n```python\ntuple_1 = (2012, 2012, 2012)\ntuple_2 = ('01', '02', '03')\n\nnew_list = []\nfor val1, val2 in zip(tuple_1, tuple_2):\n    print(val1, val2)\n    new_list.append(str(val1) + '_' + val2)\nprint(new_list)\n```\n\n```\n2012 01\n2012 02\n2012 03\n['2012_01', '2012_02', '2012_03']\n```\n\n```py\nfor x in zip(tuple_1, tuple_2):\n    print(x)\n```\n\n```\n(2012, '01')\n(2012, '02')\n(2012, '03')\n```\n\n","tags":["Python"],"categories":["Immediate to Python"]},{"title":"8 kNN Model","url":"/docs/Predicted-Analytics-6-kNN-Model.md/","content":"\n# **Similarity Based Learning**\n\n![image-20230307135237509](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230307135237509.png)\n\n**Compute the distance matrices between objects**\n\n![image-20230307135320379](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230307135320379.png)\n\n# **k Nearest Neighbor (kNN) Model**\n\n## Pros and Cons of kNN\n\n| Pros                                                         | Cons                                                         |\n| ------------------------------------------------------------ | ------------------------------------------------------------ |\n| Simple and Effective                                         | Does not produce a model, limiting the ability to understand how the features are related to the class |\n| Makes no assumption about the underlying data distribution<br/>Non-parametric | Requires selection of an appropriate value of â€˜kâ€™            |\n\n## Example\n\n![image-20230307135514058](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230307135514058.png)\n\n![image-20230307135528439](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230307135528439.png)\n\n![image-20230307135551335](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230307135551335.png)\n\n![image-20230307135600672](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230307135600672.png)\n\n![image-20230307135654082](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230307135654082.png)\n\n![image-20230307135706642](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230307135706642.png)\n\n# **kNN Model Assessment**\n\n![image-20230307135731080](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230307135731080.png)\n\n![image-20230307135744961](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230307135744961.png)\n\n# **Data Normalization: Standardization & Scaling**\n\nSuppose we have 2 data items\n\n* Height: varies from 4 â€“ 7 feet\n* Net Worth: $10,000 - $100B\n\nIf we use both the variables in a model\n\n* Net Worth will dominate because it contains large values\n\nSolution\n\n* Standardize\n* Scale\n\n## Data Standardization and Scaling\n\n![image-20230307135919181](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230307135919181.png)\n\n# kNN in Python\n\n[03_kNN_Optimum_k_Iris_Sci_L_ConMat.ipynb](https://colab.research.google.com/drive/1rkJLDzQ-40DtU-MPmGMYTANrwNn_wNeo)\n","tags":["Data Mining","DS"],"categories":["Predicted Analytics: Tools & Techniques"]},{"title":"Functions Positional & Keyword Arguments","url":"/docs/Immediate-to-Python-2-Functions-Positional-and-Keyword-Arguments/","content":"\n# Positional Arguments\n\nThese are argument that are assigned based on their position in the function definition.  A example below will make this clear.\n\n```python\ndef print_greeting(username, date):\n    \"\"\"Print a simple greeting\"\"\"\n    greeting = 'Greetings, {}. The date is {}'.format(username, date)\n    print(greeting)\n```\n\n```python\n# test the function\nprint_greeting('Ronda', '2019-01-01')\n```\n\nWhy did the function treat 'Ronda' as the username argument and '2019-01-01' as the date argument? The answer is simple: it is because 'Ronda' was the first argument passed to the function and, in the function definition, username is the first argument in the function signature.\n\nThe arguments are assigned by *position*.\n\n# Keyword Arguments\n\nKeyword arguments are arguments that are passed to a function by the argument name.  For example, when calling the `print_greeting` function we can pass the arguments as shown in the cell below.\n\n```python\nmy_name = \"Will\"\ntoday = \"2019-07-01\"\nprint_greeting(date=today, username=my_name)\n\nprint_greeting(username=\"Padma\", date=\"2019-03-01\")\n```\n\n**Sometimes you can't use keyword to specify arguments, and sometimes you can only use keywords to specify certain arguments.**\n\nMany built in Python function are implemented in C and use a position-only API for processing argument. That means that there are some function with arguments that can not be specified by keyword. \n\n```python\nhelp(sorted)\n```\n\n![image-20230301180010676](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230301180010676.png)\n\nNotice the `/` in the function signature above?  This indicates that all the argument to the left of it can ONLY be specified by position. That is you can not pass the 'iterable' argument by keyword. (You may notice the `*` in the signature as well, we will discuss that in a moment)\n\n```python\nmy_list = [2, 3, 1]\n\n# We incorrectly try to pass the iterable argument by keyword. This will produce an error\n\n# We correctly pass the iterable argument by position\nsorted_list = sorted(my_list)\nprint(sorted_list)\n```\n\nNow, what about the `*` in the function signature? This means that you can ONLY specify the arguments to the right of it by keyword.  For example, in the sorted function, the arguments `key` and `reverse` can only be specified by keywords. See an example below:\n\n```python\nmy_list = ['a_3', 'b_2', 'c_1']\n\nsorted_list = sorted(my_list, key=lambda x: x.split('_')[-1])\n\nprint(sorted_list)\n```\n\n# Introducing *args\n\nNow, take your mind back to positional arguments.  What if you want a function to accept unlimited positional arguments? This is what the `*args` argument can do. By using the `*args` argument in our function definition, all positional arguments will be collected in a **tuple** named `args`. Let's define a function below using the `*args` argument to see how this works\n\n```python\ndef args_example(*args):\n    '''Print the contents of args.'''\n    print(args)\n    \nargs_example('Miguel', 'Mary', 'Paul')\n# Output: ('Miguel', 'Mary', 'Paul')\n```\n\n## Combining \\*args with other positional arguments.\n\nYou can have positional arguments *before* `*args`. `*args` will collect all the extra position arguments passed to the function.\n\n```python\ndef print_all_the_greetings_2(greeting, *args):\n  '''Print \"{greeting}, {name} for the greeting and all the names passed as arguments\n\t'''\n\tfor name in args:\n\t\tprint(\"{}, {}\".format(greeting, name))\n        \nprint_all_the_greetings_2('Good Morning', 'Winston', 'Aarav', 'Julie')\n```\n\n# Introducing \\*\\*kwargs\n\nMuch like `*args` captures extra positional arguments, `**kwargs` captures extra keyword arguments in a dictionary called `kwargs`.\n\n```python\ndef kwargs_example(**kwargs):\n    \"\"\"Print kwargs\"\"\"\n    print(kwargs)\n    \nkwargs_example(name=\"Paloma\", occupation='teacher')\n```\n\n# Mixing Positional and Keyword Arguments.\n\nWhen using both positional and keyword arguments you must specify the positional arguments **first**.\n\n```PYTHON\nprint_greeting(username=\"Padma\", date=\"2019-03-01\")\n\nprint_greeting(\"Padma\", date=\"2019-03-01\")\n```\n\n## Mixing Positional Arguments, \\*args, Keyword Arguments and \\*\\*kwargs\n\n```python\ndef test_arg_and_kwarg(arg1, arg2, *args, kwarg1, kwarg2, **kwargs):\n    print(arg1)\n    print(arg2)\n    print(args)\n    print(kwarg1)\n    print(kwarg2)\n    print(kwargs)\n    return\n\ntest_arg_and_kwarg(1, 2, 'extra_1', 'extra_2', kwarg1='kwarg 1',\n                   kwarg2='kwarg 2', extra_kwarg1='Bonus!',\n                  extra_kwarg2=\"I'm extra!\")\n```\n\n# Default Arguments\n\nSometimes you want to specify a default argument value to one of the arguments in your function. This means that if no value is passed to that specific argument, it will still have a default value and the function will run successfully.\n\nThis is very common. In fact, let's look at the documentation for the built in function `sorted` again.\n\nSee the 'key=None' and the 'reverse=False' in the function signature? This indicates that the default value for key is None, and the default value for reverse is False. This means that if we use the sorted function, these will be the values for these arguments if we do not specify other values.\n\n```python\nhelp(sorted)\n\n# Output:\nHelp on built-in function sorted in module builtins:\n\nsorted(iterable, /, *, key=None, reverse=False)\n    Return a new list containing all items from the iterable in ascending order.\n    \n    A custom key function can be supplied to customize the sort order, and the\n    reverse flag can be set to request the result in descending order.\n```\n\n## You can use \\* and \\*\\* to pass arguments to functions.\n\nJust as we used the \\* and \\*\\* to collection arguments that are passed to a function, you can also use the to pass arguments to a function from a tuple or dictionary.  Let's walk through two examples below.\n\n```python\ndef my_function(x, y):\n    result = x**2-y**(0.5)\n    return result\n    \n# Here we use the function and pass the values directly\nresult1 = my_function(2.1, 0.6)\n\n# Belww, we define a tuple with the values, and then pass those values to the\n# function by indexing them from the tuple.\nvalues = (2.1, 0.6)\nresult2 = my_function(values[0], values[1])\n\n# Here, we just use the '*' to dump the arguments in the values tuple directly\n# to the function\nresult3 = my_function(*values)\n\n# Finally, print all three results so that we ensure all methods give the same\n#result\nprint(result1, result2, result3)\n```\n\n```python\ndef my_greeting(date, greeting):\n    print('{}: {}'.format(date, greeting))\n    \n# Here we use the function and pass the values directly\nmy_greeting('2018-11-07', \"How are you today?\")\n\n# Below, we define a tuple with the values, and then pass those values to the\n# function by indexing them from the tuple.\nmy_greeting(greeting='How are you today?', date='2018-11-07')\n\n# Here, we just use the '*' to dump the arguments in the values tuple directly\n# to the function\nvalues = {'greeting': 'How are you today?', 'date': '2018-11-07'}\nmy_greeting(**values)\n```\n\n# An example of a built-in function that actually uses `args` and `kwargs`!\n\nNow, let's look at a built-in functions that uses both `*args` and `**kwargs`.\n\nThe function is the `format` method of string objects.  Depending on which version of the prerequisite course you have taken, you may have already seen this, but we will do a quick review anyways.\n\n## The .format() method of a string object.\n\nOne of the preferred methods to format strings in Python is to use the format method of string objects. (The latest preferred method is something called 'f strings'). Observe the example below.  First, we define a string and we put `{}` in the string wherever we would like to fill in the string by a variable. We then call the `.format()` method on the string and pass to it the variables we would like to use to fill in the `{}` portions of the string.  The `{}` are filled in by the order we pass the variables to the `.format()` method.\n\n```python \nmy_string = 'Hi, my name is {}. I live in {} and I work at {}'.format('Will', 'California', 'UCI')\nprint(my_string)\n```\n\nThe point here is that `.format()` can accept *any* number of arguments, it just depend how many `{}` we have to fill in in the string.  How does `.format()` do this? It uses the `*args` argument to capture all of the positional arguments passed, and then it fills in the `{}` in the order of the arguments.\n\n## The .format() method also uses kwargs!\n\nFormat also supports keywords, observe the example below:\n\n```python\nmy_string = 'Hi, my name is {name}. I live in {home} and I work at {work}'. \\\n    format(name='Will', home='California', work='UCI')\nprint(my_string)\n```","tags":["Python"],"categories":["Immediate to Python"]},{"title":"7 Multi Variable Regression","url":"/docs/Predicted-Analytics-5-Multi Variable Regression/","content":"\n# **Multiple Variables Regression**\n\n**Definition**\n\nHow a response variable $y$ changes as the predictor (explanatory) variables $x1$, $x2$, ... $xn$ change\n\nIn a $n$ variable regression,\n\n* There is 1 response variable $y$\n* And $n$ predictor variables $x1$, $x2$, $x3$, ... $xn$, Goal is to find the values of $\\beta1$, $\\beta2$, $\\beta3$, ... $\\beta{n}$\n\n$$\ny = \\beta_1x_1+\\beta_2x_2+\\beta_3x_3+...+\\beta_nx_n+c\n$$\n\n**Regression Strategy**\n\nSame strategy used in the 2-variable regression: The least-squares regression line of y and x is the line that makes \n\n* the sum of the squares of the vertical distances of the data points from the line as small as possible\n\n# **Correlation between Multiple Variables**\n\n## Correlation Matrix\n\n**Definition: Correlation**\n\nA correlation matrix shows the linear correlation between each pair of variables under consideration in a multiple regression model\n\n**Predictor Variables Selection Criteria in Multi Regression**\n\n* Predictor variables should have a high linear correlation with the response variable\n* But do not include variables that are highly correlated among themselves\n\n**Multi-collinearity**\n\nMulti-collinearity exists between 2 explanatory (predictor) variables if they have a high linear correlation\n\n# **Normality of Residuals**\n\n![image-20230307134215162](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230307134215162.png)\n\n![image-20230307134223716](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230307134223716.png)\n\n# **Test Individual Regression Coefficients for Significance**\n\n## Coefficient: p-value\n\nSince both p-values are sufficiently small\n\n* We reject both the NULL hypothesis\n* There is a linear relationship between both the predictor variables and the response variable\n\nIf the p-value for the slope coefficient is large\n\n* We should consider removing it from the model\n\n# Example\n\n[P1_x1x2x3x4y.ipynb](https://colab.research.google.com/drive/1cWNHzXwddnd2cudiIZ9twv3Gxc7jSkNu)\n","tags":["Data Mining","DS"],"categories":["Predicted Analytics: Tools & Techniques"]},{"title":"6 Regression Quality","url":"/docs/Predicted-Analytics-4-Regression-Quality/","content":"\nMetrics to measure the Quality of Regression \n\n* Correlation between the response variable and predictor variables \n* Root Mean Square Error (RMSE) \n* R-square + Adjusted R-Square \n* p-values of the predictor variables \n* Residuals are normally distributed. \n\n# **Measure of Regression**\n![image-20230307111851997](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230307111851997.png)\n\n**Total Deviation = Explained Deviation + Unexplained Deviation**\n\n**SST = SSR + SSE**\n\n* SST = Total Sum of Squares = Total Deviation\n* SSR = Regression Sum of Squares = Explained Deviation\n* SSE = Error Sum of Squares = Unexplained Deviation\n\n![image-20230307112029424](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230307112029424.png)\n\n## Coefficient of Determination $r^2$\n\n**Definition**\n$$\nr^2 = \\frac{explainedVariation}{totalVariation}=\\frac{SSR}{SST}\n$$\n\n\n**How to compute $r^2$**\n$$\nsquareCorrelationValue = r^2 = 1-\\frac{SSE}{SST}\n$$\nThe closer $r^2$ is to $1$, the better the fir. For a perfect fit, $SSE = 0$, $r^2 =1$\n\n# **Standard Error**\n\nStandard error is the standard deviation of **the deviation of actual response variable with the predicted variable (residuals)** using the regression line.\n\nDegree of freedom\n\n* Two variables are estimated: Slope, Intercept\n* Lose 2 degree of freedom: $df=n-2$\n\n$$\ns_e=\\sqrt{\\frac{\\sum(y_i-\\hat{y_i})^2}{n-2}}=\\sqrt{\\frac{\\sum(residuals)^2}{n-2}}=\\sqrt{\\frac{SSE}{n-2}}\n$$\n\n\n\n# **Verify that the residuals are normally distributed**\n\n**If the residuals are not normally distributed, regression is not Valid**\n\n## Histogram\n\nPlot the histogram of the data, see a normal distribution\n\n**Problem with this technique**\n\n* Histograms shape change with different bin sizes\n\n## QQ Plot - Quantile-Quantile plot\n\nData is plotted against a theoretical normal distribution. If you see a straight line, data is normally distributed\n\n**Testing Procedure**\n\n* First Sort the data\n* Plot against appropriate quantiles from the standard normal distribution\n  * Divide the normal distribution curve into (n+1=10) parts, each part represents 10% of the area\n  * Compute the corresponding z-values\n\nQQ plot is\n\n* X axis: z-values taken from the standard normal distribution curve\n* Y-axis: Sorted Data values\n\n![image-20230307133050546](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230307133050546.png)","tags":["Data Mining","DS"],"categories":["Predicted Analytics: Tools & Techniques"]},{"title":"5 Linear Regression","url":"/docs/Predicted-Analytics-3.2-Linear-Regression/","content":"\n*Linear regression* *analysis* is a widely used statistical technique used to explore relationships among continuous variables. In addition, this technique can be applied to categorical variables through the use of dummy variables.\n\nThere are many situations where linear regression is useful. For instance,\n\n- A market researcher can use regression to determine which one of several media markets is the best on which to spend their advertising dollars;\n- In business, regression can be used to predict a competitive salary offer for a programmer with a given number of years of experience;\n- In Major League Baseball, regression can be used to predict the salaries of free agents;\n- In social science research, regression can predict a wide range of phenomenaâ€”from economic performance to college enrollment.\n\n# **Simple Linear Regression**\n\n*Simple linear regression* refers to the statistical relationship between a dependent continuous variable, say *y*, and a single independent continuous variable, say *x*. This relationship takes the form of an equation that lets us predict values of *y* given values of *x*. Regression equations are useful for estimating new data values or exploring â€œwhat ifâ€ questions.\n\nAs an example of simple linear regression, suppose you survey a group of workers and notice that those who have more years of education tend to have received higher starting salaries when they began working. A linear regression analysis allows you to quantify the relationship between starting salary and years of education through an equation that you can then use to make predictions about starting salaries for other individuals. \n\nLinear regression analysis allows you to (1) determine whether one or more continuous (independent) variables can effectively predict the values of an outcome (dependent) variable and (2) quantify the impact that each independent variable has on that outcome variable.\n\nThe relationship between a dependent variable and a single independent variable can be visualized using a scatter plot, as shown in Figure 1, below. The figure shows a set of data points along with the â€œbest-fitâ€ line that results from a regression analysis of that data.\n\n![A simple scatter with Fit Line of Cred card debt in the thousands by Household income in thousands showing the relationship between credit card debt and household income.](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/8z2gseSWiCfo4OvgpV_tyZr16er1sl1k6bsCGtrVhCrH8DrIRl3Jz4zaXHG7FmUiUiGqhjLPxpJXNjleOFdQP-8SW78yLrc_XzCSzZKO7wiBI7OFv7z-PCZ9VCRBgqkjM9weT-_CqN1WgquKbimuQA)\n\n*Figure 1: A scatter plot showing the relationship between credit card debt (in thousands) and household income (also in thousands)*\n\nThe line is represented in general form by the equation \n\n$$\ny = bx+a\n$$\n\nwhere *b* is the slope (the change in *y* per unit change in *x*) and *a* is the intercept (the value of *y* when *x* is zero). People are usually more interested in the slope than in the intercept. Also, note that *a* and *b* depend on the unit of measurement of the variables and, as such, do not necessarily indicate anything about the strength of the relationship.\n\nAs you can see, the data points donâ€™t necessarily fall on this line, so itâ€™s important to evaluate how well the line actually represents the data. To determine this, we start by looking at the *correlation* between the *x* and *y* values, and calculating a *correlation coefficient, r,* to quantify this relationship. Because the correlation coefficient can take on negative values and we prefer to work with only positive values, we typically square it (*r**2*) and refer to it as *R-squared.* This quantity lies on a scale from 0 (no linear association) to 1 (perfect linear association), and can be interpreted as the proportion of variation in one variable that can be predicted from the other. Thus an R-squared of 0.5 indicates that you can account for 50% of the variance in one variable if you knew the values of the other variable. Think of this value as a measure of the improvement in your ability to predict one variable from the other (or others if there are multiple independent variables).\n\nWhile R-squared provides a good starting point for determining how well your regression equation fits your data, a more rigorous way involves performing *statistical tests* to determine the *statistical significance* of the relationship. A relationship that is statistically significant means that your regression equation can reliably make predictions given new values of the independent variable. In other words, the predictions are more likely to be due to some factor of interest rather than random chance.\n\nReferring to Figure 1, notice that many points fall near the line but some are quite a distance from it. For each point, the difference between the value of the dependent variable and the value predicted by the equation (the value on the line) is called the *residual* (also known as the *error*). Points above the line have positive residuals (the equation under-predicted them), those below the line have negative residuals (the equation over-predicted them), and those points falling on the line have a residual of zero (perfect prediction). Points having relatively large residuals are of interest because they represent instances where the prediction performed poorly. Outliers, or points far from the positions of the other points, are of interest in regression because they can exert a considerable influence on the equation (especially if the sample size is small).\n\nLetâ€™s apply linear regression analysis to an example in which household income is the independent (predictor) variable and credit card debt is the dependent variable. A standard linear regression analysis generates three tables depicting the relationship between the two variables. Here are several key points about the information appearing in those tables:\n\n- The *model summary table* provides several measures of how well the model fits the data (see Table 1).\n- As discussed above, R-squared, which can range from 0 to 1, is the correlation coefficient squared. It can be interpreted as the proportion of variance of the dependent measure that can be predicted from the independent variable(s).\n- The *adjusted R-squared* represents a technical improvement over R-squared in that it explicitly adjusts for the number of predictor variables relative to the sample size. If the adjusted R-squared and R-squared differ dramatically, it is a sign that you have used too many predictor variables for the sample size.\n- The *standard error of the estimate* is a measure of the standard deviation of the residuals. It represents the amount of variation that is not accounted for by the regression line on the scale of the dependent variable.\n\n![image-20230223222504355](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230223222504355.png)\n\nWhile the goodness-of-fit measures indicate how well you can expect the regression equation to predict the dependent variable, they do not tell whether there is a statistically significant relationship between the dependent and independent variable(s). For this, we turn to an *analysis of variance* (ANOVA) table, which presents technical summaries (i.e., sums of squares and mean square statistics) of the variation accounted for by the prediction equation (Table 2). The main goal is to determine whether there is a statistically significant (non-zero) linear relation between the dependent variable and the independent variable(s).\n\nThe significance (Sig.) column in the ANOVA table provides the probability that there is no relationship between the dependent and independent variable(s). A zero or nearly zero Sig. value means that the relationship is statistically significant and that you should further investigate the results of the regression coefficients appearing in the Coefficients table (Table 3).\n\n![image-20230223222446131](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230223222446131.png)\n\n*Table 2: ANOVA table showing sums of squares, mean squares, and the probability that there is no relationship between dependent and independent variables*\n\nThe first column contains a list of the independent variables plus the intercept (constant). The intercept is the value of the dependent variable when the independent variable is 0â€”it is also *a* in the equation $y=bx+a$.\n\n- The column labeled B contains the estimated regression coefficients you would use in a prediction equation. In this example, the coefficient for household income indicates that on average, each additional unit increase in household income is associated with an increase of 0.030 in credit card debt.\n- The Std. Error column contains standard errors of the regression coefficients. The standard errors can be used to calculate a 95% confidence interval above and below the B coefficients. (This means that statistically, the true value of B will fall within this interval 95% of the time.)\n- Betas are standardized regression coefficients used to judge the relative importance of each of several independent variables.\n- The t statistics provide a significance test for each B coefficient, indicating which predictors are statistically significant.\n\n![image-20230223222431472](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230223222431472.png)\n\n*Table 3: A tabulation of statistical information for the coefficients in the regression model relating credit card debt to household income*\n\n# **Multiple Linear Regression**\n\nRegression involving more than one independent variable is called *multiple linear regression* and is a direct extension of simple linear regression. When running a multiple linear regression analysis you are again concerned with fitting a linear model to the data, determining whether any of the variables are significant predictors, and estimating the coefficients of the best-fitting prediction equation. In addition, you are interested in the relative importance of the independent variables in predicting the dependent measure.\n\nContinuing with the previous example, we add age, years with current employer, and having previously defaulted as independent variables to the model, which now explains about 43% of the variance in credit card debt. This is a substantial increase in explanatory power from the 30% we were able to explain with just one predictor variable, namely household income.\n\n![image-20230223222534173](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230223222534173.png)\n\n*Table 4: The model summary table for a multiple linear regression*\n\nNot surprisingly, we still have a statistically significant model as shown in the ANOVA table Table 5).\n\n![image-20230223222555334](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230223222555334.png)\n\n*Table 5: ANOVA table showing sums of squares, mean squares, and the probability that there is no relationship between dependent and independent variables*\n\nFinally, we can see in Table 6 that all of the variables in the model are statistically significant except age, which means that we can remove this variable from the model. We can also see that household income is the most important predictor, followed by previous defaults, and then years with the current employer. We can use the regression equation that resulted from this analysis to predict someoneâ€™s credit card debt as follows:\n\n$$\ncreditCardDebt=0.026(householdIncome)+0.067(yearsWith Employer)+1.627(previouslyDefaulted)-0.721\n$$\n\nNote that â€œpreviously defaultedâ€ is a categorical variable, which is coded as a dummy variable in which â€œnoâ€ is 0 and â€œyesâ€ is 1.\n\n![image-20230223222722318](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230223222722318.png)\n\n*Table 6: A tabulation of statistical information for the coefficients in the regression model relating credit card debt to household income, age, years with employer, and previous defaults*\n\n# **Polynomials and Interaction Terms**\n\nThis is an advanced and important topic. It is not true that linear regression can identify only linear relationships. It can handle curvilinear relationships as well if you know how to prepare the data. For instance, if you determined (through visual inspection) that income would predict credit card debt better with a quadratic relationship, you could do the following.\n\n- Create a new variable consisting of income squared\n- Add another coefficient corresponding to the new variable\n\nIt would look like this:\n\n$$\ny=a+b_1x_1+b_2x_1^2\n$$\n\nwhere $x_1$= income and $x_1^2$= income squared.\n\nAnother feature you might uncover through visual inspection is a pair of variables that interact. Here again the solution is to create a new variable. A good indicator that you have an interaction occurring is if the slopes of the regression lines for two groups (e.g., males and females) are different. In our example, this would suggest that the relationship between credit card debt and income is different for people that had previously defaulted on a loan than it is for people that had not previously defaulted on a loan (see Figure 2).\n\nThe resulting regression formula looks like this:\n\n$$\ny=a+b_1x_1+b_2x_2+b_3x_1x_2\n$$\n\nwhere $x_1$= income and $x_2$= previously defaulted. The interaction appears as the product $x_1x_2$.\n\nIf you fail to include the interaction term, the model will mathematically force the lines to be parallel and maintain the same â€œ gapâ€ over the entire income range. Figure 2 makes it clear that this would not be accurate since the gap is clearly more severe at higher levels of income. Polynomials and interaction terms can seem tricky and abstract at first, but they are critical for understanding neural networks, which will be presented in a later module.\n\n![A simple scatter with Fit Line of Cred card debt in the thousands by Household income in the thousands by Previously defaulted showing differing linear regression line slopes for customers that had and had not previously defaulted on a loan](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/Ej3sVrRFTs-bdKkJtOaIiAGX-PEAf5_Qu3oJrL9rB6FV4Ba9qaasgKCZRvvuOfmXwVbQwHnykt_t91faPELw1vPP7Z8UHXUAWMavQhsVs6EH5eqKCsegJRdqbzd7etezFIwh0fLcwoN5-UqvJjIRbw)\n\n*Figure 2: Differing linear regression line slopes for customers that had and had not previously defaulted on a loan*","tags":["ML","Data Mining","DS"],"categories":["Predicted Analytics: Tools & Techniques"]},{"title":"4 Introduction to Regression","url":"/docs/Predicted-Analytics-3.1-Introduction-to-Regression/","content":"\n# **Definition: Linear Regression**\n\n**2 variable regression** - how a response variable $y$ changes the predictor (explanatory) variable $x$ changes.\n$$\ny = \\beta_1x + c\n$$\n\n**Multiple regression** - how a response variable $y$ changes as the predictor (explanatory) variables $x1$, $x2$, ... $xn$ change\n$$\ny = \\beta_1x_1+\\beta_2x_2+\\beta_3x_3+...+\\beta_nx_n+c\n$$\n\n**Single Variable Polynomial Regression: First degree to Fifth Degree**\n\nThe concept can be extended to polynomial regression\n\n$$\n\\begin{align}\ny &= c + a_1x \\\\\ny &= c + a_1x + a_2x^2 \\\\\ny &= c + a_1x + a_2x^2 + a_3x^3 \\\\\ny &= c + a_1x + a_2x^2 + a_3x^3 + a_4x^4 \\\\\ny &= c + a_1x + a_2x^2 + a_3x^3 + a_4x^4 + a_5x^5 \\\\\ny &= c + a_1x + a_2x^2 + a_3x^3 + a_4x^4 + a_5x^5 +a_nx^n\\\\\n\\end{align}\n$$\n\n# **Regression Strategy: Ordinary Least Squares (OLS)**\n\nThe least-squares regression line of y and x is the \nline that makes the sum of the squares of the vertical \ndistances of the data points from the line as small as \npossible.\n\n![image-20230222092629577](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230222092629577.png)\n\n# Regression: Supervised Learning Method\n\nSingle split model assessment methodology\n\n* The model is tested on hold-out sample\n* Only the hold-out sample accuracy is reported\n\n![image-20230306002607033](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230306002607033.png)\n\n# **Nearest Neighbor Regression**\n\nA method for predicting a numerical variable $y$, given a value of $x$:\n\n* Identify the group of points for which the values of $x$ are close to the given value\n* The prediction is the average of the $y$ values for the group\n\n**Graph of Averages**\n\n* For each value of $x$, the predicted value of $y$ is the average of the $y$ values of the nearest neighbors.\n* Graph these predictions for all the values of $x$, That's the **graph of average**\n* If the association between the two variables is linear, then points on the graph of averages tend to fall on near a straight line. That's the **regression line.**\n\n# Solution for Regression Line\n\n* Residual = Observed value â€“ Computed Value\n\nSuppose regression equation is\n$$\ny = mx+b\n$$\n\n* $y$ is the explanatory variable, $x$ is the predictor variable\n* $m$ is the slope of the line, $b$ is the intercept\n\n$$\n\\begin{align}\nResidual &= y_i-(mx_i+b)\\\\\nResidual^2 &= (y_i-(mx_i+b))^2\\\\\nResidualSumOfSquares &= RSS = \\sum_{i=1}^{N}(y_i-(mx_i+b))^2\n\\end{align}\n$$\n\n* To find the minimum point of this function, we will take the partial derivative of RSS with respect to â€˜mâ€™ and â€˜bâ€™ and set that to zero.\n\n![image-20230306005620610](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230306005620610.png)\n\n![image-20230306005637510](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230306005637510.png)\n$$\n\\begin{align}\nm &= \\frac{\\sum(y_ix_i)-\\frac{\\sum(y_i)\\sum(x_i)}{N}}{\\sum(x_i^2)-\\frac{\\sum(x_i)^2}{N}}\\\\\nb &= (\\frac{\\sum(y_i)}{N}-m\\frac{\\sum(x_i)}{N})\n\\end{align}\n$$\n\n## Method 1\n\n![image-20230306010108743](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230306010108743.png)\n\n![image-20230306010118579](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230306010118579.png)\n\n![image-20230306010129262](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230306010129262.png)\n\n# Example\n\n## #1 Galton\n\n[galton.ipynb](https://colab.research.google.com/drive/1EsqPlpKab6Ko7YXQzjLxe0kVI88L3341)  [galton.csv](https://drive.google.com/file/d/1R3n46DKTH_jXpkUaE_D0jOQ1mzblXTEk/view?usp=sharing)\n\n## #2 Using skLearn and staysmodel\n\n[P1_Dictionary.ipynb](https://colab.research.google.com/drive/1rfAxSYwvt0VAW7Z6oDrpfGb5j9TP-yZs)\n\n## #3 FULL CODE_Regression_Model\n\n[FULL CODE_Regression_Model_Advertising.ipynb](https://colab.research.google.com/drive/1fLiSvyb-y4cQR8MFPMc7ZytbgOTjRBr7)\n","tags":["ML","Data Mining","DS"],"categories":["Predicted Analytics: Tools & Techniques"]},{"title":"3 Modeling Techniques","url":"/docs/Predicted-Analytics-2.2-ML-Techniques/","content":"\n# **Modeling Methods**\n\n| #    | Modeling Methods               | Response Variable: Numerical /Categorical | Supervised or Unsupervised | Strategy                              |\n| ---- | ------------------------------ | ----------------------------------------- | -------------------------- | ------------------------------------- |\n| 1    | Linear & Polynomial Regression | Numerical                                 | Supervised                 | Error Based<br/>Minimizing Error      |\n| 2    | Logistic Regression            | Categorical (Binary)                      | Supervised                 | Maximizing Likelihood                 |\n| 3    | Discriminant Analysis          | Categorical                               | Supervised                 |                                       |\n| 4    | K Nearest Neighbor             | Categorical                               | Supervised                 | Similarity Based                      |\n| 5    | Decision and Regression Trees  | Categorical + Numerical                   | Supervised                 | Information Based                     |\n| 6    | NaÃ¯ve Bayes                    | Categorical                               | Supervised                 | Probability Based                     |\n| 7    | Neural Networks                | Numerical + Categorical                   | Supervised                 | Mimicking Human Brain                 |\n| 8    | Clustering                     |                                           | Unsupervised               |                                       |\n| 9    | Principal Component Analysis   |                                           | Unsupervised               |                                       |\n| 10   | Support Vector Machines        | Categorical                               | Supervised                 | Error Based                           |\n| 11   | ARIMA : Time Series            | Numerical                                 | Supervised                 | Auto Regression & Moving <br/>Average |\n\n# **Estimation or Classification**\n\n**Goals of Machine Learning Application: Estimation or Classification**\n\n* **Estimation** â€“ Regression modeling technique is used\n\n  *Output is a number*\n\n  * House price\n  * Product sales for next quarter\n  * GNP growth for the next quarter\n  * Employment\n\n* **Classification** â€“ NaÃ¯ve Bayes, Decision Trees etc. modeling techniques are used\n\n  *Output is a categorical variable*\n\n  * Sports team will win or lose\n  * Email is junk or not\n  * Which grade student will get\n  * Tweet is positive or negative\n\n# **Classification of Modeling Methods**\n\n**Response Variable**\n\n* Numerical or Categorical\n\n**Supervised or unsupervised**\n\n**Strategy**\n\n* Error based learning\n* Similarity Based Learning\n* Information Based Learning\n* Probability Based Learning\n* Mimicking the Human Brain\n\n# **Supervised vs. Unsupervised**\n\n**Supervisor learning** is the most common learning type where **there is a target/output variable** (which is also called supervisor)\n\n* Supervisor (target variable) teaches the algorithm how to build/learn the pattern model\n* In PA, supervised learning â‰ˆ predictive modeling\n\n**Unsupervised learning has NO target variable**\n\n* No supervisor to teach â†’ algorithm has to learn by itself\n* In PA, unsupervised learning â‰ˆ descriptive modeling\n\n![image-20230221112637145](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230221112637145.png)\n\n# **Classifying Based on Strategy to Build a Model**\n\n## Error based learning\n\n* Linear Multi Variable Regression\n* Support Vector Machine\n\nIn error-based machine learning\n\n* We perform a search for a set of parameters for a parameterized model\n* That minimizes the total error across the predictions made by the model\n* With respect to a set of training instances (training data)\n\n![image-20230221113355334](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230221113355334.png)\n\n## Similarity Based Learning\n\n* K Nearest Neighbor\n\nCompute the distance matrices between objects\n\n![image-20230221113427635](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230221113427635.png)\n\n## Information Based Learning\n\n* Decision Trees\n* Regression Trees\n* Split of decision trees are based on the entropy of the tables\n\nLearn by Asking Questions\n\n* The Socratic approach to questioning is based on the practice of disciplined, thoughtful dialogue.\n* Socrates, the early Greek philosopher/teacher, believed that disciplined practice of thoughtful questioning enabled the student to examine ideas logically and to determine the validity of those ideas.\n\n![image-20230221113520227](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230221113520227.png)\n\n## Probability Based Learning\n\n* NaÃ¯ve Bayes\n\nProvides a way to compute *reverse* probability. \n\nGiven $P(B|A)$, we can compute $P(A|B)$\n\n$$\nP(A|B) = P(B|A)P(A)/P(B)\n$$\n\nNaÃ¯ve Assumption: Assuming Variable Independence\n\n## Mimicking the Human Brain: Neural Networks\n\n* Extract linear combinations of the inputs\n* Model the target as the non-linear functions of these features\n\n![image-20230221113814241](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230221113814241.png)\n\n**Deep Learning:** Complex set of Neural Networks with many layers of processing\n\n![image-20230221114146210](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230221114146210.png)\n\n**Main Applications of Deep Learning Neural Networks**\n\n* Image Recognition\n  * Convolution Neural Networks\n* Image Classification\n  * Convolution Neural Networks\n* Hand Writing Identification\n* Speech Recognition\n  * Long Short-Term Memory Networks","tags":["ML","Data Mining","DS"],"categories":["Predicted Analytics: Tools & Techniques"]},{"title":"2 Introduction to Predictive Modeling","url":"/docs/Predicted-Analytics-2.1-Introduction-to-Predictive-Modeling/","content":"\nData analysis involves a number of modeling techniques that can be classified as three main types:\n\n- Predictive\n- Clustering\n- Association\n\n# **Predictive Modeling**\n\n*Predictive modeling*, sometimes called ***supervised learning***, focuses on understanding or making predictions about a given variable based on one or more other variables. **The ultimate goal of predictive analysis is accuracy**. For example, you may want to predict which potential customers are more likely to apply for a credit card based on a knowledge of their demographic and financial information. In this scenario, there are two types of variables:\n\n- **The** **dependent variable** **or** **target**: The variable you are trying to predict or understand (i.e., the likelihood of applying for a credit card)\n- **The** **independent variables** **or predictors**: The variables you are using as a basis for predicting or understanding the target variable (i.e., demographic and financial information)\n\nBelow are several additional applications of predictive modeling:\n\n- Determining which students in a class will pass or fail\n- Predicting which parts of a city will experience increasing crime (predictive policing)\n- Forecasting the number of unfilled hotel rooms three months from now\n- Projecting which patients are likely to have a heart attack\n- Estimating how much each customer will purchase when shopping online\n- Classifying shipping containers according to the likelihood that they carry drugs or weapons\n- Identifying IP addresses that are sending unusual amounts of information\n\n# **Cluster Analysis**\n\nCluster analysis, or segmentation, is a type of unsupervised learning problem that is very different from predictive modeling. Cluster analysis is appropriate when you can associate each case in a dataset with other cases that share similar distinct characteristics. You end up with several groups, in which the members of each group are very similar to each other but very different compared to members of other groups.\n\nCluster analysis is often used in marketing campaigns so that customers receive ads tailored for the group to which they belong rather than generic ads. These groups are identified via cluster analysis. In such a scenario, there is no dependent variable; only independent variables are used to segment the cases.\n\nCases in which the variables have similar values are grouped into *clusters* in an attempt to find homogenous subsets. Determining the number of clusters to create is part of the challenge of such techniques. Since clustering is exploratory in nature, the resulting clusters are not necessarily right or wrong. Instead, cluster solutions should be judged by their ability to address the specific business problem you are trying to solve. Applications of cluster analysis include:\n\n- Market research\n- Plant and animal ecology\n- Medical imaging (e.g., PET Scans)\n- Service/product usage pattern identification\n- Social network analysis\n- Crime analysis\n- Anomaly detection\n\n# **Association Modeling**\n\nCompanies like Amazon and Netflix have made *association modeling* commonplace. As consumers, most of us have encountered recommendation engines where a movie is recommended based upon our prior viewing habits or books are recommended based on our prior purchases (or even just our prior browsing behavior). **Association models use transactional data to predict future transactions.** The idea is that you may be able to suggest additional items that a person may want or need based on their previous buying behavior. This results in statements such as, â€œPeople who bought product A and product B might also like product Câ€ appearing on their computer screens.\n\nAssociation modeling is often described primarily as a kind of market basket analysis, but while it is strongly associated with retail data, it can also be applied in other areas. For example, in predictive maintenance, a pair of part failures might frequently be associated with the failure of a third part even though that third part doesnâ€™t show evidence of trouble at the time the first two fail.\n\n**Transactions in association models often occur at the same time.** For instance, many items might be listed on a grocery store receipt, but, there is no indication of which purchases occurred before others. Hot dogs and hot dog buns may be frequently purchased together, but there is no â€œruleâ€ that indicates which purchase occurred first. In a predictive model, you might say that hot dogs â€œpredictâ€ buns or that buns predict hot dogs. A further refinement involves ***sequence analysis***, which does take into consideration the order in which events, such as purchases, occur. This can be useful in predictive maintenance or in web mining, where it might be beneficial to know the sequence in which website visitors click on links and buttons on a page or move to other pages in the site.\n\nApplications of association modeling include:\n\n- Market basket analysis\n- Retail data analysis\n- Web usage \n- Insurance claim analysis\n- Service usage\n- Medical procedures\n\n# **Overview of Predictive Models**\n\nThis course covers only some of the most popular predictive models. Specifically, weâ€™ll take a look at the following:\n\n- Statistical models\n- Decision Tree models\n- Machine Learning models\n\n## Statistical Models\n\n***Statistical models* produce equations and *statistical tests* guide predictor selection. These models make certain assumptions whereas *rule induction* and *machine learning* models do not.** Here are several characteristics of statistical predictive models:\n\n- Predictions are expressed as equations.\n- Equations allow users to see the effect of a one-unit change on any field and how this change impacts the outcome variable.\n- Predictive models are based on statistical theory, which involves developing hypotheses and assessing statistical significance. This allows you to easily identify the important variables.\n- Predictive models involve assumptions about the data, which may limit the situations in which some models can be used.\n\nBelow is a list of some statistical models.\n\n- Logistic Regression\n- Discriminant Analysis\n- Linear Regression\n- Generalized Linear Models\n- Cox Regression\n- Time Series\n\n## Decision Tree Models\n\nA *decision tree* or *rule induction model* is an important type of predictive model. It derives a set of rules in relation to a dependent variable. The modelâ€™s output shows the reasoning for each rule and can therefore be used to understand the decision-making process that drives a particular outcome. Models that produce decision trees belong to this class of models. Generally, decision tree predictive models:\n\n- Create segments that are mutually exclusive and exhaustive (identify homogeneous subgroups)\n- Create rules for making predictions about individual cases\n- Can easily handle a large number of predictors\n- Can account for interaction and non-linear relationships\n- Have few assumptions\n- Can create overly complex models that over-fit data (does not generalize)\n\nBelow is a list of several rule induction models:\n\n- CHAID\n- CART\n- C5.0\n- QUEST\n- Decision List\n- MARS\n\n## Machine Learning Models\n\n*Machine learning models* are optimized for learning complex patterns. Unlike traditional statistical techniques, no assumptions are made about the data. Machine learning models do not produce a set of rules like rule induction models, nor do they produce easy-to-understand equations like statistical models. Thus, machine learning models are often said to be â€œblack boxâ€ models. They produce a set of equations, but because there is a hidden layer (possibly several hidden layers), the interpretation of the coefficient weights is not straightforward as it is with traditional statistical models or rule induction models. Machine learning predictive models:\n\n- Are optimized for learning complex patterns\n- Can account for interaction and non-linear relationships\n- Have few assumptions \n- Are essentially â€œblack boxâ€ modelsâ€”their interpretation is not straight-forward\n- Are used for predictive accuracy but not for understanding the mechanics behind a prediction\n\nBelow is a list of several machine learning models:\n\n- Neural Networks\n- Support Vector Machines\n- Random Forest\n- NaÃ¯ve Bayesian Algorithms\n- Gradient Boosting Algorithms\n- K-Nearest Neighbors\n\n# **Model Validation**\n\nThe process of statistical hypothesis testing, which involves a resultâ€™s statistical significance in the context of certain data distribution assumptions (such as having normally distributed errors), helps us determine when we have found a valid and reliable result. However, most data-mining methods do not depend on specific data distribution assumptions for drawing inferences from the sample to the population. So how is validation achieved? Model validation in data mining is usually done by partitioning the data into training and testing datasets. Models are developed from the training data and then the modelsâ€™ predictions are tested on the testing data. Validity is established by demonstrating that the model applies to data different from what was used to derive the model. Statisticians often recommend such validation for statistical models, but it is crucial for more general (less distribution-bound) data-mining techniques.\n\n# **How to Choose a Model**\n\nChoosing a model is difficult. Obviously, if you have a variable in the data file that you want to predict, then any of the predictive models (depending on the target variableâ€™s level of measurement) will perform the task albeit with varying degrees of success. If you want to find groups of individuals that behave similarly on a number of fields in the data, then any of the clustering methods are appropriate. The use of association rules, while not directly giving you the ability to make predictions, are extremely useful as a tool for understanding the various patterns within the data.\n\nHowever, determining which particular prediction technique will work best depends specifically on how the variables you want to predict are related to the predictors. There are suggested guidelines as to when one technique may work better than another, but these are only suggestions and not rules.\n\nFrom the previous discussion, it follows that more than one prediction model can be used to predict an outcome. The business context provides the first deciding factor in selecting a model. For example, if your goal is to extract a set of rules from the model, a rul*e induction model* is the only choice. Alternatively, if the model itself is of no interest but must nevertheless be as accurate as possible, then any of the models could be a candidate for the task. When one class of model is preferred but there are many models within that class, how do you choose a specific model?\n\nEach model has different characteristics when it comes to they way in which:\n\n- Missing values are handled\n- Continuous predictors are handled\n- Categorical predictors are handled\n- Outliers are handled\n- The number of predictors impacts prediction\n- The model scores data\n\nThere are many subtle differences between the models. In the end, however, it is always the business users who balance the pros and cons, and decide which model or combination of models should be used. There is a wide range of possibilities and it is only the business user who can decide what to do.\n\nData analytics and reporting tools such as KNIME provide for simplicity in building models. Machine learning models, rule induction models (decision trees), and statistical models can be built with great ease and speed, and their results compared. You must remember that data mining is an iterative process: models will be built, broken down, and often even combined before the user is satisfied with the results.\n\nOne final yet important point to keep in mind when building models is that software will only find rules or patterns in data if they actually exist. You cannot extract a model with high predictive accuracy if there are no associations between the predictors and dependent variables.\n\n# **Reference** \n\n**Course text:** UCI. (2020). [Introduction to Predictive Modeling. ](https://docs.google.com/document/d/1Hzpxlu7ypOIbBC8IjuNPG_Mx3ndYB9RFPcMYawpTUJU/edit?usp=sharing)\n\n","tags":["ML","Data Mining","DS"],"categories":["Predicted Analytics: Tools & Techniques"]},{"title":"Sysargv, Argparse & Function Docstring","url":"/docs/Immediate-to-Python-1-Sysargv-Argparse-and-Function-Docstring/","content":"\n# **Sysargv**\n\n```python\n'''\nThis is a simple script.\n'''\nimport sys\n\nprint(__doc__)\t# output: docstring\n\nprint(sys.argv)\t# output: System argument vector\n```\n\n# **Argparse**\n\n## Example 1\n\n```python\n'''\nThis is a simple script.\n'''\nimport argparse\n\nparser = argparse.ArgumentParser(description=__doc__)\nparser.add_argument('echo', help=\"the string you want to write to the file\")\nargs = parser.parse_args()\nprint(args.echo)\n```\n\n**Output:**\n\n![image-20230226181135214](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230226181135214.png)\n\n## Example 2\n\n```python\nimport argparse\n\nparser = argparse.ArgumentParser(description=__doc__)\nparser.add_argument('x', help=\"the value for x\", type=int)\nparser.add_argument('y', help=\"the value for y\", type=int)\nparser.add_argument('-f', '--formula', help=\"the formula you'd like to run\",\n                    choices=[\"power\", \"subtract\"], default=\"power\")\n\nargs = parser.parse_args()\n\nif args.formula == \"power\":\n    print(args.x ** args.y)\nelif args.formula == \"subtract\":\n    print(args.x - args.y)\n\n```\n\n**Output:**\n\n![image-20230226183445409](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230226183445409.png)\n\n# **Function Docstring**\n\n## One-line Docstring\n\nThis is a simple function with a simple one-line docstring. Notice how the docstring is written as a command, \"Return the value...\" not a description \"This function returns the value...\"\n\n```py\ndef add_2(num):\n    \"\"\"Return the value of num + 2.\"\"\"\n    return_num = num + 2\n    return num\n```\n\n### Printing Docstrings\n\nWe can print doc strings by printing the **__doc__** attribute of the function.  This is a built-in attribute and all objects have it (even if the value is None).\n\nMany editors and IDEs have special functionality / commands to print docstrings. For example, if you write the '?' key after a function name, in Jupyter Notebook, and then evaluate the cell, it will open a window with the docstrings.  See the example below.\n\n```py\n# Below is an example of printing the docstring directly\nprint(add_2.__doc__)\n```\n\n![image-20230226185514333](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230226185514333.png)\n\n```py\nhelp(add_2)\n```\n\n![image-20230226185537947](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230226185537947.png)\n\n```python\n# Below is an example of using Jupyter Notebooks '?' command\nadd_2?\n```\n\n## Multi-line Docstrings\n\nA multi-line docstring provide more information but it still starts with single line description, followed by a blank line, and then a more detailed description. The more detailed description includes a description of the arguments, the return value(s), exceptions that the function raises, and any side effects.  It may also included references to similar functions and other helpful information.\n\n```py\ndef circle(radius):\n    \"\"\"Return the circumference and area of a circle, given the radius.\n    \n    Parameters\n    ----------\n    radius : float, int\n        the radius of the circle.\n        \n    Returns\n    -------\n    circumference : float\n        the circumference of the circle.\n    \n    area : float\n        the area of the circle.\n    \n    \"\"\"\n    circumference = 2*math.pi*radius\n    area = math.pi*(radius**2)\n    return circumference, area\n```\n\n### Printing Docstrings\n\n```python\n# method 1\nprint(circle.__doc__)\n\n# method 2\ncircle?\n```\n\n![image-20230226191611138](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230226191611138.png)","tags":["Python"],"categories":["Immediate to Python"]},{"title":"1 Introduction to Predictive Analytics","url":"/docs/Predicted-Analytics-1-Introduction-to-Predictive-Analytics/","content":"\n# **The Data Mining Process**\n\n***Data mining* is a general term that encompasses a number of data analysis techniques used to extract meaningful information from (large) data** files without necessarily having preconceived notions about what will be discovered. The useful information often consists of patterns and relationships in the data that were **previously unknown or even unsuspected**.\n\nA common misconception is that data mining involves passing huge amounts of data through intelligent technologies that find patterns and give magical solutions to business problems. This is not true, although data mining does involve more automation than one typically finds in traditional statistical analyses.\n\n**Data mining is an interactive and iterative process.** Business expertise must be used together with advanced technologies to identify underlying relationships and features in the data. A seemingly useless pattern in data discovered by data mining can often be transformed into a valuable piece of actionable information using business experience and expertise.\n\nMany of the techniques used in data mining are referred to as ***modeling*** and require a different approach for model generation and testing compared to standard, traditional statistics. While **traditional statistics focuses on probabilities and hypothesis** testing using data-specific research, **data mining focuses on using historical data** accumulated during the normal course of business. It is then the responsibility of the data miner to select, prepare, and analyze the data to determine whether it is acceptable and likely to generalize to the population of interest. Due to the typically large files involved and the weak assumptions made about the distribution of the data, **data mining tends to be less focused on statistical significance tests and more focused on practical importance.**\n\nData mining has been used in hundreds of applications, including:\n\n- Detecting fraudulent financial activity;\n- Identifying specific purchases that are more likely to lead to additional purchases;\n- Classifying customers into groups based on distinct purchase or usage patterns; and\n- Predicting which page a website visitor will visit next.\n\n# **The Art and Practice of Data Mining**\n\nThis is the definition that Keith McCormick has previously used in books and presentations. \n\n> *Data mining is the selection and analysis of data accumulated during the normal course of business. The goal is to find (and confirm) previously unknown relationships that can be used to develop predictive models that, when applied to new data, can produce valuable insight for making business decisions. Several points are worth emphasizing:*\n>\n> - *The data is not new.*\n> - *The data is not collected solely to perform data mining.*\n> - *The data miner is not testing known relationships (neither hypotheses nor hunches) against the data.*\n> - *The patterns must be verifiable.*\n> - *The resulting models must be capable of something useful.*\n> - *The resulting models must actually work when deployed on new data.*\t\n\nFor additional information on data mining, please visit [https://keithmccormick.com/data mining-defined/](https://keithmccormick.com/data-mining-defined/)\n\n# **Introducing CRISP-DM**\n\nThe typical data mining process can become complicated very quickly. There is much to keep track ofâ€”complex business problems, multiple data sources, varying data quality across data sources, an array of data mining techniques, different ways of measuring data mining success, and so on. To stay on track, it helps to have an explicitly defined process model for data mining that can guide you through critical issues and ensure that important points are addressed. This process model can serve as a data mining road map that helps you stay on course as you dig into the complexities of the data.\n\nThe data mining process model we recommend is the ***CRoss-Industry Standard Process for Data Mining* (CRISP-DM)**, which is considered the de facto standard for conducting a data mining project. As you can tell from the name, this model is designed as a general model that can be applied to a wide variety of business problems in just about any industry. The model includes six phases starting with *business understanding*.\n\n![image-20230223173559821](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230223173559821.png)\n\n![image-20230223173655160](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230223173655160.png)\n\n## Business Understanding\n\nThe business objectives and question(s) to be answered, and formulating a concrete plan for proceeding through the data mining process. You need to:\n\n- Identify business objectives and success criteria;\n- Perform a situational assessment (resources, constraints, assumptions, risks, costs, and benefits);\n- Determine the goals of the data mining project;\n- Identify success criteria; and\n- Produce a project plan.\n\n## Data Understanding\n\nOnce the business understanding phase is complete, youâ€™re ready to begin collecting data and associated relevant information (such as the source of the data and the manner in which it was collected) for use in the project. It is also crucial to meet with subject matter experts (SMEs) to review what you have been collecting to ensure completeness (i.e., that no necessary data is missing) and verify your understanding of the data. It is also important to discuss the data in the context of the business problem youâ€™re addressing. Sometimes, it may be necessary to return to the business understanding phase before proceeding.\n\nWith data in hand, you can begin exploring it and becoming thoroughly familiar with its characteristics. For each field in your dataset, you should review **the distribution, range (for continuous fields), outliers, anomalies, and missing values (type and amount)**. You can also begin looking for obvious, interesting patterns in the data such as relationships between a predictor and a target field. Youâ€™ll need to:\n\n- Understand your data resources;\n- Know the characteristics of the data;\n- Describe the data;\n- Explore the data; and\n- Verify data quality.\n\n## Data Preparation\n\nAfter cataloging your data resources, itâ€™s time to prepare your data for mining. Data preparation is by far the most time-consuming step in the data mining process. **Various estimates suggest that 70% to 90% of the time spent on a data mining project is allocated to data preparation;** this is because you are using data that was collected for other reasons (for normal business operations, not for data mining). Preparations include:\n\n- Selecting data;\n- Cleaning data;\n- Constructing data;\n- Integrating data; and\n- Formatting data.\n\nThese tasks will likely be performed multiple times and not in any prescribed order. They can be very time-consuming but are critical for the success of the data mining project. In particular, data construction is a critical aspect of data preparation. Models work much better when the variables have been adjusted (e.g., by creating ratios, determining change scores, and calculating total scores) to make patterns appear more clearly. \n\n## Modeling\n\n![image-20230223173722789](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20230223173722789.png)\n\nOnce you have prepared the data, you are ready for modeling, which involves sophisticated analytical methods that can extract information from the data. This phase involves:\n\n- Selecting the modeling technique;\n- Generating a test design;\n- Building the model; and\n- Assessing the model.\n- Validation\n  - Split the data\n    - Training: Build the model\n    - Testing: Test the model with testing dataset and compare the results with observed data\n\nDeveloping a model is an iterative process and you can expect to try several models and modeling techniques before finding the best one. **One feature that separates data mining from other approaches is the use of multiple models to make predictions, building on the strengths of each technique.**\n\nAn important part of model development is ***validating*** the modelâ€™s predictive capability. Briefly, the process involves dividing your dataset into two parts: **a *training dataset* and a *testing dataset***. You develop your model using the training data and then test it by making predictions using the testing data. If the predictions made using the two datasets are in agreement, you can begin applying the model to new data. In summary, you conclude that the model is valid by demonstrating that it applies to (fits) a dataset that is independent of the one used in the modelâ€™s derivation. Statisticians often recommend such validation for statistical models, generally, but it is especially important when employing data mining techniques. \n\n## Evaluation\n\nOnce you have chosen your models, you are ready to **evaluate how the data mining results can help you achieve your business objectives**. At this stage of the project, you have built one or more models that appear to be of high quality from a data analysis perspective. Before writing final reports and deploying the model, however, it is important to more thoroughly evaluate the model and review the steps taken in constructing the model so you can be certain it properly achieves your business objectives. A key aim is to determine if there are any important business issues that have not been sufficiently considered. At the end of this phase, a decision will be made on the use of the data mining results. The evaluation phase tasks are:\n\n- Evaluate results;\n- Review the process; and\n- Determine the next steps.\n\nEvaluation is frequently confused with *model **assessment***â€”the last task of the modeling phase. **Assessing the model focuses on the â€œdata analysis perspectiveâ€** **and includes metrics like model accuracy and stability.** The authors of CRISP-DM considered calling this phase *business evaluation* because it has to be conducted in the language of the business using the metrics of the business as indicators of success. The value of a predictive model arises in two ways (Khabaza, 2010):\n\n1. The modelâ€™s predictions lead to improved (more effective) action; and\n2. The model delivers insight (new knowledge), which leads to improved strategy.\n\nKeep in mind that the value of a predictive model is not determined by any technical measure. Data miners should *not* focus on predictive accuracy, model stability, or any other technical metric for predictive models at the expense of business insight and business fit.\n\n## Deployment\n\nDepending on the business requirements, deployment can be as simple as generating a report or as complex as implementing a repeatable data mining process. **Keep in mind that creating the model is generally not the end of the project.** Even if the purpose of the model is only to increase oneâ€™s knowledge of the data, the knowledge gained will need to be organized and presented in a way that the organization can use for decision-making. So in essentially all projects, a final report will need to be produced and distributed.\n\n**Most critical is the deployment of the model to make predictions or create scores against new data.** This might be relatively simple if done within the data mining software you used, or more complex if the model is to be applied directly against an existing database. Whatever the case, a plan should be developed to monitor the modelâ€™s predictions and success in order to verify that the model is still valid.\n\nIn many projects, It is not unusual for the deployment team to be different from the modeling team; in some situations deployment may be the responsibility of team members having more of an IT focus. The tasks in the deployment phase are:\n\n- Plan the deployment;\n- Plan monitoring and maintenance activities;\n- Produce a final report; and\n- Review the project.\n\nSee also[ https://keithmccormick.com/crispdm](https://keithmccormick.com/crispdm) for additional information about CRISP-DM.\n\n# **Reference**\n\n**Course text:** UCI. (2020). [Introduction to Predictive Analytics.](https://docs.google.com/document/d/11zk4wIP0Hb_TeOo24fdTR8IX-lbGvWVCUHRvK563RUk/edit?usp=sharing)\n\n","tags":["Data Mining","DS"],"categories":["Predicted Analytics: Tools & Techniques"]},{"title":"Descriptive Analytics Data Visualization & Storytelling With Data","url":"/docs/Descriptive-Analytics-Data-Visualization-and-Storytelling-with-Data/","content":"\n**Readings:**\n\n1. [Introduction to Data Analytics and Data-Driven Decision Making.](https://docs.google.com/document/d/1dThO2qtamnsh5sB9aYIwUUvnMRMOpDGhuuYR8KKYw1Q/edit?usp=sharing)\n2. [Analysis and Data Exploration.](https://docs.google.com/document/d/1ItWcrCbE9BVJRNiAq3_UddVZ4lAmW7W_F7p_czgT4hA/edit?usp=sharing)\n\n3. [Creating Business Value through Data Presentation.](https://docs.google.com/document/d/1DcSZldqFKi_VVJD1lV1NyyMb-ta49ZvDewUsOJN_QxA/edit?usp=sharing)\n4. [Introduction to Descriptive Analytics.](https://docs.google.com/document/d/1tJR-OEVltVD8fHzJLEoJ-Ncg4F84y-T3t3ldtaPTS-c/edit?usp=sharing)\n5. [Introduction to Diagnostics Analytics.](https://docs.google.com/document/d/12veeK_yI_KUGKtCCCol9yGBV9GLsYALcj-YMLF2Vflw/edit?usp=sharing)\n6. [Introduction to Predictive Analytics.](https://docs.google.com/document/d/1sgn-PHP4qKIkl_NO36iYCbg50wLfKgo3e1NtH16twSk/edit?usp=sharing)\n7. [Introduction to Prescriptive Analytics.](https://docs.google.com/document/d/1hEeO0PHrxzVpCeQm8yRq3iPQhjDjJXzepJQ6kHZh_Ng/edit?usp=sharing)\n8. [The Future of Data Analytics.](https://docs.google.com/document/d/17edECjc4KAuY22e9v5pRwhdcpHjvqqgdziqCpVH1MWg/edit?usp=sharing)\n\n","tags":["DS"],"categories":["Data Analytics"]},{"title":"Intro to Analyzing Data for Business","url":"/docs/Intro-to-Analyzing-Data-for-Business/","content":"\n**Readings:**\n\n1. [Descriptive Analytics and Data Visualization](https://docs.google.com/document/d/11EGIvW8j3YHpxcWA-jxTk2qbQm6yGudVBZShg51VLYI/edit?usp=sharing)\n2. [Univariate Descriptive Analytics. ](https://docs.google.com/document/d/19vZW66el7k71v4t_NeMW7yPioi8ggiHLUDFfAe9qq8U/edit?usp=sharing)\n3. [Multivariate Descriptive Analytics.](https://docs.google.com/document/d/1EIPnOk-VsfoC9tsvyiDjVlZGNofTpaYzXXz2uJ9-MV8/edit?usp=sharing)\n4. [Network and Spatial Analysis.](https://docs.google.com/document/d/1SJn7Kkfsurw7Wy7zSmbK-yttgzKhykK5K2-2QZwBYYg/edit?usp=sharing)\n\n5. [Data Through Time.](https://docs.google.com/document/d/13M5CpLT56B-2K8pceZjOjkSNQVHjZ3SvPpllO5Jv6yo/edit?usp=sharing)\n\n6. [From Data to Visual Understanding.](https://docs.google.com/document/d/1btDfrphVZVQoMrelD1JXcLjWAeHmU1tHGoeNeCRQJV0/edit?usp=sharing) \n7. [Perception and Communication.](https://docs.google.com/document/d/1bYj9dpGDHz6VL55xcCLx7bbloQqu9lasGIeiLl4sLjE/edit?usp=sharing)\n8. [Going Beyond.](https://docs.google.com/document/d/1IltLJgC79jAuX0tT1RObB9cKQ_sviXwwLYR6b3IyafE/edit?usp=sharing)\n","tags":["DS"],"categories":["Data Analytics"]},{"title":"TO-DO List","url":"/docs/Other-TO-DO-list/","content":"\n# **To Be Continued**\n\n- [ ] Hung-yi Lee's Machine Learning\n\n- [ ] Intermediate to Python\n\n- [x] Predictive Analytics\n\n- [x] Introduce to Data Analytics for Business\n\n- [x] Descriptive Analytics: Visualization  \n\n# **About Website**\n\n- [ ] ç›®å½•äºŒçº§æ ‡é¢˜ç¼©è¿›\n\n- [x] Motify `Archives` and `Tags`\n\n- [x] å…¬å¼è¶…å‡ºå¡ç‰‡èŒƒå›´ [Reference]( https://docs.mathjax.org/en/latest/options/output/index.html#options-common-to-all-output-processors)\n\n- [x] å®žçŽ°ç›®å½•åˆ†çº§æ˜¾ç¤º \n\n- [x] è°ƒæ•´å›¾ç‰‡å¤§å°\n\n- [x] å›¾ç‰‡ç‚¹å‡»é¢„è§ˆ Fansy Box\n\n- [x] æœç´¢ä¼˜åŒ–\n\n- [x] <mark>ä¿®æ”¹é«˜äº®é¢œè‰²</mark>\n\n- [x] ä¿®æ”¹ä»£ç å—é«˜äº®\n\n- [x] å®žçŽ°tableå±…ä¸­æ˜¾ç¤º\n\n- [x] æ–‡ç« ç½®é¡¶\n\n- [x] å®žçŽ°homeé¡µé¢å®½å±å¤§å›¾ç‰‡\n\n- [x] è¡¨æ ¼å®½åº¦æ ¹æ®å†…å®¹å®½åº¦è¿›è¡Œè°ƒæ•´\n\n- [x] å®žçŽ°Search\n\n- [x] Emoji :thinking:\n\n- [x] å­—ä½“è‡ªé€‚åº”çª—å£å¤§å°\n\n- [x] è®¾è®¡æ»‘åŠ¨æ \n\n\n\n\n\n\n\n","categories":["Other"]},{"title":"1 Introduction of Deep Learning","url":"/docs/Machine-Learning-1-Introduction-of-Deep-Learning/","content":"\n# **Machine Learning â‰ˆ Looking for function**\n\n![image-20221206053236150](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221206053236150.png)\n\n# **Different Types of functions**\n\n- **Regression**: The function outputs a scalar\n- **Classification**: Given options (classes), the function outputs the correct one.\n- **Structured Learning:** create something with structure (image, document)\n\n![image-20221206053348699](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221206053348699.png)\n\n# **How to find a function**\n\nA case study\n\n## The function we want to find â€¦\n\n![image-20221206053424557](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221206053424557.png)\n\n![image-20221208221245405](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221208221245405.png)\n\n![image-20221208223257254](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221208223257254.png)\n\n![image-20221208223309541](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221208223309541.png)\n\n![image-20221208223318275](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221208223318275.png)\n\n![image-20221208223349650](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221208223349650.png)\n\n![image-20221208223401234](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221208223401234.png)\n\n![image-20221208223418427](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221208223418427.png)\n\n# **ML Framework**\n\n![image-20221206053857892](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221206053857892.png)\n\n## Step1. Model\n\ndepend on domain knowledge\n\n### Linear Models\n\nhave model bias (limitation), \n\n$$\ny = b + \\sum_{j=1}^{n}w_jx_j\n$$\n\n\n![image-20221206054003950](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221206054003950.png)\n\n![image-20221206054205148](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221206054205148.png)\n\n![image-20221206054210957](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221206054210957.png)\n\n### Sigmoid Function\n\n![image-20221206054229272](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221206054229272.png)\n\n![image-20221206054237612](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221206054237612.png)\n\n$$\ny=b+\\sum_{i}c_isigmoid(b_i+w_ix_i)\n$$\n\n### New Model: More Features\n\n![image-20221206054403705](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221206054403705.png)\n\nhyperparameter: \n\n* $i$ : no. of features\n* $j$ : no. of sigmoid\n\n![image-20221206054613829](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221206054613829.png)\n\n![image-20221206054619581](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221206054619581.png)\n\n### ReLu\n\n![image-20221206054710081](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221206054710081.png)\n\n### Deeper Model\n\n![image-20221206054747992](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221206054747992.png)\n\n## Step2. Loss\n\n![image-20221206054758334](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221206054758334.png)\n\n## Step 3. optimization\n\n![image-20221206054825545](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221206054825545.png)\n\n![image-20221206054848495](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221206054848495.png)\n\n![image-20221206054855172](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221206054855172.png)\n\n# **Deep Learning**\n\n![image-20221206054927207](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221206054927207.png)\n\n![image-20221208223439982](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221208223439982.png)\n\n\n\n","tags":["ML"],"categories":["HUNG-YI LEE | MACHINE LEARNING"]},{"title":"Class Intro","url":"/docs/Machine-Learning-0-Course-Intro/","content":"\n# **How to find a function**\n\n## Lecture 1-5: Supervised Learning\n\n![image-20221208230201056](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221208230201056.png)\n\n**Limit**: it is not easy to label for every assignments\n\n## Lecture 7: Self-Supervised Learning\n\n![image-20221208230316476](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221208230316476.png)\n\n![image-20221208230321680](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221208230321680.png)\n\n## Lecture 6: Generative Adversarial Network\n\n![image-20221208230347313](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221208230347313.png)\n\n## Lecture 12: Reinforcement Learning\n\n![image-20221208230407336](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221208230407336.png)\n\n## Lecture 8: Anomaly Detection\n\n![image-20221208230425964](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221208230425964.png)\n\n## Lecture 9: Explainable AI\n\n![image-20221208230443881](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221208230443881.png)\n\n## Lecture 10: Model Attack\n\n![image-20221208230457959](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221208230457959.png)\n\n## Lecture 11: Domain Adaption\n\n![image-20221208230520016](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221208230520016.png)\n\n## Lecture 13: Network Compression\n\n![image-20221208230533994](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221208230533994.png)\n\n## Lecture 14: Life-Long Learning\n\n![image-20221208230551887](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221208230551887.png)\n\n## Lecture 15: Met Learning\n\n![image-20221208230612831](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221208230612831.png)\n\n","tags":["ML"],"categories":["HUNG-YI LEE | MACHINE LEARNING"]},{"title":"Image Classification | Paper","url":"/docs/Computer-Vision-Image-Classification-Paper/","content":"\n- LeNet http://yann.lecun.com/exdb/lenet/index.html\n\n- AlexNet http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n\n- ZFNet Visualizing and Understanding Convolutional Networks https://arxiv.org/abs/1311.2901\n\n- VGG https://arxiv.org/abs/1409.1556\n\n- GoogLeNet, Inceptionv1(Going deeper with convolutions) https://arxiv.org/abs/1409.4842\n\n- Batch Normalization https://arxiv.org/abs/1502.03167\n\n- Inceptionv3(Rethinking the Inception Architecture for Computer Vision) https://arxiv.org/abs/1512.00567\n\n- Inceptionv4, Inception-ResNet https://arxiv.org/abs/1602.07261\n\n- Xception(Deep Learning with Depthwise Separable Convolutions) https://arxiv.org/abs/1610.02357\n\n- ResNet https://arxiv.org/abs/1512.03385\n\n- ResNeXt https://arxiv.org/abs/1611.05431\n\n- DenseNet https://arxiv.org/abs/1608.06993\n\n- NASNet-A(Learning Transferable Architectures for Scalable Image Recognition) https://arxiv.org/abs/1707.07012\n\n- SENet(Squeeze-and-Excitation Networks) https://arxiv.org/abs/1709.01507\n\n- MobileNet(v1) https://arxiv.org/abs/1704.04861\n\n- MobileNet(v2) https://arxiv.org/abs/1801.04381\n\n- MobileNet(v3) https://arxiv.org/abs/1905.02244\n\n- ShuffleNet(v1) https://arxiv.org/abs/1707.01083\n\n- ShuffleNet(v2) https://arxiv.org/abs/1807.11164\n\n- Bag of Tricks for Image Classification with Convolutional Neural Networks https://arxiv.org/abs/1812.01187\n\n- EfficientNet(v1) https://arxiv.org/abs/1905.11946\n\n- EfficientNet(v2) https://arxiv.org/abs/2104.00298\n\n- CSPNet https://arxiv.org/abs/1911.11929\n\n- RegNet https://arxiv.org/abs/2003.13678\n\n- NFNets(High-Performance Large-Scale Image Recognition Without Normalization) https://arxiv.org/abs/2102.06171\n\n- Vision Transformer https://arxiv.org/abs/2010.11929\n\n- DeiT(Training data-efficient image transformers ) https://arxiv.org/abs/2012.12877\n\n- Swin Transformer https://arxiv.org/abs/2103.14030\n\n- Swin Transformer V2: Scaling Up Capacity and Resolution https://arxiv.org/abs/2111.09883\n\n- BEiT: BERT Pre-Training of Image Transformers https://arxiv.org/abs/2106.08254\n\n- MAE(Masked Autoencoders Are Scalable Vision Learners) https://arxiv.org/abs/2111.06377\n\n- ConvNeXt(A ConvNet for the 2020s) https://arxiv.org/abs/2201.03545","tags":["CV"],"categories":["Computer Vision"]},{"title":"Cross Entropy","url":"/docs/Other-ML-Cross-Entropy/","content":"\näº¤å‰ç†µ (Cross Entropy) æ˜¯æ·±åº¦å­¦ä¹ ä¸­å¸¸ç”¨çš„ä¸€ä¸ªæ¦‚å¿µï¼Œä¸€èˆ¬ç”¨æ¥æ±‚ç›®æ ‡ä¸Žé¢„æµ‹å€¼ä¹‹é—´çš„å·®è·ã€‚\n\n<!--MORE-->\n\näº¤å‰ç†µæ˜¯ä¿¡æ¯è®ºä¸­çš„ä¸€ä¸ªæ¦‚å¿µï¼Œè¦æƒ³äº†è§£äº¤å‰ç†µçš„æœ¬è´¨ï¼Œéœ€è¦å…ˆä»Žæœ€åŸºæœ¬çš„æ¦‚å¿µè®²èµ·ã€‚\n\n# **1 ä¿¡æ¯é‡**\né¦–å…ˆæ˜¯ä¿¡æ¯é‡ã€‚å‡è®¾æˆ‘ä»¬å¬åˆ°äº†ä¸¤ä»¶äº‹ï¼Œåˆ†åˆ«å¦‚ä¸‹ï¼š\n\n> äº‹ä»¶Aï¼šå·´è¥¿é˜Ÿè¿›å…¥äº†2018ä¸–ç•Œæ¯å†³èµ›åœˆã€‚\n> äº‹ä»¶Bï¼šä¸­å›½é˜Ÿè¿›å…¥äº†2018ä¸–ç•Œæ¯å†³èµ›åœˆã€‚\n\nä»…å‡­ç›´è§‰æ¥è¯´ï¼Œæ˜¾è€Œæ˜“è§äº‹ä»¶Bçš„ä¿¡æ¯é‡æ¯”äº‹ä»¶Açš„ä¿¡æ¯é‡è¦å¤§ã€‚ç©¶å…¶åŽŸå› ï¼Œæ˜¯å› ä¸ºäº‹ä»¶Aå‘ç”Ÿçš„æ¦‚çŽ‡å¾ˆå¤§ï¼Œäº‹ä»¶Bå‘ç”Ÿçš„æ¦‚çŽ‡å¾ˆå°ã€‚æ‰€ä»¥å½“è¶Šä¸å¯èƒ½çš„äº‹ä»¶å‘ç”Ÿäº†ï¼Œæˆ‘ä»¬èŽ·å–åˆ°çš„ä¿¡æ¯é‡å°±è¶Šå¤§ã€‚è¶Šå¯èƒ½å‘ç”Ÿçš„äº‹ä»¶å‘ç”Ÿäº†ï¼Œæˆ‘ä»¬èŽ·å–åˆ°çš„ä¿¡æ¯é‡å°±è¶Šå°ã€‚é‚£ä¹ˆä¿¡æ¯é‡åº”è¯¥å’Œäº‹ä»¶å‘ç”Ÿçš„æ¦‚çŽ‡æœ‰å…³ã€‚\n\nå‡è®¾$X$æ˜¯ä¸€ä¸ªç¦»æ•£åž‹éšæœºå˜é‡ï¼Œå…¶å–å€¼é›†åˆä¸º$x$,æ¦‚çŽ‡åˆ†å¸ƒå‡½æ•° $p(x)=Pr(X=x)$, $xâˆˆÏ‡$åˆ™å®šä¹‰äº‹ä»¶$X=x_0$çš„ä¿¡æ¯é‡ä¸ºï¼š\n\n$$\nI(x_0)=âˆ’log(p(x_0))\n$$\n\nç”±äºŽæ˜¯æ¦‚çŽ‡æ‰€ä»¥$p(x_0)$çš„å–å€¼èŒƒå›´æ˜¯ $[0,1]$,ç»˜åˆ¶ä¸ºå›¾å½¢å¦‚ä¸‹ï¼Œå¯è§è¯¥å‡½æ•°ç¬¦åˆæˆ‘ä»¬å¯¹ä¿¡æ¯é‡çš„ç›´è§‰ã€‚\n\n![image-20221111062251336](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20221111062251336.png)\n\n# **2 ç†µ**\n\nè€ƒè™‘å¦ä¸€ä¸ªé—®é¢˜ï¼Œå¯¹äºŽæŸä¸ªäº‹ä»¶ï¼Œæœ‰$n$ç§å¯èƒ½æ€§ï¼Œæ¯ä¸€ç§å¯èƒ½æ€§éƒ½æœ‰ä¸€ä¸ªæ¦‚çŽ‡$p(xi)$\nè¿™æ ·å°±å¯ä»¥è®¡ç®—å‡ºæŸä¸€ç§å¯èƒ½æ€§çš„ä¿¡æ¯é‡ã€‚ä¸¾ä¸€ä¸ªä¾‹å­ï¼Œå‡è®¾ä½ æ‹¿å‡ºäº†ä½ çš„ç”µè„‘ï¼ŒæŒ‰ä¸‹å¼€å…³ï¼Œä¼šæœ‰ä¸‰ç§å¯èƒ½æ€§ï¼Œä¸‹è¡¨åˆ—å‡ºäº†æ¯ä¸€ç§å¯èƒ½çš„æ¦‚çŽ‡åŠå…¶å¯¹åº”çš„ä¿¡æ¯é‡\n\n| åºå· | äº‹ä»¶         | æ¦‚çŽ‡ $p$ | ä¿¡æ¯é‡ $I$         |\n| ---- | ------------ | ----- | ------------------- |\n| A    | ç”µè„‘æ­£å¸¸å¼€æœº | 0.7   | $-log(p(A))=0.36$   |\n| B    | ç”µè„‘æ— æ³•å¼€æœº | 0.2   | $-log(p(B))=1.61$ |\n| C    | ç”µè„‘çˆ†ç‚¸äº†   | 0.1   | $-log(p(C))=2.30$ |\n\n> æ³¨ï¼šæ–‡ä¸­çš„å¯¹æ•°å‡ä¸ºè‡ªç„¶å¯¹æ•°\n\næˆ‘ä»¬çŽ°åœ¨æœ‰äº†ä¿¡æ¯é‡çš„å®šä¹‰ï¼Œè€Œç†µç”¨æ¥è¡¨ç¤ºæ‰€æœ‰ä¿¡æ¯é‡çš„æœŸæœ›ï¼Œå³ï¼š\n\n$$\nH(X)=âˆ’\\sum_{i=1}^np(x_i)log(p(x_i))\n$$\n\nå…¶ä¸­$n$ä»£è¡¨æ‰€æœ‰çš„$n$ç§å¯èƒ½æ€§ï¼Œæ‰€ä»¥ä¸Šé¢çš„é—®é¢˜ç»“æžœå°±æ˜¯\n\n$$\n\\begin{aligned}\nH(X) &= âˆ’[p(A)log(p(A))+p(B)log(p(B))+p(C))log(p(C))]\n\\\\&= 0.7Ã—0.36+0.2Ã—1.61+0.1Ã—2.30\n\\\\&= 0.804\n\\end{aligned}\n$$\n\nç„¶è€Œæœ‰ä¸€ç±»æ¯”è¾ƒç‰¹æ®Šçš„é—®é¢˜ï¼Œæ¯”å¦‚æŠ•æŽ·ç¡¬å¸åªæœ‰ä¸¤ç§å¯èƒ½ï¼Œå­—æœä¸Šæˆ–èŠ±æœä¸Šã€‚ä¹°å½©ç¥¨åªæœ‰ä¸¤ç§å¯èƒ½ï¼Œä¸­å¥–æˆ–ä¸ä¸­å¥–ã€‚æˆ‘ä»¬ç§°ä¹‹ä¸º0-1åˆ†å¸ƒé—®é¢˜ï¼ˆäºŒé¡¹åˆ†å¸ƒçš„ç‰¹ä¾‹ï¼‰ï¼Œå¯¹äºŽè¿™ç±»é—®é¢˜ï¼Œç†µçš„è®¡ç®—æ–¹æ³•å¯ä»¥ç®€åŒ–ä¸ºå¦‚ä¸‹ç®—å¼ï¼š\n\n$$\n\\begin{aligned}\nH(X)&=âˆ’\\sum_{i=1}^np(xi)log(p(xi))\\\\\n&=âˆ’p(x)log(p(x))âˆ’(1âˆ’p(x))log(1âˆ’p(x))\n\\end{aligned}\n$$\n\n# **3 ç›¸å¯¹ç†µï¼ˆKLæ•£åº¦ï¼‰**\nç›¸å¯¹ç†µåˆç§°KLæ•£åº¦,å¦‚æžœæˆ‘ä»¬å¯¹äºŽåŒä¸€ä¸ªéšæœºå˜é‡ $ x $ æœ‰ä¸¤ä¸ªå•ç‹¬çš„æ¦‚çŽ‡åˆ†å¸ƒ $ P(x) $ å’Œ $ Q(x)$ ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ KL æ•£åº¦ï¼ˆKullback-Leibler divergenceï¼‰æ¥è¡¡é‡è¿™ä¸¤ä¸ªåˆ†å¸ƒçš„å·®å¼‚\n\n> ç»´åŸºç™¾ç§‘å¯¹ç›¸å¯¹ç†µçš„å®šä¹‰\n> In the context of machine learning, $D_{KL}(pâ€–q) $ is often called the information gain achieved if $P$ is used instead of $Q$.\n\nå³å¦‚æžœç”¨ $P$ æ¥æè¿°ç›®æ ‡é—®é¢˜ï¼Œè€Œä¸æ˜¯ç”¨  $ Q$  æ¥æè¿°ç›®æ ‡é—®é¢˜ï¼Œå¾—åˆ°çš„ä¿¡æ¯å¢žé‡ã€‚\n\nåœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œ$P$ å¾€å¾€ç”¨æ¥è¡¨ç¤ºæ ·æœ¬çš„çœŸå®žåˆ†å¸ƒï¼Œæ¯”å¦‚ $[1,0,0]$ è¡¨ç¤ºå½“å‰æ ·æœ¬å±žäºŽç¬¬ä¸€ç±»ã€‚$Q$ ç”¨æ¥è¡¨ç¤ºæ¨¡åž‹æ‰€é¢„æµ‹çš„åˆ†å¸ƒï¼Œæ¯”å¦‚ $[0.7,0.2,0.1]$\n\nç›´è§‚çš„ç†è§£å°±æ˜¯å¦‚æžœç”¨ $P$ æ¥æè¿°æ ·æœ¬ï¼Œé‚£ä¹ˆå°±éžå¸¸å®Œç¾Žã€‚è€Œç”¨ $Q$ æ¥æè¿°æ ·æœ¬ï¼Œè™½ç„¶å¯ä»¥å¤§è‡´æè¿°ï¼Œä½†æ˜¯ä¸æ˜¯é‚£ä¹ˆçš„å®Œç¾Žï¼Œä¿¡æ¯é‡ä¸è¶³ï¼Œéœ€è¦é¢å¤–çš„ä¸€äº›â€œä¿¡æ¯å¢žé‡â€æ‰èƒ½è¾¾åˆ°å’Œ $P$ ä¸€æ ·å®Œç¾Žçš„æè¿°ã€‚å¦‚æžœæˆ‘ä»¬çš„ $Q$ é€šè¿‡åå¤è®­ç»ƒï¼Œä¹Ÿèƒ½å®Œç¾Žçš„æè¿°æ ·æœ¬ï¼Œé‚£ä¹ˆå°±ä¸å†éœ€è¦é¢å¤–çš„â€œä¿¡æ¯å¢žé‡â€ï¼Œ $Q$ ç­‰ä»·äºŽ$P$ã€‚\n\nKLæ•£åº¦çš„è®¡ç®—å…¬å¼ï¼š\n\n$$\nD_{KL}(p||q)=\\sum_{i=1}^np(x_i)log(\\frac{p(x_i)}{q(x_i)})\n$$\n\n$n$ ä¸ºäº‹ä»¶çš„æ‰€æœ‰å¯èƒ½æ€§ã€‚\n\n$D_{KL}$çš„å€¼è¶Šå°ï¼Œè¡¨ç¤º $P$ åˆ†å¸ƒå’Œ $Q$ åˆ†å¸ƒè¶ŠæŽ¥è¿‘\n\n# **4 äº¤å‰ç†µ**\nå¯¹å¼*KLæ•£åº¦çš„è®¡ç®—å…¬å¼*å˜å½¢å¯ä»¥å¾—åˆ°ï¼š\n\n$$\n\\begin{aligned}\nD_{KL}(p||q)&=\\sum_{i=1}^np(xi)log(p(xi))âˆ’\\sum_{i=1}^np(xi)log(q(xi))\\\\\n&=âˆ’H(p(x))+[âˆ’\\sum_{i=1}^np(xi)log(q(xi))]\n\\end{aligned}\n$$\n\nç­‰å¼çš„å‰ä¸€éƒ¨åˆ†æ°å·§å°±æ˜¯ $p$ çš„ç†µï¼Œç­‰å¼çš„åŽä¸€éƒ¨åˆ†ï¼Œå°±æ˜¯äº¤å‰ç†µï¼š\n\n$$\nH(p,q)=âˆ’\\sum_{i=1}^np(xi)log(q(xi))\n$$\n\nåœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬éœ€è¦è¯„ä¼° `label` å’Œ `predicts` ä¹‹é—´çš„å·®è·ï¼Œä½¿ç”¨KLæ•£åº¦åˆšåˆšå¥½ï¼Œå³ $D_{KL}(y||\\hat{y})$ï¼Œç”±äºŽKLæ•£åº¦ä¸­çš„å‰ä¸€éƒ¨åˆ† $âˆ’H(y)$ ä¸å˜ï¼Œæ•…åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œåªéœ€è¦å…³æ³¨äº¤å‰ç†µå°±å¯ä»¥äº†ã€‚æ‰€ä»¥ä¸€èˆ¬åœ¨æœºå™¨å­¦ä¹ ä¸­ç›´æŽ¥ç”¨ç”¨äº¤å‰ç†µåš`loss`ï¼Œè¯„ä¼°æ¨¡åž‹ã€‚\n\n# **5 æœºå™¨å­¦ä¹ ä¸­äº¤å‰ç†µçš„åº”ç”¨**\n\n## 5.1 ä¸ºä»€ä¹ˆè¦ç”¨äº¤å‰ç†µåšlosså‡½æ•°ï¼Ÿ\n\nåœ¨çº¿æ€§å›žå½’é—®é¢˜ä¸­ï¼Œå¸¸å¸¸ä½¿ç”¨MSEï¼ˆMean Squared Errorï¼‰ä½œä¸ºlosså‡½æ•°ï¼Œæ¯”å¦‚ï¼š\n\n$$\nloss=\\frac{1}{2m}\\sum_{i=1}^m(y_iâˆ’\\hat{y_i})^2\n$$\n\nè¿™é‡Œçš„ $ m $  è¡¨ç¤º  $ m $ ä¸ªæ ·æœ¬çš„ï¼Œ $ loss $ ä¸º $ m $ ä¸ªæ ·æœ¬çš„ $ loss $ å‡å€¼ã€‚\n\nMSEåœ¨çº¿æ€§å›žå½’é—®é¢˜ä¸­æ¯”è¾ƒå¥½ç”¨ï¼Œé‚£ä¹ˆåœ¨é€»è¾‘åˆ†ç±»é—®é¢˜ä¸­è¿˜æ˜¯å¦‚æ­¤ä¹ˆï¼Ÿ\n\n## 5.2 äº¤å‰ç†µåœ¨å•åˆ†ç±»é—®é¢˜ä¸­çš„ä½¿ç”¨\nè¿™é‡Œçš„å•ç±»åˆ«æ˜¯æŒ‡ï¼Œæ¯ä¸€å¼ å›¾åƒæ ·æœ¬åªèƒ½æœ‰ä¸€ä¸ªç±»åˆ«ï¼Œæ¯”å¦‚åªèƒ½æ˜¯ç‹—æˆ–åªèƒ½æ˜¯çŒ«ã€‚\n\näº¤å‰ç†µåœ¨å•åˆ†ç±»é—®é¢˜ä¸ŠåŸºæœ¬æ˜¯æ ‡é…çš„æ–¹æ³•\n\n$$\nloss=âˆ’\\sum_{i=1}^ny_ilog(\\hat{y_i})\n$$\n\nä¸Šå¼ä¸ºä¸€å¼ æ ·æœ¬çš„  $ loss  $ è®¡ç®—æ–¹æ³•ã€‚å¼ä¸­ $ n  $ ä»£è¡¨ç€ $ n $ ç§ç±»åˆ«ã€‚\n\nä¸¾ä¾‹è¯´æ˜Žï¼Œæ¯”å¦‚æœ‰å¦‚ä¸‹æ ·æœ¬ï¼š\n\n![SouthEast-16489642743263](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/SouthEast-16489642743263.png)\n\nå¯¹åº”çš„æ ‡ç­¾å’Œé¢„æµ‹å€¼\n\n|       | çŒ«   | é’è›™ | è€é¼  |\n| ----- | ---- | ---- | ---- |\n| Label | 0    | 1    | 0    |\n| Pred  | 0.3  | 0.6  | 0.1  |\n\né‚£ä¹ˆ\n\n$$\n\\begin{aligned}\nloss&=âˆ’(0Ã—log(0.3)+1Ã—log(0.6)+0Ã—log(0.1)\\\\&=âˆ’log(0.6)\n\\end{aligned}\n$$\n\nå¯¹åº”ä¸€ä¸ªbatchçš„ $ loss $ å°±æ˜¯\n\n$$\nloss=âˆ’\\frac1m\\sum_{j=1}^m\\sum_{i=1}^ny_{ji}log(\\hat{y_{ji}})\n$$\n\n $ m $ ä¸ºå½“å‰ $ batch $ çš„æ ·æœ¬æ•°\n\n## 5.3 äº¤å‰ç†µåœ¨å¤šåˆ†ç±»é—®é¢˜ä¸­çš„ä½¿ç”¨\n\nè¿™é‡Œçš„å¤šç±»åˆ«æ˜¯æŒ‡ï¼Œæ¯ä¸€å¼ å›¾åƒæ ·æœ¬å¯ä»¥æœ‰å¤šä¸ªç±»åˆ«ï¼Œæ¯”å¦‚åŒæ—¶åŒ…å«ä¸€åªçŒ«å’Œä¸€åªç‹—\n\nå’Œå•åˆ†ç±»é—®é¢˜çš„æ ‡ç­¾ä¸åŒï¼Œå¤šåˆ†ç±»çš„æ ‡ç­¾æ˜¯**n-hot**ã€‚\n\næ¯”å¦‚ä¸‹é¢è¿™å¼ æ ·æœ¬å›¾ï¼Œå³æœ‰é’è›™ï¼Œåˆæœ‰è€é¼ ï¼Œæ‰€ä»¥æ˜¯ä¸€ä¸ªå¤šåˆ†ç±»é—®é¢˜ã€‚\n\n![SouthEast-16489643649396](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/SouthEast-16489643649396.png)\n\nå¯¹åº”çš„æ ‡ç­¾å’Œé¢„æµ‹å€¼ï¼š\n\n|       | çŒ«   | é’è›™ | è€é¼  |\n| ----- | ---- | ---- | ---- |\n| Label | 0    | 1    | 1    |\n| Pred  | 0.1  | 0.7  | 0.8  |\n\n\nå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™é‡Œçš„ `Pred` ä¸å†æ˜¯é€šè¿‡ `softmax` è®¡ç®—çš„äº†ï¼Œè¿™é‡Œé‡‡ç”¨çš„æ˜¯**sigmoid**ã€‚å°†æ¯ä¸€ä¸ªèŠ‚ç‚¹çš„è¾“å‡ºå½’ä¸€åŒ–åˆ° $ [0,1] $ ä¹‹é—´ã€‚æ‰€æœ‰ `Pred` å€¼çš„å’Œä¹Ÿä¸å†ä¸º1ã€‚æ¢å¥è¯è¯´ï¼Œå°±æ˜¯æ¯ä¸€ä¸ª `Label` éƒ½æ˜¯ç‹¬ç«‹åˆ†å¸ƒçš„ï¼Œç›¸äº’ä¹‹é—´æ²¡æœ‰å½±å“ã€‚æ‰€ä»¥äº¤å‰ç†µåœ¨è¿™é‡Œæ˜¯å•ç‹¬å¯¹æ¯ä¸€ä¸ªèŠ‚ç‚¹è¿›è¡Œè®¡ç®—ï¼Œæ¯ä¸€ä¸ªèŠ‚ç‚¹åªæœ‰ä¸¤ç§å¯èƒ½å€¼ï¼Œæ‰€ä»¥æ˜¯ä¸€ä¸ªäºŒé¡¹åˆ†å¸ƒã€‚å‰é¢è¯´è¿‡å¯¹äºŽäºŒé¡¹åˆ†å¸ƒè¿™ç§ç‰¹æ®Šçš„åˆ†å¸ƒï¼Œç†µçš„è®¡ç®—å¯ä»¥è¿›è¡Œç®€åŒ–ã€‚\n\nåŒæ ·çš„ï¼Œäº¤å‰ç†µçš„è®¡ç®—ä¹Ÿå¯ä»¥ç®€åŒ–ï¼Œå³\n\n$$\nloss=âˆ’ylog(\\hat{y})âˆ’(1âˆ’y)log(1âˆ’\\hat{y})\n$$\n\næ³¨æ„ï¼Œä¸Šå¼åªæ˜¯é’ˆå¯¹ä¸€ä¸ªèŠ‚ç‚¹çš„è®¡ç®—å…¬å¼ã€‚è¿™ä¸€ç‚¹ä¸€å®šè¦å’Œå•åˆ†ç±» $ loss $ åŒºåˆ†å¼€æ¥ã€‚\n\nä¾‹å­ä¸­å¯ä»¥è®¡ç®—ä¸ºï¼š\n\n$$\n\\begin{aligned}\nloss_{cat}&=âˆ’0Ã—log(0.1)âˆ’(1âˆ’0)log(1âˆ’0.1)=âˆ’log(0.9)\\\\\nloss_{frog}&=âˆ’1Ã—log(0.7)âˆ’(1âˆ’1)log(1âˆ’0.7)=âˆ’log(0.7)\\\\\nloss_{mouse}&=âˆ’1Ã—log(0.8)âˆ’(1âˆ’1)log(1âˆ’0.8)=âˆ’log(0.8)\n\\end{aligned}\n$$\n\nå•å¼ æ ·æœ¬çš„ $ loss $ å³ä¸º$loss=loss_{cat}+loss_{frog}+loss_{mouse}$\n\næ¯ä¸€ä¸ªbatchçš„losså°±æ˜¯ï¼š\n\n$$\nloss=\\sum_{j=1}^m\\sum_{i=1}^nâˆ’y_{ji}log(\\hat{y_{ji}})âˆ’(1âˆ’y_{ji})log(1âˆ’\\hat{y_{ji}})\n$$\n\nå¼ä¸­ $ m $ ä¸ºå½“å‰batchä¸­çš„æ ·æœ¬é‡ï¼Œ $ n $ ä¸ºç±»åˆ«æ•°ã€‚\n\n# **Reference**\n\nhttps://blog.csdn.net/tsyccnh/article/details/79163834\n","tags":["ML"],"categories":["Other"]},{"title":"Basic Knowledge of CNN","url":"/docs/Computer-Vision-The-Basic-Info-of-CNN/","content":"\nç®€å•ä»‹ç»å·ç§¯ç¥žç»ç½‘ç»œä¸­å¸¸è§çš„å…¨è¿žæŽ¥å±‚ï¼Œå·ç§¯å±‚ã€æ± åŒ–å±‚ä»¥åŠè¯¯å·®åå‘ä¼ æ’­è¿‡ç¨‹å’Œè®­ç»ƒä¼˜åŒ–å™¨çš„åŽŸç†ã€‚\n\n<!--more-->\n\n# **1 å·ç§¯ç¥žç»ç½‘ç»œ CNN**\n\n* å·ç§¯ç¥žç»ç½‘ç»œï¼Œå³åŒ…å«å·ç§¯å±‚çš„ç¥žç»ç½‘ç»œã€‚\n* ç¬¬ä¸€ä¸ªå·ç§¯ç¥žç»ç½‘è·¯ï¼šLeCunçš„LeNetï¼ˆ1998ï¼‰ç½‘ç»œç»“æž„\n\n![Net_LeNet](https://github.com/Jiayi-Zeng/Jiayi-Zeng.github.io/blob/pic/img/Net_LeNet_covoer.png?raw=true)\n\n## 1.1 CNNçš„å‘å±•\n\n![](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220327213127062.png)\n\n## 1.2 CNNçš„åº”ç”¨\n\nå›¾åƒæ£€æµ‹ã€å›¾åƒæ£€ç´¢ã€å›¾åƒæ£€æµ‹ã€å›¾åƒåˆ†å‰²ã€æ— äººé©¾é©¶ã€GPUã€å›¾åƒè¿ç§»\n\n![image-20220327213228684](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220327213228684.png)\n\n![image-20220327213244394](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220327213244394.png)\n\n![image-20220327213324662](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220327213324662.png)\n\n# **2 ç¥žç»å…ƒ**\n\n![image-20220403112023201](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220403112023201.png)\n\n$$\ny=f(x_1Â·w_1+x_2Â·w_2+x_3Â·w_3-1)\n$$\n\nå…¶ä¸­ï¼Œ$x$ä¸ºæ¿€åŠ±ï¼Œ$w$ä¸ºç¥žç»å…ƒè¿žæŽ¥æƒå€¼ï¼Œ$-1$ä¸ºåç½®ï¼Œ$f(x)$ä¸ºæ¿€æ´»å‡½æ•°ã€‚\n\n# **3 å…¨è¿žæŽ¥å±‚**\n\n![image-20220403113627551](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220403113627551.png)\n\n# **4 BPç¥žç»ç½‘ç»œ**\n\n## 4.1 BPç¥žç»ç½‘ç»œç®€ä»‹\n\n**BPï¼ˆBack Propagationï¼‰ç®—æ³•åŒ…æ‹¬ä¿¡å·çš„å‰å‘ä¼ æ’­å’Œè¯¯å·®çš„åå‘ä¼ æ’­ä¸¤ä¸ªè¿‡ç¨‹**ï¼Œå³è®¡ç®—è¯¯å·®è¾“å‡ºæ—¶æŒ‰ä»Žè¾“å…¥åˆ°è¾“å‡ºçš„æ–¹å‘è¿›è¡Œï¼Œè€Œè°ƒæ•´æƒå€¼å’Œé˜ˆå€¼åˆ™ä»Žè¾“å‡ºåˆ°è¾“å…¥çš„æ–¹å‘è¿›è¡Œã€‚\n\n## 4.2 BPç¥žç»ç½‘ç»œå®žä¾‹\n\n### 1.è¾“å…¥å±‚\n\n1. å½©è‰²RGBå›¾åƒ --> ç°åº¦åŒ–--> ç°åº¦å›¾åƒ --> äºŒå€¼åŒ–--> äºŒå€¼å›¾åƒ\n\n![image-20220327214232774](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220327214232774.png)\n\n2. çª—å£æ»‘åŠ¨ï¼šè®¡ç®—ç™½è‰²åƒç´ å æ•´ä¸ªæ¡†åƒç´ çš„æ¯”ä¾‹\n\n![image-20220327214413416](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220327214413416.png)\n\n3. æŒ‰è¡Œå±•å¼€ï¼Œæ‹¼æŽ¥æˆ**è¡Œå‘é‡ï¼ˆç¥žç»ç½‘ç»œè¾“å…¥å±‚ï¼‰**\n\n![image-20220327214508874](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220327214508874.png)\n\n### 2. è¾“å‡ºå±‚\n\n**one-hotç¼–ç **\n\n![image-20220327214704028](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220327214704028.png)\n\n### 3. ç¥žç»ç½‘ç»œ\n\n![image-20220327214728420](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220327214728420.png)\n\n# **5 å·ç§¯å±‚**\n\n**ç›®çš„ï¼š**è¿›è¡Œå›¾åƒç‰¹å¾æå–\n\n**ç‰¹æ€§ï¼š**æ‹¥æœ‰å±€éƒ¨æ„ŸçŸ¥æœºåˆ¶ï¼Œæƒå€¼å…±äº«ï¼ˆå‡å°‘å‚æ•°ï¼‰\n\n> **æƒå€¼å…±äº«**\n>\n> ![image-20220327215001204](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220327215001204.png)\n\n## 5.1 å·ç§¯è¿‡ç¨‹\n\n![SouthEast](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/SouthEast.gif)\n\n![image-20220327215117151](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220327215117151.png)\n\né€šè¿‡è§‚å¯Ÿå¯ä»¥å‘çŽ°ï¼š\n\n* **`å·ç§¯æ ¸channel`ä¸Ž`è¾“å…¥å±‚çš„channel`ç›¸åŒ**\n* **è¾“å‡ºçš„`ç‰¹å¾çŸ©é˜µchannel`ä¸Ž`å·ç§¯æ ¸ä¸ªæ•°`ç›¸åŒ**\n\n## 5.2 æ¿€æ´»å‡½æ•°\n\nç›®çš„ï¼š**å¼•å…¥éžçº¿æ€§å› ç´ ï¼Œä½¿å…¶å…·å¤‡è§£å†³éžçº¿æ€§é—®é¢˜çš„èƒ½åŠ›ã€‚**\n\n1. **`Sigmoid`æ¿€æ´»å‡½æ•°**\n\n* é¥±å’Œæ—¶æ¢¯åº¦éžå¸¸å°ï¼Œæ•…ç½‘ç»œå±‚æ•°è¾ƒæ·±æ—¶æ˜“å‡ºçŽ°æ¢¯åº¦æ¶ˆå¤±ã€‚\n* è®¡ç®—å¤šç±»æŸå¤±æœ€åŽä½¿ç”¨`softmax`æ¿€æ´»å‡½æ•°ï¼Œç»è¿‡`softmax`å¤„ç†åŽæ‰€æœ‰è¾“å‡ºèŠ‚ç‚¹æ¦‚çŽ‡å’Œä¸º1ã€‚\n\n$$\no_1=\\frac{e^{y_1}}{e^{y_1}+e^{y_2}}\\\\\no_2=\\frac{e^{y_2}}{e^{y_1}+e^{y_2}}\n$$\n\n2. **ReLUæ¿€æ´»å‡½æ•°**\n\n* ç¼ºç‚¹åœ¨äºŽå½“åå‘ä¼ æ’­çš„è¿‡ç¨‹ä¸­æœ‰ä¸€ä¸ªéžå¸¸å¤§çš„æ¢¯åº¦ç»è¿‡æ—¶ï¼Œåå‘ä¼ æ’­æ›´æ–°åŽå¯èƒ½å¯¼è‡´æƒé‡åˆ†å¸ƒä¸­å¿ƒå°äºŽ0ï¼Œå¯¼è‡´è¯¥å¤„çš„å€’æ•°å§‹ç»ˆä¸º0ï¼Œåå‘ä¼ æ’­æ— æ³•æ›´æ–°æƒé‡ï¼Œå³è¿›å…¥å¤±æ´»çŠ¶æ€ã€‚\n* å¤±æ´»åŽæ— æ³•â€œå¤æ´»â€ã€‚å»ºè®®ä¸€å¼€å§‹ä¸ä½¿ç”¨è¾ƒå¤§å­¦ä¹ çŽ‡ï¼Œå¦åˆ™å¤§å¤šæ•°ç¥žç»å…ƒå®¹æ˜“å¤±æ´»ã€‚\n\n![image-20220327215434274](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220327215434274.png)\n\n## 5.3 è¶Šç•Œæƒ…å†µ\n\nåˆ©ç”¨**padding**è¡¥é›¶ï¼Œç»å·ç§¯åŽçš„çŸ©é˜µå°ºå¯¸å¤§å°è®¡ç®—å…¬å¼ä¸ºï¼š\n\n$$\nN=(W-F+2P)/S+1\n$$\n\nå…¶ä¸­ï¼Œè¾“å…¥å›¾ç‰‡å¤§å°ä¸º$WÃ—W$ï¼Œ`Filter`å¤§å°ä¸º$FÃ—F$ï¼Œæ­¥é•¿ä¸º$S$ï¼Œ`padding`çš„åƒç´ æ•°ä¸º$P$.\n\n# **6 æ± åŒ–å±‚**\n\nç›®çš„ï¼šå¯¹ç‰¹å¾å›¾è¿›è¡Œç¨€ç–å¤„ç†ï¼Œå‡å°‘æ•°æ®è¿ç®—é‡\n\n* æ²¡æœ‰è®­ç»ƒå‚æ•°\n* åªæ”¹å˜ç‰¹å¾çŸ©é˜µ`W`å’Œ`h`ï¼Œä¸æ”¹å˜`channel`\n* ä¸€èˆ¬`pool size`å’Œ`stride`ç›¸åŒ\n\n1. **MaxPooling ä¸‹é‡‡æ ·å±‚**\n\n![image-20220403114548879](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220403114548879.png)\n\n2. **AveragePooling ä¸‹é‡‡æ ·å±‚**\n\n![image-20220403114623086](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220403114623086.png)\n\n# **7 åå‘ä¼ æ’­**\n\n## 7.1 è¯¯å·®è®¡ç®—\n\n**Cross Entropy Loss äº¤å‰ç†µæŸå¤±**\n\n1. é’ˆå¯¹**å¤šåˆ†ç±»**é—®é¢˜ï¼ˆæœ€åŽä¸€å±‚ä¸º**softmax**è¾“å‡ºï¼Œæ‰€æœ‰è¾“å‡ºæ¦‚çŽ‡å’Œä¸º1ï¼‰\n\n$$\nH=-\\sum_io^*log(o_i)\n$$\n\n2. é’ˆå¯¹**äºŒåˆ†ç±»**é—®é¢˜ï¼ˆæœ€åŽä¸€å±‚ä¸º**sigmoid**è¾“å‡ºï¼Œè¾“å‡ºèŠ‚ç‚¹ä¹‹é—´ç›¸äº’ä¸ç›¸å¹²ï¼‰\n\n$$\nH=-\\frac{1}{N}\\sum_{i=1}^N[o_i^*log(o_i)+(1-o_i^*)log(1-[o_i)]\n$$\n\nå…¶ä¸­ï¼Œ$o_i^*$ä¸ºçœŸå®žæ ‡ç­¾å€¼ï¼Œ$o_i$ä¸ºé¢„æµ‹å€¼ï¼Œé»˜è®¤$log$=$ln$\n\n## 7.2 è¯¯å·®çš„åå‘ä¼ æ’­\n\n![image-20220328221513797](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220328221513797.png)\n\n![image-20220328221543152](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220328221543152.png)\n\n![image-20220328221622680](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220328221622680.png)\n\n## 7.3 æƒé‡çš„æ›´æ–°\n\n![image-20220328221732785](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220328221732785.png)\n\n![image-20220328221848547](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220328221848547.png)\n\n# **8 ä¼˜åŒ–å™¨ optimizer**\n\n![image-20220328222050506](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220328222050506.png)\n\n## 8.1 SGD\n\n![image-20220328222130228](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220328222130228.png)\n\n## 8.2 SGD+Momentumä¼˜åŒ–å™¨\n\n![image-20220328222335145](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220328222335145.png)\n\n## 8.3 Adagradä¼˜åŒ–å™¨ï¼ˆè‡ªé€‚åº”å­¦ä¹ çŽ‡ï¼‰\n\n![image-20220328222425119](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220328222425119.png)\n\n## 8.4 RMSPropä¼˜åŒ–å™¨\n\n![image-20220328222528324](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220328222528324.png)\n\n## 8.5 Adamä¼˜åŒ–å™¨ï¼ˆè‡ªé€‚åº”å­¦ä¹ çŽ‡ï¼‰\n\n![image-20220328222551419](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/image-20220328222551419.png)","tags":["CV"],"categories":["Computer Vision"]},{"title":"Real-Time Rendering & DLSS 2.0","url":"/docs/Other-Real-time-Rendering-and-DLSS-2.0/","content":"\næœ¬æ–‡æ˜¯åœ¨è®¡ç®—æœºå›¾å½¢å­¦æœŸæœ«è€ƒå¯Ÿçš„èƒŒæ™¯ä¸‹ï¼Œé€šè¿‡ä¸€å‘¨çš„è°ƒæŸ¥å¹¶æŒ‰ä¸ªäººç†è§£æ•´ç†å¾—å‡ºçš„ã€‚è™½è¯´æ–‡ç« ä¹Ÿæ˜¯æ€»ç»“çš„ç²¾åŽï¼Œä½†ä¸ªäººä»¥ä¸ºè¿˜é¢‡æœ‰ç²—ç³™ä¹‹å¤„ï¼ˆå¦‚æœ‰é”™è¯¯æ¬¢è¿ŽæŒ‡æ­£ï¼‰ã€‚åœ¨æ­¤ï¼Œç¬”è€…æŠŠå‚è€ƒèµ„æºæ”¾äºŽæ–‡ç« ä¹‹å‰â€”â€”ç›¸æ¯”äºŽæœ¬æ–‡ï¼Œå‚è€ƒèµ„æ–™åœ¨å­¦æœ¯ä¸Šæ›´å‡†ç¡®ï¼Œå†…å®¹æ·±åˆ»ï¼Œè¡¨è¾¾å¾—ä½“ï¼›å¦ä¸€æ–¹é¢ä¹Ÿæ˜¯å¸Œæœ›å¤§å®¶ä¼˜å…ˆä»Žå‚è€ƒèµ„æ–™ä¸‹æ‰‹ï¼Œä»Žä¸­å¾—å‡ºè‡ªå·±çš„æ€è€ƒï¼Œå†æ¥ç¬‘çœ‹è¿™ç¯‡å¤šæ–¹å€Ÿé‰´çš„æ€»ç»“ã€‚\n\n<!--more-->\n\n# **Reference**\n\n* å¦‚æžœæƒ³è¦å…¥é—¨æˆ–è€…å¿«é€Ÿäº†è§£ä¸€ä¸‹å¯ä»¥å…ˆçœ‹Bç«™çš„ç§‘æ™®è§†é¢‘ï¼Œå¯ä»¥å¤§è‡´äº†è§£DLSSæ˜¯ä»€ä¹ˆï¼Œä¸ºä»€ä¹ˆï¼Œæ€Žä¹ˆåšã€‚çœ‹å®Œè§†é¢‘æ–¹ä¾¿ä½ è¿›ä¸€æ­¥æå‡ºé—®é¢˜ï¼ŒæŸ¥æ‰¾èµ„æ–™ã€‚\n\t*  [Bç«™ï¼šDLSSåˆ°åº•æ˜¯ä»€ä¹ˆæŠ€æœ¯ï¼Ÿä¸ºä½•èƒ½æå‡æ¸¸æˆæ€§èƒ½ï¼Ÿæœ‰ä»£ä»·å—ï¼Ÿ](https://www.bilibili.com/video/BV1Dy4y117BM?spm_id_from=333.999.0.0),\n\t* [Bç«™ï¼šã€ç¡¬ä»¶ç§‘æ™®ã€‘å…è´¹æå‡ç”»è´¨å’Œå¸§æ•°ï¼Ÿè¯¦è§£DLSS2.0çš„å·¥ä½œåŽŸç†ä¸Žä½œç”¨](https://www.bilibili.com/video/BV1PA41187g2?spm_id_from=333.999.0.0)\n* NVIDIAå®˜ç½‘ä»‹ç»ï¼š[NVIDIAï¼šDLSS 2.0](https://www.nvidia.com/en-us/geforce/news/nvidia-dlss-2-0-a-big-leap-in-ai-rendering/)\næ­¤å¤–ï¼ŒNVIDAè¿˜åœ¨GTCä¸Šç»™å‡ºäº†ç›¸åº”çš„Talkï¼Œå…¶ä¸­ä»‹ç»äº†DLSS 2.0ã€é’ˆå¯¹äºŽæ¸¸æˆçš„å›¾åƒè¶…åˆ†è¾¨çŽ‡çš„æŒ‘æˆ˜ä»¥åŠDLSS 2.0å¼•æ“Žé›†æˆï¼šNVIDA's Talkï¼š[GTC 2020: DLSS 2.0 - Image Reconstruction for Real-time Rendering with Deep Learning](https://www.youtube.com/watch?v=d5knHzv0IQE)ï¼ˆè¿™æ˜¯æ²¹ç®¡ä¸Šé¢çš„ï¼Œè‹±è¯­å¬åŠ›okçš„å¯ä»¥ç›´æŽ¥å†²ï¼Œè‹±è¯­ä¸å¤ªè¡Œçš„å¯ä»¥å¼€ä¸­æ–‡å­—æ¯ã€‚å¥½åƒbç«™ä¹Ÿä¸Šçº¿äº†ï¼Œä½†è¿˜æ²¡ç‚¹è¿›åŽ»è¿‡ï¼Œä¸çŸ¥é“æœ‰æ²¡æœ‰ç¿»è¯‘ï¼‰æœ¬æ–‡æœ‰å…³äºŽDLSS 2.0çš„ä»‹ç»å¤§æŠµä¸Šä¹Ÿå‡ºè‡ªè¿™ä¸ªTalk.\n* åŒæ—¶ï¼ŒDLSSå›¢é˜Ÿæˆå‘˜[æ–‡åˆ€ç§‹äºŒ](https://www.zhihu.com/people/edliu/posts)ä¹Ÿæ˜¯è¿™ä¸ªTalkçš„æ±‡æŠ¥äººåœ¨çŸ¥ä¹Žä¸Šä¹Ÿå¯¹Talkè¿›è¡Œäº†æ€»ç»“ï¼Œè¯¦è§ï¼š[DLSS 2.0 - åŸºäºŽæ·±åº¦å­¦ä¹ çš„å®žæ—¶æ¸²æŸ“å›¾åƒé‡å»º](https://zhuanlan.zhihu.com/p/123642175)\n* [Beyond3D: Diving into Anti-Aliasing](https://www.beyond3d.com/content/news/798)ä¸ªäººè§‰å¾—æ˜¯å¾ˆå…¨é¢å¾ˆæœ‰é€»è¾‘çš„æŠ—é”¯é½¿çš„ä»‹ç»ã€‚æœ‰éœ€è¦çš„æœ‹å‹å¯ä»¥è‡ªå–ï¼\n* ä¹¦ç±ï¼š[Real time rendering](https://www.taylorfrancis.com/books/mono/10.1201/9781315365459/real-time-rendering-tomas-akenine-mo%CC%88ller-eric-haines-naty-hoffman) åœ¨å®žæ—¶æ¸²æŸ“å’Œè®¡ç®—æœºå›¾å½¢å­¦é¢†åŸŸï¼Œã€ŠReal-Time Renderingã€‹è¿™æœ¬ä¹¦ä¸€ç›´å¤‡å—æŽ¨å´‡ã€‚æœ‰äººè¯´ï¼Œå®ƒå®žæ—¶æ¸²æŸ“çš„åœ£ç»ã€‚ä¹Ÿæœ‰äººè¯´ï¼Œå®ƒæ˜¯ç»ä¸–æ­¦åŠŸçš„ç›®å½•ã€‚è¿™æ¬¡è°ƒæŸ¥ä¸»è¦é˜…è¯»äº†æœ¬ä¹¦å…³äºŽæŠ—é”¯é½¿æ–¹é¢çš„ä»‹ç»ã€‚å¦‚æœ‰æœºä¼šå¯ä»¥è¿›å†›ç›¸å…³é¢†åŸŸï¼Œè¿˜æ˜¯å¾ˆæœŸå¾…å¯ä»¥æŠŠè¿™æœ¬ä¹¦è¯»ä¸€ä¸‹çš„ã€‚å½“ç„¶æ¯›æ˜Ÿäº‘ä¹Ÿåœ¨CSDNä¸Šå‘å¸ƒäº†è¿™æœ¬ä¹¦ç¬¬ä¸‰ç‰ˆæç‚¼æ€»ç»“çš„ä¸“æ ï¼š[ã€ã€ŠReal-Time Rendering 3rdã€‹æç‚¼æ€»ç»“ã€‘](https://blog.csdn.net/poem_qianmo/category_9269285.html?spm=1001.2014.3001.5482)ï¼Œä¸¤è€…å¯ä»¥é…åˆé£Ÿç”¨ã€‚\n* ä¸€ç¯‡2021å¹´7æœˆçš„æœŸåˆŠï¼š[An overview of current deep learned rendering technologies](https://www.webofscience.com/wos/alldb/full-record/INSPEC:20799965) ç€é‡è®¨è®ºå®žæ—¶æ¸²æŸ“å’Œæ·±åº¦å­¦ä¹ æ¸²æŸ“ã€‚å…¶ä¸­ä»‹ç»äº†å®žæ—¶æ¸²æŸ“æŠ€æœ¯ä¸­çš„æŠ—é”¯é½¿å’Œè¶…åˆ†è¾¨çŽ‡ï¼Œæ·±åº¦å­¦ä¹ æ¸²æŸ“æŠ€æœ¯ä¸­çš„DLSSå’ŒNSSæ¨¡åž‹ï¼Œå¹¶ä¸”ä»‹ç»äº†DLSRæŠ€æœ¯é¢ä¸´çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡çš„æ€è·¯ä¹Ÿä»Žè¿™ç¯‡æœŸåˆŠè€Œæ¥ã€‚\n\n# **1 å‰è¨€**\n\né€šä¿—è®²ï¼Œ**æ¸²æŸ“**ï¼ˆRender)æ˜¯å¤„ç†å™¨å°†éœ€è¦è®¡ç®—çš„ç”»é¢ä¿¡æ¯ï¼Œè®¡ç®—å¹¶â€œç»˜åˆ¶â€åœ¨æ˜¾ç¤ºå±å¹•ä¸Šçš„è¿‡ç¨‹ã€‚éšç€æ˜¾ç¤ºæŠ€æœ¯çš„è¿›æ­¥ï¼Œæ¸²æŸ“æŠ€æœ¯ä¹Ÿæ…¢æ…¢åœ°å‡ºçŽ°äº†ä¸¤æ¡ä¸»æµåˆ†æ”¯ï¼šä¸€ç§ç”¨äºŽè§†é¢‘æ¸¸æˆæŠ€æœ¯ï¼Œå¦ä¸€ç§åˆ™æ˜¯ç”¨äºŽå½±è§†æŠ€æœ¯ã€‚è¿™ä¸¤ç±»éœ€æ±‚å¯¹åº”çš„æ¸²æŸ“æŠ€æœ¯åˆ†åˆ«ä¸ºï¼š**å®žæ—¶æ¸²æŸ“ä¸Žç¦»çº¿æ¸²æŸ“**ã€‚\n\n**å®žæ—¶æ¸²æŸ“**ï¼ˆReal-time renderingï¼‰æŒ‡çš„æ˜¯åœ¨è®¡ç®—æœºä¸Šå¿«é€Ÿç”Ÿæˆå›¾åƒã€‚å®ƒæ˜¯è®¡ç®—æœºå›¾å½¢å­¦ä¸­æœ€å…·äº¤äº’æ€§çš„é¢†åŸŸã€‚é¦–å…ˆä¸€å¹…å›¾åƒæ˜¾ç¤ºåœ¨å±å¹•ä¸Šï¼Œç„¶åŽè§‚å¯Ÿè€…åšå‡ºåŠ¨ä½œä¸Žååº”ï¼Œå¹¶ä¸”å…¶åŠ¨ä½œåé¦ˆä¼šå½±å“æŽ¥ä¸‹æ¥çš„ç”Ÿæˆå†…å®¹ã€‚ç”±äºŽè¿™ç§åé¦ˆã€æ¸²æŸ“çš„å¾ªçŽ¯é€Ÿåº¦è¶³å¤Ÿå¿«ï¼Œè§‚å¯Ÿè€…å°±ä¸ä¼šåªçœ‹åˆ°ç‹¬ç«‹çš„å›¾åƒï¼Œè€Œæ˜¯ä¼šæ²‰æµ¸åœ¨è¿™ç§åŠ¨æ€è¿‡ç¨‹ä¸­ã€‚\n\nç”±äºŽè¿½æ±‚é«˜åˆ†è¾¨çŽ‡å’Œé«˜å¸§çŽ‡çš„çœŸå®žæ€§ä½“éªŒï¼ŒRTRæŠ€æœ¯éš¾åº¦å‘ˆæŒ‡æ•°åž‹ä¸Šå‡ã€‚æ˜¾ç¤ºè®¾å¤‡çš„æ›´æ–°æ¢ä»£ä»¥åŠç‰©ç†ç€è‰²ã€å…‰çº¿è¿½è¸ªã€ç²¾ç¡®ç‰©ç†å¼•æ“Žã€æ›´é«˜è´¨é‡çš„çº¹ç†æ¨¡åž‹çš„å®žçŽ°ä½¿æœ€æ–°ä¸€ä»£ GPU ä¹Ÿéš¾ä»¥åœ¨ä¸å½±å“å¸§çŽ‡çš„æƒ…å†µä¸‹ä»¥åŽŸå§‹åˆ†è¾¨çŽ‡æ¸²æŸ“å›¾åƒã€‚æ­¤æ—¶ï¼Œ**ä½Žåˆ†è¾¨çŽ‡çš„æ€§èƒ½å¼€é”€å®žçŽ°é«˜åˆ†è¾¨çŽ‡çš„ç”»é¢**æˆä¸ºå¤§åŠ¿æ‰€è¶‹ã€‚\n\n<center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/f5622c6950d64a19b46d2bd8a9a10748.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_11,color_FFFFFF,t_70,g_se,x_16#pic_center\"><br>\n    <div style=\"color: #999;\">å›¾1 RTX</div>\n</center>\n\nå€ŸåŠ©äºŽæ·±åº¦å­¦ä¹ è¶…é‡‡æ ·ï¼ŒNVIDIAæŽ¨å‡ºDLSS 2.0ã€‚å…¶å®žçŽ°äº†é€šè¿‡æ¸²æŸ“æ›´å°‘çš„åƒç´ ï¼Œä½¿ç”¨ AI æž„å»ºæ¸…æ™°ã€åˆ†è¾¨çŽ‡æ›´é«˜çš„å›¾åƒã€‚ DLSS2.0ç”±GeForce RTX GPUä¸Šçš„ä¸“ç”¨ AI å¤„ç†å™¨Tensor Cores æä¾›æ”¯æŒï¼Œæ˜¯ä¸€ç§ç»è¿‡æ”¹è¿›çš„å…¨æ–°æ·±åº¦å­¦ä¹ ç¥žç»ç½‘ç»œï¼Œå¯åœ¨ç”Ÿæˆç²¾ç¾Žã€æ¸…æ™°çš„æ¸¸æˆå›¾åƒçš„åŒæ—¶æé«˜å¸§é€Ÿã€‚å®ƒä¸ºæ¸¸æˆçŽ©å®¶æä¾›äº†æœ€å¤§åŒ–å…‰çº¿è¿½è¸ªè®¾ç½®å’Œæé«˜è¾“å‡ºåˆ†è¾¨çŽ‡çš„æ€§èƒ½ç©ºé—´ã€‚ \n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/809cd174ff5a4637bbe8dfa58f3c55fa.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_20,color_FFFFFF,t_70,g_se,x_16\"> <br>\n    <div style=\"color: #999\">å›¾2 NVIDIAæŽ¨å‡ºDLSSæŠ€æœ¯</div>\n</center>\n\næœ¬æ–‡æ—¨åœ¨ä»‹ç»å®žæ—¶æ¸²æŸ“å›¾åƒé‡å»ºæŠ€æœ¯éƒ¨åˆ†åŸºç¡€ï¼ŒåŒæ—¶ç€é‡äºŽè®¨è®ºæ–°å…´çš„åŸºäºŽæ·±åº¦å­¦ä¹ çš„å®žæ—¶æ¸²æŸ“é‡å»ºDLSS 2.0ã€‚æ–‡ç« ç»“æž„å¦‚ä¸‹ï¼š**ç¬¬ä¸€éƒ¨åˆ†è®¨è®ºäº†å®žæ—¶æ¸²æŸ“é‡å»ºæŠ€æœ¯åŸºç¡€ï¼ŒåŒ…æ‹¬æŠ—é”¯é½¿å’Œè¶…åˆ†è¾¨çŽ‡é‡‡æ ·ä¸¤ä¸ªæ–¹å‘ï¼›ç¬¬äºŒéƒ¨åˆ†ç€é‡è®¨è®ºDLSS 2.0æŠ€æœ¯çš„ç†è®ºã€å·¥ä½œåŽŸç†å’Œå®žçŽ°æ•ˆæžœï¼›ç¬¬ä¸‰éƒ¨åˆ†åˆ†æžDLSS 2.0çš„ä¼˜ç‚¹å’Œç¼ºç‚¹ï¼Œå¹¶è®¨è®ºäº†DLSRæŠ€æœ¯é¢ä¸´çš„æŒ‘æˆ˜å’Œå±•æœ›ã€‚**\n\n# **2 å®žæ—¶æ¸²æŸ“å›¾åƒé‡å»ºæŠ€æœ¯åŸºç¡€**\n\n## 2.1 æŠ—é”¯é½¿æŠ€æœ¯\n\n**Aliasing**ï¼ˆé”¯é½¿ï¼‰è¿™ä¸ªæœ¯è¯­æœ€æ—©å‡ºçŽ°åœ¨ä¿¡å·å¤„ç†è¿™é—¨å­¦ç§‘ä¸­ï¼ŒæŒ‡çš„æ˜¯å½“ä¸€ä¸ªè¿žç»­ä¿¡å·è¢«é‡‡æ ·åŽå’Œå…¶ä»–éžä¸€è‡´ä¿¡å·æ··æ·†çš„çŽ°è±¡ã€‚åœ¨3Dæ¸²æŸ“ä¸­ï¼Œè¿™ä¸ªæœ¯è¯­æœ‰ç€æ›´ç‰¹æ®Šçš„æ„æ€â€”â€”å®ƒæ¶µç›–äº†æ‰€æœ‰3Dæ¸²æŸ“å…‰æ …åŒ–åŽäº§ç”Ÿçš„ç”»é¢ç‘•ç–µã€‚3Dåœºæ™¯æ¸²æŸ“åœ¨å…‰æ …åŒ–ä¹‹å‰æ˜¯è¿žç»­ä¿¡å·ï¼Œä½†åœ¨è¿›è¡Œåƒç´ æ¸²æŸ“ï¼ˆå¯¹æ¯ä¸ªåƒç´ ç”Ÿæˆç›¸åº”çš„è‰²å½©å€¼ï¼‰çš„æ—¶å€™å°±ä¸å¾—ä¸å¯¹è¿žç»­ä¿¡å·è¿›è¡Œé‡‡æ ·ä»¥èŽ·å¾—èƒ½å¤Ÿè¾“å‡ºåˆ°æ˜¾ç¤ºå™¨çš„ç»“æžœã€‚åé”¯é½¿çš„ç›®æ ‡å°±åœ¨äºŽè®©æœ€ç»ˆè¾“å‡ºçš„ç”»é¢å’ŒåŽŸç”Ÿåœºæ™¯å°½å¯èƒ½æŽ¥è¿‘ï¼Œä¿®å¤æ¸²æŸ“ç‘•ç–µã€‚\n\n**æ‰€æœ‰çš„æ¸²æŸ“å¤±çœŸéƒ½å¯ä»¥å½’å› äºŽé‡‡æ ·é—®é¢˜**ï¼ˆç”¨æœ‰é™çš„åƒç´ å±•ç¤ºæ— é™çš„ç»†èŠ‚ï¼‰ï¼Œä½¿ç”¨å“ªç§åé”¯é½¿æ‰‹æ®µä¸Žé”¯é½¿æˆå› æ¯æ¯ç›¸å…³ã€‚å› æ­¤ï¼Œä¸ºäº†æŽ¢è®¨ä¸åŒåé”¯é½¿æ‰‹æ®µçš„ä¼˜åŠ¿å’ŒåŠ£åŠ¿ï¼Œæˆ‘ä»¬å…ˆå°†3Dæ¸²æŸ“çš„ç‘•ç–µæ ¹æ®å…¶æˆå› ç®€å•å½’çº³ä¸º6ä¸ªç±»åˆ«ï¼š**å‡ ä½•å¤±çœŸã€é€æ˜Žå¤±çœŸã€å­åƒç´ å¤±çœŸã€çº¹ç†å¤±çœŸã€æ¸²æŸ“å¤±çœŸã€é—ªçƒæƒ…å½¢ï¼ˆæ—¶é—´æ€§é”¯é½¿ï¼‰**ã€‚\n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/6501b6475d9644f79dc48c0d99094aec.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_12,color_FFFFFF,t_70,g_se,x_16\"><br>\n    <div style=\"color: #999;\">å›¾ 3 å„ç§ç±»åž‹çš„å¤±çœŸã€‚åˆ†åˆ«ä¸ºï¼š<br>é€æ˜Žå¤±çœŸã€å‡ ä½•å¤±çœŸï¼ˆ2Dï¼‰ã€å­åƒç´ å¤±çœŸã€<br>å‡ ä½•å¤±çœŸï¼ˆ3Dï¼‰ã€çº¹ç†å¤±çœŸã€æ¸²æŸ“å¤±çœŸ</div>\n</center>\n\nçŽ°å¦‚ä»Šçš„æŠ—é”¯é½¿æŠ€æœ¯å¯ä»¥åˆ†ä¸ºä¸¤ç±»ï¼š**ä¸€ç±»æ˜¯é€šè¿‡æé«˜é‡‡æ ·è´¨é‡æ¥å‡å°‘æ¸²æŸ“æ—¶é”¯é½¿ï¼Œå¦ä¸€ç±»åˆ™æ˜¯é€šè¿‡å¯¹å·²æ¸²æŸ“å¥½çš„å›¾ç‰‡åˆ†æžå’ŒåŽå¤„ç†æ¥å‡å°‘é”¯é½¿ã€‚**\n\n### 2.1.1 åŸºäºŽé‡‡æ ·çš„æŠ—é”¯é½¿æŠ€æœ¯\n\né¦–å…ˆè®¨è®ºåŸºäºŽé‡‡æ ·çš„åé”¯é½¿æŠ€æœ¯ï¼Œå…¶å®žè´¨åˆ™æ˜¯é€šè¿‡æ¸²æŸ“æ¯”å±å¹•åˆ†è¾¨çŽ‡æ›´é«˜çš„ç”»é¢è€ŒåŽå†é™é‡‡æ ·è‡³å±å¹•ç©ºé—´åˆ†è¾¨çŽ‡ã€‚æ ·æœ¬æ•°é‡ï¼Œæ ·æœ¬ä½ç½®ã€é‡‡æ ·æ¨¡å¼å’Œæ ·æœ¬èžåˆæ–¹å¼éƒ½ä¼šå½±å“æœ€ç»ˆçš„ç”»é¢è´¨é‡ã€‚\n\n#### æ ·æœ¬æ•°é‡\n\næ˜¾è€Œæ˜“è§ï¼Œå€˜è‹¥ç”Ÿæˆä¸€ä¸ªåƒç´ çš„é‡‡æ ·ç‚¹è¶‹è¿‘äºŽæ— ç©·å¤šï¼Œé‚£ä¹ˆæœ€ç»ˆçš„æ•ˆæžœå°±ä¼šæ— é™è¶‹è¿‘â€œå®Œç¾Žâ€çš„å…‰æ …åŒ–æ•ˆæžœã€‚å› æ­¤ï¼ŒæŠ—é”¯é½¿çš„æ•ˆæžœå’Œæ ·æœ¬æ•°é‡å¯†åˆ‡ç›¸å…³ã€‚å½“ç„¶æ ·æœ¬æ•°é‡ä¹Ÿå…³ç³»åˆ°è®¾å¤‡æ€§èƒ½ï¼šé€šå¸¸åœ¨æ¸¸æˆä¸­æ¯ä¸ªåƒç´ ä¼šä½¿ç”¨2ä¸ªæˆ–4ä¸ªé‡‡æ ·ç‚¹ï¼Œè€Œåœ¨é«˜ç«¯æ˜¾ç¤ºå™¨ä¸­å¯èƒ½ä¼šä½¿ç”¨åˆ°8ä¸ªåŠä»¥ä¸Šçš„é‡‡æ ·ç‚¹ã€‚\n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/3bbef153efc74eeebc26a7bf68b36446.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_20,color_FFFFFF,t_70,g_se,x_16\"><br>\n    <div style=\"color: #999;\">å›¾ 4 ä¸‰è§’å½¢å…‰æ …åŒ–ï¼Œæ¯ä¸ªåƒç´ æœ‰4ä¸ªæœ‰åºæ ·æœ¬</div>\n</center>\n\n#### æ ·æœ¬ä½ç½®\n\n1. **é¡ºåºæ …æ ¼è¶…çº§é‡‡æ · (Ordered Grid Super-Samplingï¼ŒOGSS)**\n\næ ·æœ¬ä½ç½®çš„é€‰å–å¯¹æœ€ç»ˆç”»é¢è´¨é‡æœ‰ç€è‡³å…³é‡è¦çš„å½±å“ã€‚ç‰¹åˆ«æ˜¯åœ¨æ¸¸æˆæ¸²æŸ“ä¸­ï¼Œç”±äºŽé‡‡æ ·ç‚¹æ•°é‡å°‘ï¼Œæ ·æœ¬ä½ç½®å°±æ›´ä¸ºé‡è¦ã€‚ç”±äºŽæ ·æœ¬ä½ç½®å‘ˆæœ‰åºç‚¹æŽ’åˆ—ï¼Œè¿™ç§æŠ—é”¯é½¿ä¹Ÿè¢«ç§°ä½œé¡ºåºæ …æ ¼è¶…çº§é‡‡æ ·ã€‚\n\n2. **æ—‹è½¬æ …æ ¼è¶…çº§é‡‡æ · (Rotated Grid Super-Samplingï¼ŒRGSS)**\n\nç„¶è€Œï¼Œå¯¹äºŽæŽ¥è¿‘åž‚ç›´æˆ–æŽ¥è¿‘æ°´å¹³çš„çº¿è€Œè¨€ï¼Œä½¿ç”¨æŽ’åˆ—æœ‰åºçš„é‡‡æ ·ç½‘æ ¼æ•ˆæžœå¾€å¾€ä¸ä½³ã€‚æ­¤æ—¶ï¼Œé‡‡ç”¨æ—‹è½¬æ …æ ¼è¶…çº§é‡‡æ ·å¯ä»¥èŽ·å¾—æ›´å¥½çš„ç»“æžœã€‚\n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/349d5e3fd91d4f06bbb93e08706fdb10.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_15,color_FFFFFF,t_70,g_se,x_16\"><br>\n    <div style=\"color: #999;\">å›¾ 5 è¿‘åž‚ç›´çº¿åœºæ™¯ã€å®Œç¾ŽæŠ—é”¯é½¿å…‰æ …åŒ–ã€4ä¸ªæœ‰åºæ ·æœ¬çš„å…‰æ …åŒ–ã€4ä¸ªç¨€ç–æ …æ ¼æ ·æœ¬çš„æŠ—é”¯é½¿</div>\n</center> \n\nä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å°†é‡‡æ ·ç‚¹ç¨€ç–æ‘†æ”¾åœ¨ä¸åŒçš„åˆ—ã€‚å¯¹äºŽæŠ—é”¯é½¿æ¥è¯´ï¼Œç†æƒ³çš„æ‘†æ”¾åº”å½“æ˜¯ç¨€ç–çš„ã€‚æ¢å¥è¯è¯´ï¼Œå¯¹äºŽNä¸ªé‡‡æ ·ç‚¹ï¼Œä»»æ„ä¸¤ä¸ªé‡‡æ ·ç‚¹ä¸ä¼šåœ¨ä¸€ä¸ª$N\\times N$ç½‘æ ¼çš„åŒä¸€åˆ—ã€è¡Œä»¥åŠå¯¹è§’çº¿ä¸Šã€‚é€šè¿‡å¯¹Nçš‡åŽé—®é¢˜æ±‚è§£å¯å¾—åˆ°æ»¡è¶³è¿™ç§æ¡ä»¶çš„é‡‡æ ·ç‚¹æ‘†æ”¾æ–¹å¼ï¼Œåœ¨æ­¤ä¸å†èµ˜è¿°ã€‚è¿™ç§ç¨€ç–æ‘†æ”¾é‡‡æ ·ç‚¹çš„æŠ—é”¯é½¿ä¹Ÿè¢«ç§°ä½œ**ç¨€ç–æ …æ ¼æŠ—é”¯é½¿ï¼ˆSparse Grid Anti-aliasingï¼ŒSGAAï¼‰**ã€‚\n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/2d6763203ca54651992b293a2164e1ed.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_18,color_FFFFFF,t_70,g_se,x_16\"><br>\n    <div style=\"color: #999;\">å›¾ 6  å·¦å›¾ï¼šæœ‰åºé‡‡æ ·ç‚¹ å³å›¾ï¼šNçš‡åŽé‡‡æ ·</div>\n</center> \n\n#### é‡‡æ ·ç±»åž‹\n\n1. **è¶…é‡‡æ ·æŠ—é”¯é½¿ï¼ˆSuper-sampling Anti-aliasingï¼ŒSSAAï¼‰**\n\n   åŸºäºŽé‡‡æ ·çš„æŠ—é”¯é½¿æ–¹æ³•å¯¹æ¯ä¸ªé‡‡æ ·ç‚¹éƒ½è¿›è¡Œäº†è¿ç®—ã€‚è™½ç„¶è¿™æ ·çš„é‡‡æ ·æ–¹å¼å¯ä»¥æ¶ˆé™¤å„ç±»æ¸²æŸ“å¤±çœŸï¼Œä½†ä¹Ÿéžå¸¸è€—è´¹èµ„æºã€‚ä¸¾ä¸ªä¾‹å­ï¼ŒNå€é‡‡æ ·å°†ä¼šç»™åƒç´ æ¸²æŸ“ã€å…‰æ …å•å…ƒã€å†…å­˜å¸¦å®½ä»¥åŠå†…å­˜å®¹é‡æ–½åŠ Nå€çš„è®¡ç®—åŽ‹åŠ›ã€‚è¿™ç§å¯¹æ¯ä¸ªé‡‡æ ·ç‚¹éƒ½è¿›è¡Œç‹¬ç«‹è®¡ç®—çš„é‡‡æ ·ä¹Ÿè¢«ç§°ä¸ºè¶…é‡‡æ ·æŠ—é”¯é½¿ã€‚\n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/778a6c9f6e7947c9afff4a06b76f34b4.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_12,color_FFFFFF,t_70,g_se,x_16\"><br>\n    <div style=\"color: #999;\">å›¾ 7 2 x SGSSAA</div>\n</center> \n\n\n2. **å¤šé‡é‡‡æ ·æŠ—é”¯é½¿ï¼ˆmulti-sample anti-aliasingï¼ŒMSAAï¼‰**\n\n   åœ¨è¿›å…¥21ä¸–çºªåŽï¼Œå¤šé‡é‡‡æ ·æŠ—é”¯é½¿å¼€å§‹ä½œä¸ºSSAAçš„ä¸€ç§ä¼˜åŒ–è§£è¢«å¹¿æ³›åº”ç”¨ã€‚MSAAå®žè´¨æ˜¯åªå¯¹ Z ç¼“å­˜ï¼ˆZ-Bufferï¼‰å’Œæ¨¡æ¿ç¼“å­˜ ï¼ˆStencil Bufferï¼‰ä¸­çš„æ•°æ®è¿›è¡Œè¶…çº§é‡‡æ ·æŠ—é”¯é½¿çš„å¤„ç†ã€‚å¯ä»¥ç†è§£ä¸ºåªå¯¹è¾¹ç¼˜è¿›è¡ŒæŠ—é”¯é½¿å¤„ç†ã€‚å½“ç¡¬ä»¶æ”¯æŒZç¼“å­˜å’Œæ¨¡æ¿ç¼“å­˜æ—¶ï¼ˆè€ŒçŽ°ä»Šå¤§éƒ¨åˆ†GPUéƒ½å·²ç»æ”¯æŒè¿™äº›ç‰¹æ€§ï¼‰ï¼ŒMSAAå¸¦æ¥çš„å†…å­˜å¸¦å®½å¼€é”€ä¼šè¿›ä¸€æ­¥ç¼©å°ã€‚ç›¸æ¯”SSAAå¯¹ç”»é¢ä¸­æ‰€æœ‰æ•°æ®è¿›è¡Œå¤„ç†ï¼ŒMSAAå¤§å¤§å‡å¼±å¯¹èµ„æºçš„æ¶ˆè€—ã€‚ä½†ç”±äºŽMSAAä»…é’ˆå¯¹å‡ ä½•ä½“è¾¹ç¼˜è¿›è¡ŒæŠ—é”¯é½¿ï¼Œå…¶ä»–ç±»åˆ«çš„å¤±çœŸï¼ˆé€æ˜Žå¤±çœŸã€çº¹ç†å¤±çœŸå’Œæ¸²æŸ“å¤±çœŸç­‰ï¼‰éƒ½æ— æ³•è¢«æ¶ˆé™¤ã€‚\n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/81d2caa0a8c64832b54dd7ce81f1c227.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_19,color_FFFFFF,t_70,g_se,x_16\">\n    <br><div style=\"color: #999;\">å›¾ 8 MSAAä¿¡æ¯å­˜å‚¨å’Œç›¸åº”çš„EQAA</div>\n</center> \n\n\n<center>\n   <img style=\"border-radius: 0.3125em;\n   box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n   src=\"https://img-blog.csdnimg.cn/b7046a4f52b44e188e241173ba2c5966.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_12,color_FFFFFF,t_70,g_se,x_16\">\n   <br><div style=\"color: #999;\">\n   å›¾ 9 2xMSAA</div>\n</center> \n\n3. **è¦†ç›–é‡‡æ ·æŠ—é”¯é½¿ï¼ˆcoverage sampling anti-aliasingï¼ŒCSAAï¼‰**\n\n   ç¬¬ä¸‰ç§é‡‡æ ·ç±»åž‹åˆ™æ˜¯NVIDIAåœ¨2006å¹´å¼•å…¥çš„è¦†ç›–é‡‡æ ·æŠ—é”¯é½¿ã€‚CSAAåœ¨MSAAçš„åŸºç¡€ä¸Šè¿˜å¢žåŠ äº†è¦†ç›–é‡‡æ ·(Coverage Sample)ã€‚ç®€å•è¯´ CSAA å°±æ˜¯å°†è¾¹ç¼˜å¤šè¾¹å½¢é‡Œéœ€è¦å–æ ·çš„å­åƒç´ åæ ‡è¦†ç›–æŽ‰ï¼ŒæŠŠåŽŸåƒç´ åæ ‡å¼ºåˆ¶å®‰ç½®åœ¨ç¡¬ä»¶å’Œé©±åŠ¨ç¨‹åºé¢„å…ˆç®—å¥½çš„åæ ‡ä¸­ã€‚è¿™å°±å¥½æ¯”å–æ ·æ ‡å‡†ç»Ÿä¸€çš„MSAAï¼Œèƒ½å¤Ÿæœ€é«˜æ•ˆçŽ‡çš„æ‰§è¡Œè¾¹ç¼˜å–æ ·ï¼Œæ•ˆèƒ½æå‡éžå¸¸çš„æ˜¾è‘—ã€‚æ¯”æ–¹è¯´16xCSAAå–æ ·æ€§èƒ½ä¸‹é™å¹…åº¦ä»…æ¯”4xMSAAç•¥é«˜ä¸€ç‚¹ï¼Œå¤„ç†æ•ˆæžœå´å‡ ä¹Žå’Œ 8xMSAAä¸€æ ·ã€‚8xCSAAæœ‰ç€4xMSAAçš„å¤„ç†æ•ˆæžœï¼Œæ€§èƒ½æ¶ˆè€—å´å’Œ2xMSAAç›¸åŒã€‚\n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/81052231830a4a5f9159826f0fb92178.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_12,color_FFFFFF,t_70,g_se,x_16\">\n    <br><div style=\"color: #999;\">\n    å›¾ 10  8x MSAA + alpha-to-coverage</div>\n</center> \n\n#### æ ·æœ¬èžåˆæ–¹å¼\n\nå½±å“é‡‡æ ·æŠ—é”¯é½¿è´¨é‡çš„æœ€åŽä¸€ä¸ªè¦ç´ å°±æ˜¯æ ·æœ¬èžåˆæ¨¡å¼ï¼Œå³å¦‚ä½•å°†é‡‡æ ·ç‚¹åŠ æƒè®¡ç®—å‡ºä¸€ä¸ªåƒç´ å€¼ã€‚\n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/34d90a7c43744fd5aae46040bd69f5c1.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_20,color_FFFFFF,t_70,g_se,x_16\">\n    <br><div style=\"color: #999;\">\n    å›¾ 11 å·¦å›¾ï¼šç›’å¼è¿‡æ»¤å™¨ï¼Œä¸­é—´ï¼šé«˜åˆ†è¾¨çŽ‡æŠ—é”¯é½¿æ–¹æ³• å³å›¾ï¼šTent filteræŠ—é”¯é½¿. æœ€å¸¸è§çš„æ··åˆæ–¹æ³•æ˜¯ä»¥ç›¸åŒæƒé‡æ··åˆæ¯ä¸ªé‡‡æ ·ç‚¹ï¼Œè¿™è¢«ç§°ä½œBox Filterï¼Œä¹Ÿæ˜¯æ‰€æœ‰ä¼ ç»ŸMSAAæ‰€é‡‡ç”¨çš„æ¨¡å¼ã€‚</div>\n</center> \n\n1. **é«˜åˆ†è¾¨çŽ‡æŠ—é”¯é½¿æ–¹æ³• (High Resolution Anti-Aliasingï¼ŒHRAA)**\n\nä¸€ç§æ”¹è¿›çš„èžåˆæ–¹æ³•è¢«ç§°ä½œé«˜åˆ†è¾¨çŽ‡æŠ—é”¯é½¿æ–¹æ³•ä¹Ÿç§° Quincunx æ–¹æ³•ï¼Œå‡ºè‡ª NVIDIA å…¬å¸ã€‚QuincunxæŒ‡çš„æ˜¯5ä¸ªç‰©ä½“çš„æŽ’åˆ—æ–¹å¼ï¼šå…¶ä¸­ 4 ä¸ªåœ¨æ­£æ–¹å½¢è§’ä¸Šï¼Œç¬¬äº”ä¸ªåœ¨æ­£æ–¹å½¢ä¸­å¿ƒï¼Œä¹Ÿå°±æ˜¯æ¢…èŠ±å½¢ï¼Œå¾ˆåƒå…­è¾¹æ¨¡åž‹ä¸Šçš„äº”ç‚¹å›¾æ¡ˆæ¨¡å¼ã€‚è¿™ä½¿å¾—é‡‡æ ·ç‚¹ä¸ªæ•°ä¸å¤šçš„æƒ…å†µä¸‹çš„æŠ—é”¯é½¿æ•ˆæžœæ˜Žæ˜¾å¢žå¼ºï¼Œä½†ç”±äºŽä¸€ä¸ªåƒç´ ç‚¹è¿‡å¤šåœ°æ··åˆäº†å‘¨è¾¹çš„é‡‡æ ·ç‚¹ä¿¡æ¯è€Œä½¿å›¾åƒè¾¹ç¼˜å˜å¾—æ¨¡ç³Šï¼Œå¸¦æ¥äº†ç”»é¢é”åº¦é™ä½Žçš„é—®é¢˜ã€‚\n\n2. **å¯ç¼–ç¨‹è¿‡æ»¤æŠ—é”¯é½¿ï¼ˆCustom Filter Anti-Aliasingï¼ŒCFAAï¼‰**\n\nä¸€ç§æ›´çµæ´»çš„æ–¹æ³•åˆ™å‡ºçŽ°äºŽ2007å¹´AMDçš„HD2900ç³»åˆ—æ˜¾å¡ä¸­ï¼Œå…¶ç§°ä¸º**Tent Filter**ã€‚HD2900ç³»åˆ—æä¾›äº†å¯ç¼–ç¨‹çš„æ··åˆèƒ½åŠ›ï¼Œä¹Ÿç§°ä¸ºå¯ç¼–ç¨‹è¿‡æ»¤æŠ—é”¯é½¿ï¼Œå¹¶å€Ÿè¿™ç§å·¥å…·å®žçŽ°äº†Narrow Tentå’ŒWide Tentä¸¤ç§æ¨¡å¼ã€‚å¦‚å›¾9æ‰€ç¤ºï¼Œè¿™ä¸¤ç§æ–°åž‹é‡‡æ ·æ¨¡å¼åœ¨æ··åˆé‡‡æ ·ç‚¹æ—¶å¹¶æ²¡æœ‰ä½¿ç”¨ç›¸åŒæƒé‡ï¼Œè€Œæ˜¯æ ¹æ®é‡‡æ ·ç‚¹ç¦»åƒç´ ä¸­å¿ƒçš„è·ç¦»å†³å®šç›¸åº”çš„æ··åˆæƒé‡ã€‚Narrowå’ŒWideä¸¤ç§æ··åˆæ¨¡å¼çš„åŒºåˆ«ä»…åœ¨äºŽå…¶ä½¿ç”¨çš„è¿‡æ»¤æ ¸å¿ƒ(Filter Kernel)çš„å¤§å°ä¸Šã€‚è¿™ç§æ··åˆæ¨¡å¼å¯ä»¥æ ¹æ®éœ€è¦ä½¿ç”¨ä¸åŒæ ·æœ¬æ•°é‡ã€‚ç›¸è¾ƒäºŽQuincunxæ–¹æ³•ï¼Œè¿™ç§æŠ—é”¯é½¿æ¨¡å¼å¯ä»¥è¯´æ˜¯å¹³è¡¡äº†ç”»é¢é”åº¦å’ŒæŠ—é”¯é½¿åŠ›åº¦ã€‚\n\n<center>\n   <img style=\"border-radius: 0.3125em;\n   box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n   src=\"https://img-blog.csdnimg.cn/f9c56223eb1f490b9521a50c7dfc8697.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_20,color_FFFFFF,t_70,g_se,x_16\">\n   <br><div style=\"color: #999;\">\n   å›¾ 12 6x Narrow Tent | 6x Wide Tent</div>\n</center> \n\n### 2.1.2 åŽå¤„ç†æŠ—é”¯é½¿æŠ€æœ¯\n\nè™½ç„¶åŸºäºŽé‡‡æ ·çš„æŠ—é”¯é½¿ç®—æ³•ä¸ä»…åŽŸç†ç®€å•ï¼Œåœ¨é‡‡æ ·ç‚¹è¶³å¤Ÿçš„æƒ…å†µä¸‹ä¹Ÿæœ‰å¾ˆä¼˜ç§€çš„æ•ˆæžœï¼Œä½†åœ¨æ€§èƒ½æ–¹é¢ä»ç„¶ä¼šå¸¦æ¥å·¨å¤§çš„å¼€é”€ã€‚æ­¤å¤–åŸºäºŽé‡‡æ ·çš„æŠ—é”¯é½¿åœ¨è¿‘æœŸæµè¡Œçš„æ¸²æŸ“æ¨¡å¼ä¸­ï¼ˆä¾‹å¦‚å»¶è¿Ÿæ¸²æŸ“ï¼‰åŸºäºŽå„ç§åŽŸå› æ›´éš¾è¢«å®žçŽ°ã€‚ç”±æ­¤è¯žç”Ÿäº†å¦ä¸€ç§éžåŸºäºŽé‡‡æ ·çš„æŠ—é”¯é½¿æ–¹æ³•â€”â€”åŽå¤„ç†æŠ—é”¯é½¿ã€‚è¿™ä¸ªæ–¹æ³•æ¸²æŸ“å‡ºæœªä½¿ç”¨æŠ—é”¯é½¿çš„åŽŸç”Ÿç”»é¢ï¼ˆæ— ä»»ä½•é‡‡æ ·å’Œç¼©æ”¾ï¼‰ï¼ŒéšåŽå°è¯•é€šè¿‡å¯¹æˆå“ç”»é¢çš„åˆ†æžæ¥å‡å°‘é”¯é½¿å’Œå¤±çœŸã€‚\n\næ€»ä½“è€Œè¨€ï¼Œæ‰€æœ‰çš„åŽå¤„ç†æŠ—é”¯é½¿éƒ½åŒ…å«äº†ä»¥ä¸‹ä¸‰ä¸ªæ­¥éª¤ï¼Œè€Œä¸åŒçš„åŽå¤„ç†æŠ—é”¯é½¿ä¸»è¦åŒºåˆ«å°±åœ¨è¿™ä¸‰ä¸ªæ­¥éª¤çš„å…·ä½“å®žçŽ°æ–¹æ³•ä¸Šã€‚\n\n1. **æ£€æµ‹å›¾åƒä¸­ä¸è¿žç»­çš„éƒ¨åˆ†ï¼Œå³æ£€æµ‹è¾¹ç¼˜ä¿¡æ¯**\n2. **é€šè¿‡è¿™äº›ä¸è¿žç»­éƒ¨åˆ†çš„ä¿¡æ¯é‡å»ºåŽŸå§‹è¾¹ç¼˜ä¿¡æ¯**\n3. **å¯¹ä¼°æµ‹è¾¹ç¼˜ä¸Šçš„åƒç´ è¿›è¡Œé‡ç€è‰²**\n\n**å½¢æ€æŠ—é”¯é½¿ï¼ˆMorphological Anti-Aliasingï¼Œç®€ç§° MLAAï¼‰**ï¼Œæ˜¯ AMD æŽ¨å‡ºçš„å®Œå…¨åŸºäºŽ CPU å¤„ç†çš„æŠ—é”¯é½¿è§£å†³æ–¹æ¡ˆã€‚ä¾‹å¦‚å›¾10å±•ç¤ºäº†MLAAå¯¹è¾¹ç¼˜çš„è¯†åˆ«å’Œé‡å»ºæ–¹å¼ã€‚å·¦ä¾§æ˜¯èµ°æ ·å›¾åƒã€‚æˆ‘ä»¬çš„ç›®çš„æ˜¯ç¡®å®šè¾¹ç¼˜çš„å¯èƒ½æ–¹å‘ã€‚ä¸­é—´å›¾å±•ç¤ºäº†ç®—æ³•é€šè¿‡æ£€æŸ¥ç›¸é‚»åƒç´ æ¥è®°å½•å…¶ä¸ºè¾¹ç¼˜çš„å¯èƒ½æ€§ï¼Œå›¾ä¸­æ˜¾ç¤ºäº†ä¸¤ä¸ªå¯èƒ½çš„è¾¹ç¼˜ä½ç½®ã€‚å³ä¾§å›¾åˆ™å±•ç¤ºä½¿ç”¨äº†æœ€ä½³çš„æŽ¨æµ‹è¾¹ç¼˜åŽï¼Œå°†ç›¸é‚»çš„é¢œè‰²ä¸Žä¼°è®¡çš„è¦†ç›–çŽ‡æˆæ¯”ä¾‹åœ°æ··åˆåˆ°ä¸­å¿ƒåƒç´ ä¸­ã€‚\n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/083100ab8f5d45f99fc4717e34186570.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_20,color_FFFFFF,t_70,g_se,x_16\">\n    <br><div style=\"color: #999;\">\n    å›¾ 13 å½¢æ€å­¦æŠ—é”¯é½¿</div>\n</center> \n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/ffee10ce178c4057b5427684ed580f76.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_16,color_FFFFFF,t_70,g_se,x_16\">\n<br><div style=\"color: #999;\">å›¾ 14 å·¦å›¾ï¼šMLAA æ¨¡å¼åŠå…¶é‡å»ºè¾¹ç¼˜ï¼Œå³å›¾ï¼šMLAA ä¸­ä½¿ç”¨çš„ä¸è¿žç»­æ¨¡å¼</div></center> \n\n### 2.1.3 æ··åˆæŠ—é”¯é½¿ç®—æ³•\n\näº†è§£äº†ä¸¤å¤§ç±»æŠ—é”¯é½¿æŠ€æœ¯æ–¹å‘åŽï¼Œæˆ‘ä»¬ä¼šåŽ»æ€è€ƒçŽ°æœ‰æµè¡Œçš„æŠ—é”¯é½¿æŠ€æœ¯æ˜¯å¦‚ä½•å¼¥è¡¥ä¼ ç»Ÿç®—æ³•çš„é—æ†¾ã€‚\n\n**ä¸€ç§å¯è¡Œçš„è§£å†³æ–¹æ¡ˆæ˜¯å°†åŸºäºŽé‡‡æ ·çš„æŠ—é”¯é½¿ç®—æ³•å’ŒåŽæœŸå¤„ç†æŠ—é”¯é½¿æŠ€æœ¯ç»“åˆèµ·æ¥ã€‚**è¿™ç§æ–°çš„æ··åˆæŠ—é”¯é½¿ç®—æ³•åœ¨å¯¹ç”»é¢è¿›è¡Œå¤šæ¬¡é‡‡æ ·çš„åŒæ—¶ï¼Œä¼šç»“åˆåŽæœŸå¤„ç†æŠ—é”¯é½¿ç®—æ³•ä»¥è¾“å‡ºæœ€ç»ˆç”»é¢ã€‚è¿™æ ·åšçš„å¥½å¤„æ˜¯æ˜¾è€Œæ˜“è§çš„ï¼šæ–°ç®—æ³•æ—¢é¿å…äº†çº¯åŽæœŸå¤„ç†æŠ—é”¯é½¿çš„ç§ç§ç¼ºç‚¹(æ¯”å¦‚è¯´ä¸èƒ½å¤„ç†å­åƒç´ å¤±çœŸå’Œé€ æˆè¾¹ç¼˜é—ªçƒçš„é—®é¢˜)ï¼Œåœ¨åŒæ ·çš„æ€§èƒ½æŸå¤±ä¸‹ï¼Œå¯¹äºŽå‡ ä½•å¤±çœŸçš„å¤„ç†ç»“æžœåˆæ¯”çº¯ç²¹åŸºäºŽé‡‡æ ·çš„æŠ—é”¯é½¿ç®—æ³•å¥½å¾—å¤šã€‚\n\n1. **å¿«é€Ÿè¿‘ä¼¼æŠ—é”¯é½¿ï¼ˆFast Approximate Anti-Aliasingï¼ŒFXAAï¼‰**\n\n  å¿«é€Ÿè¿‘ä¼¼æŠ—é”¯é½¿æ˜¯ä¼ ç»Ÿ MSAAæ•ˆæžœçš„ä¸€ç§é«˜æ€§èƒ½è¿‘ä¼¼ã€‚å®ƒæ˜¯ä¸€ç§å•ç¨‹åƒç´ ç€è‰²å™¨ï¼Œå’Œ MLAAä¸€æ ·è¿è¡ŒäºŽç›®æ ‡æ¸¸æˆæ¸²æŸ“ç®¡çº¿çš„åŽæœŸå¤„ç†é˜¶æ®µï¼Œä½†ä¸åƒåŽè€…é‚£æ ·ä½¿ç”¨ DirectComputeï¼Œè€Œåªæ˜¯å•çº¯çš„åŽæœŸå¤„ç†ç€è‰²å™¨ï¼Œä¸ä¾èµ–äºŽä»»ä½•GPUè®¡ç®— APIã€‚æ­£å› ä¸ºå¦‚æ­¤ï¼ŒFXAAæŠ€æœ¯å¯¹æ˜¾å¡æ²¡æœ‰ç‰¹æ®Šè¦æ±‚ï¼Œå®Œå…¨å…¼å®¹ NVIDIAã€AMD çš„ä¸åŒæ˜¾å¡(MLAAä»…æ”¯æŒAMD)å’Œ DirectX 9.0ã€DirectX 10ã€DirectX 11ã€‚\n\n2. **æ—¶é—´æ€§æŠ—é”¯é½¿ï¼ˆTemporal Anti-Aliasingï¼ŒTXAAï¼‰**\n\n  æ—¶é—´æ€§æŠ—é”¯é½¿å°†MSAAã€æ—¶é—´æ»¤æ³¢ä»¥åŠåŽæœŸå¤„ç†ç›¸ç»“åˆï¼Œç”¨äºŽå‘ˆçŽ°æ›´é«˜çš„è§†è§‰ä¿çœŸåº¦ã€‚ä¸ŽCGç”µå½±ä¸­æ‰€é‡‡ç”¨çš„æŠ€æœ¯ç±»ä¼¼ï¼ŒTXAAé›†MSAAçš„å¼ºå¤§åŠŸèƒ½ä¸Žå¤æ‚çš„è§£æžæ»¤é•œäºŽä¸€èº«ï¼Œå¯å‘ˆçŽ°å‡ºæ›´åŠ å¹³æ»‘çš„å›¾åƒæ•ˆæžœã€‚æ­¤å¤–ï¼ŒTXAAè¿˜èƒ½å¤Ÿå¯¹å¸§ä¹‹é—´çš„æ•´ä¸ªåœºæ™¯è¿›è¡ŒæŠ–åŠ¨é‡‡æ ·ï¼Œä»¥å‡å°‘é—ªçƒæƒ…å½¢ï¼ˆæ—¶é—´æ€§é”¯é½¿ï¼‰ã€‚ç›®å‰ï¼ŒTXAAæœ‰ä¸¤ç§æ¨¡å¼ï¼šTXAA 2Xå’Œ TXAA 4Xã€‚TXAA 2Xå¯æä¾›å ªæ¯”8X MSAAçš„è§†è§‰ä¿çœŸåº¦ï¼Œç„¶è€Œæ‰€éœ€æ€§èƒ½å´ä¸Ž 2X MSAAç›¸ç±»ä¼¼ï¼›TXAA4Xçš„å›¾åƒä¿çœŸåº¦èƒœè¿‡8XMSAAï¼Œæ‰€éœ€æ€§èƒ½ä»…ä»…ä¸Ž4X MSAAç›¸å½“ã€‚\n\n3. **å¤šå¸§é‡‡æ ·æŠ—é”¯é½¿ï¼ˆMulti-Frame Sampled Anti-Aliasingï¼ŒMFAAï¼‰**\n\n  å¤šå¸§é‡‡æ ·æŠ—é”¯é½¿ï¼ˆMulti-Frame Sampled Anti-Aliasingï¼ŒMFAAï¼‰æ˜¯NVIDIAå…¬å¸æ ¹æ®MSAA æ”¹è¿›å‡ºçš„ä¸€ç§æŠ—é”¯é½¿æŠ€æœ¯ã€‚ç›®å‰ä»…æ­è½½Maxwellæž¶æž„GPUçš„æ˜¾å¡æ‰èƒ½ä½¿ç”¨ã€‚å¯ä»¥å°†MFAAç†è§£ä¸ºMSAAçš„ä¼˜åŒ–ç‰ˆï¼Œèƒ½å¤Ÿåœ¨å¾—åˆ°å‡ ä¹Žç›¸åŒæ•ˆæžœçš„åŒæ—¶æå‡æ€§èƒ½ä¸Šçš„è¡¨çŽ°ã€‚MFAAä¸ŽMSAAæœ€å¤§çš„å·®åˆ«å°±åœ¨äºŽåœ¨åŒæ ·å¼€å¯4å€æ•ˆæžœçš„æ—¶å€™MSAAæ˜¯çœŸæ­£çš„é’ˆå¯¹æ¯ä¸ªè¾¹ç¼˜åƒç´ å‘¨å›´çš„ 4 ä¸ªåƒç´ è¿›è¡Œé‡‡æ ·ï¼ŒMFAAåˆ™æ˜¯ä»…ä»…åªæ˜¯é‡‡ç”¨äº¤é”™çš„æ–¹å¼é‡‡æ ·è¾¹ç¼˜æŸä¸ªåƒç´ å‘¨å›´çš„ä¸¤ä¸ªåƒç´ ã€‚\n\nå¦ä¸€ç§å¯è¡Œçš„æ–¹æ¡ˆè¿˜æœªè¢«å¹¿æ³›åº”ç”¨ï¼š**åœ¨æ¸²æŸ“æ—¶è®°å½•é¢å¤–çš„å‡ ä½•ä¿¡æ¯ï¼Œä»¥ä¾›åŽå¤„ç†æŠ—é”¯é½¿ä½¿ç”¨ã€‚**ç›®å‰çš„å®žçŽ°æœ‰GPAA (Geometric Post-process Anti-Aliasing) ä»¥åŠGBAA(Geometry Buffer Anti-Aliasing)ç­‰ã€‚\n\n## 2.2 è¶…åˆ†è¾¨çŽ‡æŠ€æœ¯\n\nå›¾åƒåˆ†è¾¨çŽ‡ä½“çŽ°äº†ç³»ç»Ÿå®žé™…æ‰€èƒ½åæ˜ ç‰©ä½“ç»†èŠ‚ä¿¡æ¯çš„èƒ½åŠ›ã€‚ç›¸è¾ƒäºŽä½Žåˆ†è¾¨çŽ‡å›¾åƒï¼Œé«˜åˆ†è¾¨çŽ‡å›¾åƒé€šå¸¸åŒ…å«æ›´å¤§çš„åƒç´ å¯†åº¦ã€æ›´ä¸°å¯Œçš„çº¹ç†ç»†èŠ‚åŠæ›´é«˜çš„å¯ä¿¡èµ–åº¦ã€‚ç”±æ­¤ï¼Œä»Žè½¯ä»¶å’Œç®—æ³•çš„è§’åº¦ç€æ‰‹ï¼Œå®žçŽ°å›¾åƒè¶…åˆ†è¾¨çŽ‡é‡å»ºçš„æŠ€æœ¯æˆä¸ºäº†å›¾åƒå¤„ç†å’Œè®¡ç®—æœºè§†è§‰ç­‰å¤šä¸ªé¢†åŸŸçš„çƒ­ç‚¹ç ”ç©¶è¯¾é¢˜ã€‚\n\nå›¾åƒçš„è¶…åˆ†è¾¨çŽ‡é‡å»ºæŠ€æœ¯æŒ‡çš„æ˜¯å°†ç»™å®šçš„ä½Žåˆ†è¾¨çŽ‡å›¾åƒé€šè¿‡ç‰¹å®šçš„ç®—æ³•æ¢å¤æˆç›¸åº”çš„é«˜åˆ†è¾¨çŽ‡å›¾åƒã€‚è¶…åˆ†è¾¨çŽ‡æ–¹æ³•é€šå¸¸åˆ†ä¸º**å•å¸§è¶…åˆ†è¾¨çŽ‡ï¼ˆSingle Image Superresolutionï¼ŒSISRï¼‰å’Œå¤šå¸§è¶…åˆ†è¾¨çŽ‡ï¼ˆMulti-image Superresolutionï¼ŒMISRï¼‰ã€æ—¶åŸŸè¶…é‡‡æ ·ï¼ˆTemporal Super Samplingï¼‰**ã€‚\n\n### 2.2.1 å•å¸§è¶…åˆ†è¾¨çŽ‡\n\nå•å¸§è¶…åˆ†è¾¨çŽ‡æ˜¯å’ŒDLSSéžå¸¸ç›¸å…³çš„ä¸€ä¸ªç ”ç©¶æ–¹å‘ã€‚å°¤å…¶æ˜¯è·Ÿç€è¿™ä¸¤å¹´æ·±åº¦å­¦ä¹ çš„åº”ç”¨çš„çƒ­åº¦ï¼Œè¿™ä¸ªé—®é¢˜çš„state of the artä¹Ÿæé«˜äº†å¾ˆå¤šï¼Œè¿™ä¸ªæ–¹å‘çš„ç ”ç©¶ä¹Ÿç»å¸¸ä¸Šå›½å†…å…¬ä¼—å·çš„å¤´æ¡ï¼Œä¾‹å¦‚SRCNNï¼ŒSRGANï¼ŒESRGANç­‰ç­‰ã€‚\n\nå¯æ˜¯å•å¸§è¶…åˆ†è¾¨çŽ‡å…¶å®žæ˜¯ä¸ªéžå¸¸å›°éš¾çš„é—®é¢˜ï¼Œå› ä¸ºæœ¬è´¨ä¸Šéœ€è¦ç”Ÿæˆä½Žåˆ†è¾¨çŽ‡å›¾åƒä¸­å®Œå…¨ä¸å­˜åœ¨çš„ä¿¡æ¯ã€‚ç”¨ç¥žç»ç½‘ç»œè§£å†³è¿™ä¸€ç±»é—®é¢˜ï¼Œ**æœ¬è´¨ä¸Šå°±æ˜¯åœ¨è®­ç»ƒé›†ä¸­å­¦ä¹ åˆ°å„ç§ä½Žåˆ†è¾¨çŽ‡çš„åƒç´ å’Œé«˜åˆ†è¾¨çŽ‡åƒç´ çš„ä¸€ä¸ªå¯¹åº”å…³ç³»ã€‚**æœ‰äº†è¿™ä¸ªæ˜ å°„åŽï¼Œç¥žç»ç½‘ç»œèƒ½åšåˆ°æ¯”ä¸€èˆ¬çš„åŸºäºŽæ’å€¼ï¼ˆinterpolationï¼‰çš„æ–¹æ³•æ›´å¥½çš„æ•ˆæžœã€‚\n\n<center>\n   <img style=\"border-radius: 0.3125em;\n   box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n   src=\"https://img-blog.csdnimg.cn/a89f7614853a4ff7aa2ecff3c609bc9b.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_20,color_FFFFFF,t_70,g_se,x_16\">\n   <br><div style=\"color: #999;\">å›¾ 15 å•å¸§è¶…åˆ†è¾¨çŽ‡</div></center> \n\n**ä½†å°½ç®¡å¦‚æ­¤ï¼Œè¿™æ ·ç”Ÿæˆå‡ºæ¥çš„ä¿¡æ¯å…¶å®žæ˜¯å®Œå…¨åŸºäºŽè®­ç»ƒé›†å›¾ç‰‡ä¸­çš„æ•°æ®åˆ†å¸ƒï¼Œè€Œå¹¶ä¸æ˜¯å¯¹æˆ‘ä»¬å®žé™…æ­£åœ¨æ¸²æŸ“çš„åœºæ™¯çš„é‡‡æ ·ã€‚**æ‰€ä»¥å•å¸§è¶…åˆ†è¾¨çŽ‡çš„ç»“æžœç»å¸¸ä¼šå’ŒåŽŸç”Ÿåˆ†è¾¨çŽ‡æ¸²æŸ“åœ¨é£Žæ ¼å’Œæ ·å¼ä¸Šä¸ä¸€è‡´ã€‚å¯¹äºŽDLSSæ¥è¯´ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯é‡å»ºå‡ºå’Œé«˜åˆ†è¾¨çŽ‡æ¸²æŸ“ä¸€æ¨¡ä¸€æ ·çš„ç»“æžœï¼Œæ‰€ä»¥å•å¸§è¶…åˆ†è¾¨çŽ‡ä¸€ç±»çš„å·¥ä½œå¯¹å®žæ—¶æ¸²æŸ“æ¥è¯´å¾ˆéš¾é€‚ç”¨ã€‚\n\n### 2.2.2 å¤šå¸§è¶…åˆ†è¾¨çŽ‡\n\nå¦ä¸€ç±»è¶…é‡‡æ ·çš„å·¥ä½œåˆ™æ˜¯é’ˆå¯¹è§†é¢‘ï¼Œæˆ–è€…æ‰‹æœºæ‘„å½±çš„å¤šå¸§è¶…åˆ†è¾¨çŽ‡ã€‚å¤šå¸§è¶…åˆ†è¾¨çŽ‡å¹¶ä¸åƒå•å¸§è¶…åˆ†è¾¨çŽ‡é‚£ä¹ˆçš„å›°éš¾ï¼Œå› ä¸ºæˆ‘ä»¬ä¸å®Œå…¨éœ€è¦å¡«è¡¥åŽŸæœ¬ä¸å­˜åœ¨çš„ä¿¡æ¯ã€‚æœ‰å¤šä¸ªä½Žåˆ†è¾¨çŽ‡å›¾ç‰‡çš„æƒ…å†µä¸‹ï¼Œè¿™ä¸ªé—®é¢˜ä¼šå¯æŽ§å¾ˆå¤šï¼Œå¤šå¸§åˆæˆçš„é«˜åˆ†è¾¨çŽ‡å›¾ç‰‡å¾€å¾€åœ¨å…‰å­¦ç»†èŠ‚ä¸Šçš„è¿˜åŽŸçš„è´¨é‡ä¸Šä¼šæ¯”å•å¸§è¶…åˆ†è¾¨çŽ‡çš„é«˜è®¸å¤šã€‚\n\nç„¶è€Œé’ˆå¯¹è§†é¢‘æˆ–è€…æ‘„å½±çš„ç®—æ³•ä¹Ÿå¹¶ä¸å¤ªèƒ½ç›´æŽ¥æ¬è¿‡æ¥ç”¨äºŽå®žæ—¶æ¸²æŸ“ï¼ŒåŽŸå› æœ‰è®¸å¤šã€‚\n\n1. åœ¨æ¸²æŸ“ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨åˆ°çš„æ•°æ®æ˜¯æ¯”è§†é¢‘å¤šå¾ˆå¤šçš„ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—æ¯ä¸ªåƒç´ ç²¾ç¡®çš„è¿åŠ¨å‘é‡ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥æœ‰åœºæ™¯å‡½æ•°çš„ç²¾ç¡®é‡‡æ ·ï¼Œæœ‰HDRé¢œè‰²ï¼Œç”šè‡³åƒç´ çš„ç²¾ç¡®æ·±åº¦ã€‚**ä¸åˆ©ç”¨è¿™äº›ä¿¡æ¯ï¼Œè®¾è®¡å‡ºæ¥çš„ç®—æ³•åœ¨æ•ˆçŽ‡å’Œè´¨é‡ä¸Šéƒ½ä¸ä¼šæ˜¯æœ€ä¼˜çš„ã€‚**\n\n2. è®¸å¤šè§†é¢‘è¶…åˆ†è¾¨çŽ‡çš„å·¥ä½œæ˜¯éœ€è¦**ç”¨æ—¶åºä¸Šæœªæ¥çš„å›¾ç‰‡æ¥é‡å»ºå½“å‰å¸§çš„å›¾ç‰‡çš„**ã€‚å› ä¸ºå®žæ—¶æ¸²æŸ“å¯¹å»¶è¿Ÿçš„è¦æ±‚ï¼Œè¿™ä¹Ÿæ˜¾ç„¶ä¸é€‚ç”¨ã€‚\n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/c7ba067031a4440baefb6e0d02c61e5e.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_20,color_FFFFFF,t_70,g_se,x_16\">\n    <br><div style=\"color: #999;\">å›¾ 16 å¤šå¸§è¶…åˆ†è¾¨çŽ‡</div></center> \n\n### 2.2.3 æ—¶åŸŸè¶…é‡‡æ ·\n\nåˆ©ç”¨æŠŠæ¸²æŸ“çš„æ ·æœ¬åˆ†å¸ƒåœ¨å¤šä¸ªå¸§ä¸Šï¼Œå¹¶ä¸”ç”¨è¿™äº›å¤šå¸§å¤åˆçš„æ ·æœ¬é‡å»ºå‡ºæœ€ç»ˆæ¸²æŸ“çš„å›¾ç‰‡ï¼Œè¿™åœ¨å®žæ—¶æ¸²æŸ“é¢†åŸŸå¤ªå¸ç©ºè§æƒ¯äº†ã€‚å‡ ä¹Žæ‰€æœ‰å¼•æ“Žéƒ½åœ¨ç”¨çš„Temporal Antialiasing(TAA)ï¼Œæˆ–è€…æ¸¸æˆä¸»æœºä¸Šéžå¸¸æµè¡Œçš„Checkerboard Renderingéƒ½æ˜¯è¿™ä¹ˆä¸€ä¸ªæ€è·¯ã€‚\n\n<center>\n   <img style=\"border-radius: 0.3125em;\n   box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n   src=\"https://img-blog.csdnimg.cn/3dae0dd648e44c07b6eeaa4da296d74f.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_20,color_FFFFFF,t_70,g_se,x_16\">\n   <br><div style=\"color: #999;\"> å›¾ 17 æ£‹ç›˜æ¸²æŸ“</div>\n</center> \n\nè¿™ä¸€ç±»ç®—æ³•åˆ©ç”¨äº†æ¸²æŸ“å›¾ç‰‡çš„**æ—¶åŸŸè¿žè´¯æ€§ï¼ˆTemporal coherencyï¼‰**ï¼Œæ—¢æ¸²æŸ“ç»“æžœçš„å¸§ä¸Žå¸§ä¹‹é—´å¤§ä½“æ˜¯è¿žç»­çš„ï¼Œå‘ç”Ÿé«˜é¢‘å˜åŒ–çš„æ¦‚çŽ‡ä¸é«˜ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬å¯ä»¥å‡è®¾éœ€è¦æ¸²æŸ“çš„åœºæ™¯åœ¨ä¸Šä¸€å¸§å’Œå½“å‰å¸§å‡ ä¹Žä¸€æ ·ã€‚å¦‚æžœè¿™ä¸ªå‡è®¾æˆç«‹çš„è¯ï¼Œæˆ‘ä»¬å¤§å¯ä»¥ä¹‹é—´å¤ç”¨è¿‡åŽ»å¸§ä¸Šå¯¹åœºæ™¯é‡‡æ ·çš„æ ·æœ¬æ¥é‡å»ºå½“å‰å¸§ã€‚\n\n<center>\n   <img style=\"border-radius: 0.3125em;\n   box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n   src=\"https://img-blog.csdnimg.cn/50ae89e4ff2f4e0d86364167110377af.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_17,color_FFFFFF,t_70,g_se,x_16\">\n   <br><div style=\"color: #999;\">å›¾ 18 TAA</div>\n</center> \n\nè¿™æ ·åšçš„å¥½å¤„æ˜¯ï¼Œæ¯ä¸€å¸§çš„é‡‡æ ·çŽ‡éžå¸¸ä½Žï¼Œæ‰€ä»¥æ¸²æŸ“æ€§èƒ½ä¼šæœ‰å¾ˆå¤§çš„æå‡ï¼Œç„¶è€Œé‡å»ºå›¾åƒæ—¶çš„æ ·æœ¬è¿˜éƒ½ç¡®å®žæ˜¯å¯¹äºŽåœºæ™¯å‡½æ•°çš„æ— åé‡‡æ ·ï¼Œæ‰€ä»¥æœ€ç»ˆé‡å»ºçš„å›¾åƒè´¨é‡ä¹Ÿä¼šå’ŒåŽŸç”Ÿåˆ†è¾¨çŽ‡æ¸²æŸ“éžå¸¸ä¸€è‡´ã€‚\n\nç„¶è€Œå¤©ä¸‹å“ªæœ‰è¿™ç­‰å¥½äº‹ï¼Œå®žæ—¶æ¸²æŸ“æˆ–è€…æ¸¸æˆçš„å›¾ç‰‡åºåˆ—ä¸­å‡ ä¹Žæ¯ä¸€å¸§éƒ½æœ‰æˆ–å¤šæˆ–å°‘çš„å˜åŒ–ï¼Œä»Žè§’è‰²åŠ¨ç”»ï¼Œåˆ°åŠ¨æ€å…‰å½±ï¼Œåˆ°ç²’å­ç‰¹æ•ˆã€‚ç›´æŽ¥å¤ç”¨è¿‡åŽ»å¸§çš„æ ·æœ¬æ¥é‡å»ºå½“å‰å¸§çš„å›¾ç‰‡ä¼šä½¿é‡å»ºçš„ç»“æžœä¸­äº§ç”Ÿå¾ˆå¤§çš„é”™è¯¯ã€‚**è¿™ç§é”™è¯¯åœ¨æ¸²æŸ“å›¾ç‰‡ä¸­ä¼šä»¥å»¶è¿Ÿï¼Œæˆ–è€…é¬¼å½±ï¼ˆghostingï¼‰çš„å½¢å¼å‘ˆçŽ°å‡ºæ¥ã€‚**è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆï¼Œæ‰€æœ‰å®žæ—¶æ¸²æŸ“ä¸­çš„æ—¶åŸŸè¶…é‡‡æ ·ç®—æ³•ï¼Œä¾‹å¦‚TAAï¼Œéƒ½æœ‰éžå¸¸é‡è¦çš„ä¸€æ­¥åŽ»â€œçº æ­£â€è¿‡åŽ»å¸§ä¸­æ ·æœ¬çš„é”™è¯¯ã€‚\n\n<center>\n   <img style=\"border-radius: 0.3125em;\n   box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n   src=\"https://img-blog.csdnimg.cn/8395216147f04632a142ba5f02cfcb52.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_14,color_FFFFFF,t_70,g_se,x_16\">\n   <br><div style=\"color: #999;\">å›¾ 19 é¬¼å½±çŽ°è±¡</div>\n</center> \n\nè¿™ä¸€ç±»ç®—æ³•éœ€è¦é¦–å…ˆæ£€æµ‹è¿‡åŽ»å¸§å’Œå½“å‰å¸§å› ä¸ºåœºæ™¯çš„å˜åŒ–å¯¼è‡´çš„æ ·æœ¬é”™è¯¯ï¼Œç„¶åŽåœ¨ä¸å½±å“ç”»è´¨å¤ªå¤šçš„æƒ…å†µä¸‹ï¼Œâ€œåˆç†â€çš„çº æ­£é‚£äº›é”™è¯¯çš„æ ·æœ¬ã€‚ä¹ä¸€çœ‹è¿™ç®€ç›´æ˜¯ä¸ªè®¡ç®—æœºè§†è§‰é—®é¢˜ï¼Œç„¶è€Œåœ¨å®žæ—¶æ¸²æŸ“ä¸­è¿™ä¸€æ­¥éœ€è¦éžå¸¸é«˜æ•ˆçš„å®Œæˆã€‚æ‰€ä»¥è¿‡åŽ»åå‡ å¹´ï¼Œæ¸¸æˆå¼€å‘è€…ç»žå°½è„‘æ±çš„å‘æ˜Žäº†å„ç§**â€œå¯å‘å¼â€çš„æ–¹æ³•ï¼ˆHeuristicsï¼‰**ã€‚\n\nç›®å‰è§£å†³è¿™ä¸ªé—®é¢˜æ•ˆæžœæœ€å¥½ï¼Œä¹Ÿæœ€å¸¸ç”¨çš„Heuristicï¼Œå«åš**Neighborhood Clamping**ï¼Œæ˜¯Epicçš„Brian Karisåœ¨14å¹´çš„SIGGRAPHçš„ä¸€ä¸ªå®žæ—¶æ¸²æŸ“è®²åº§é‡Œæœ€å…ˆæåˆ°çš„ã€‚æ€è·¯å…¶å®žå¾ˆç®€å•ï¼Œå°±æ˜¯æŠŠè¿‡åŽ»å¸§é‡‡æ ·çš„æ ·æœ¬çš„å€¼çš„èŒƒå›´ï¼Œé™åˆ¶åœ¨å½“å‰å¸§åƒç´ å‘¨å›´3x3å¤§å°çš„Local neighborhoodçš„æ‰€æœ‰æ ·æœ¬çš„å€¼çš„èŒƒå›´å†…ã€‚\n\n# **3 DLSS 2.0æŠ€æœ¯ä»‹ç»**\n\n## 3.1 DLSS 2.0åŽŸç†ä¸Žæ€è·¯\n\nDLSS 2.0é¦–å…ˆä¹Ÿæ˜¯ä¸€ä¸ªåŸºäºŽå¤šå¸§çš„å›¾åƒé‡å»ºæŠ€æœ¯ã€‚å› ä¸ºæˆ‘ä»¬çš„ç›®æ ‡æ˜¯é‡å»ºå‡ºå’ŒåŽŸç”Ÿæ¸²æŸ“ä¸€è‡´çš„ç”»é¢ï¼Œæ‰€ä»¥åŸºäºŽç”¨å•å¸§çš„ç®—æ³•åŽ»â€œæƒ³è±¡â€ä¸å­˜åœ¨çš„ä¿¡æ¯æ˜¯ä¸é€‚ç”¨çš„ã€‚\n\né‚£DLSS 2.0å’ŒçŽ°æœ‰çš„å®žæ—¶æ¸²æŸ“ä¸­çš„æ—¶åŸŸè¶…é‡‡æ ·æœ‰ä»€ä¹ˆåŒºåˆ«å‘¢ï¼ŸDLSS 2.0æŠ›å¼ƒäº†äººè‚‰æ‰‹è°ƒçš„å¯å‘å¼ç®—æ³•ï¼Œç”¨ä¸€ä¸ªåœ¨è¶…çº§è®¡ç®—æœºä¸Šæ•°ä¸‡å¼ è¶…é«˜è´¨é‡å›¾ç‰‡è®­ç»ƒçš„ç¥žç»ç½‘ç»œæ¥ä»£æ›¿è¿™äº›Heuristicsã€‚å°±åƒæ·±åº¦å­¦ä¹ åœ¨å‡ å¹´å‰åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸè¶…è¶Šäº†å„ç§æ‰‹è°ƒçš„ç‰¹å¾æå–ç®—æ³•ä¸€æ ·ï¼Œæ·±åº¦å­¦ä¹ ç¬¬ä¸€æ¬¡åœ¨å®žæ—¶æ¸²æŸ“ä¸­ä¹Ÿéžå¸¸åˆç†çš„è·‘èµ¢äº†å›¾å½¢é¢†åŸŸçš„æ‰‹è°ƒç®—æ³•ã€‚\n\nç”¨DLSS 2.0é‡å»ºçš„æ¸²æŸ“å›¾åƒåºåˆ—è¾¾åˆ°äº†éžå¸¸é«˜çš„å¤šå¸§æ ·æœ¬åˆ©ç”¨çŽ‡ï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆåªç”¨å››åˆ†ä¹‹ä¸€çš„æ ·æœ¬å°±å¯ä»¥é‡å»ºå‡ºåª²ç¾ŽåŽŸç”Ÿåˆ†è¾¨çŽ‡æ¸²æŸ“çš„å›¾åƒè´¨é‡çš„åŽŸå› ã€‚\n\n<center>\n   <img style=\"border-radius: 0.3125em;\n   box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n   src=\"https://img-blog.csdnimg.cn/02404b8cfec84daab16a1f45208ebbd8.png\">\n   <br><div style=\"color: #999;\">å›¾ 20 DLSSé‡å»ºæ–¹æ³•</div>\n</center> \n\nä¸‹å›¾æ˜¯DLSS 2.0çš„ç²—ç•¥æž¶æž„ï¼š\n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/cbb1df0e4c1c4ce1a9eef04f984b5a88.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_20,color_FFFFFF,t_70,g_se,x_16\">\n    <br><div style=\"color: #999;\">å›¾ 21 DLSS 2.0æž¶æž„å›¾</div></center> \n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/ea9e3becdfcc4a36aab12011303e20c4.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_17,color_FFFFFF,t_70,g_se,x_16\">\n    <br><div style=\"color: #999;\">å›¾ 22 DLSS 2.0 ç¥žç»ç½‘ç»œ è¾“å…¥åŠè¾“å‡º</div></center> \n\n\n## 3.2 DLSS 2.0å¼•æ“Žé›†æˆ\n\nDLSS 2.0å¹¶ä¸æ˜¯ä¸€ä¸ªå•çº¯çš„å›¾åƒè¶…åˆ†è¾¨çŽ‡ç®—æ³•ã€‚å®ƒæ˜¯ä¸€ä¸ªä¸“é—¨é’ˆå¯¹å®žæ—¶æ¸²æŸ“åº”ç”¨çš„ç®—æ³•ã€‚æ‰€ä»¥å¼•æ“Žè¦é›†æˆDLSS 2.0ï¼Œéœ€è¦é…åˆçš„ä½œå‡ºç›¸åº”çš„æ”¹åŠ¨ã€‚ä½†æ‰€å¹¸æ”¹åŠ¨çš„å¹…åº¦è¿œæ¯”ç±»ä¼¼Checkerboard renderingç®€å•ã€‚\n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/d9402c34f41d477583a316ff71201788.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_20,color_FFFFFF,t_70,g_se,x_16\">\n    <br><div style=\"color: #999;\">å›¾ 23 DLSSå¼•æ“Žé›†æˆ</div></center> \n\n\n**é¦–å…ˆå½“ç„¶æ˜¯å¼•æ“Žè¦æŠŠæ‰€æœ‰çš„åƒç´ ç€è‰²å·¥ä½œåœ¨ä½Žåˆ†è¾¨çŽ‡æ‰§è¡Œï¼Œ**é€šå¸¸è¿™äº›åŒ…æ‹¬GBufferæ¸²æŸ“ï¼ŒåŠ¨æ€å…‰å½±ï¼Œå±å¹•ç‰¹æ•ˆï¼Œå…‰çº¿è¿½è¸ªç­‰ã€‚\n\n**å…¶æ¬¡ï¼ŒDLSS 2.0æ˜¯ä¸€ä¸ªèžåˆäº†æŠ—é”¯é½¿å’Œè¶…é‡‡æ ·çš„ç®—æ³•**ã€‚å¼•æ“ŽçŽ°æœ‰çš„æŠ—é”¯é½¿è§£å†³æ–¹æ¡ˆä¾‹å¦‚TAAéœ€è¦è¢«ç§»é™¤ï¼Œç„¶åŽDLSSéœ€è¦è¢«æ’å…¥åœ¨åŽå¤„ç†ï¼ˆpost processingï¼‰ä¹‹å‰ï¼Œè¿™æ ·åŽå¤„ç†å¯ä»¥å¤„ç†æŠ—é”¯é½¿åŽçš„å¹³æ»‘å›¾ç‰‡ä»¥é¿å…å„ç§artifactã€‚\n\nDLSSçš„è¾“å‡ºä¼šæ˜¯ä¸€ä¸ªé«˜åˆ†è¾¨çŽ‡çš„å›¾ç‰‡ï¼Œæ‰€ä»¥**å¼•æ“Žéœ€è¦è¶…é‡‡æ ·åŽçš„åˆ†è¾¨çŽ‡ä¸‹è®¡ç®—å„ç§åŽå¤„ç†ç‰¹æ•ˆ**ï¼Œä¾‹å¦‚æ™¯æ·±ï¼ŒåŠ¨æ€æ¨¡ç³Šï¼Œtonemappingä»¥åŠæ¸²æŸ“UIã€‚\n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/da4bfb6c94204f45bf40e13979f0e309.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_18,color_FFFFFF,t_70,g_se,x_16\">\n    <br><div style=\"color: #999;\">å›¾ 24 DLSS 2.0 æ¸²æŸ“æ­¥éª¤</div></center> \n\n## 3.3 DLSS 2.0æ¸²æŸ“åŠ é€Ÿ\n\nDLSS 2.0åŠ é€Ÿæ¸²æŸ“çš„åŽŸç†å¾ˆç®€å•ã€‚å¼€å¯DLSSåŽï¼Œå¼•æ“Žçš„æ¸²æŸ“ä¼šåœ¨1/2åˆ°1/4åƒç´ çš„ä½Žåˆ†è¾¨çŽ‡ä¸‹è¿è¡Œã€‚è¿™æ„å‘³ç€ï¼Œä¸€å¤§åŠçš„åƒç´ çº§åˆ«çš„è®¡ç®—ç›´æŽ¥è¢«ç²—æš´çš„ç æŽ‰äº†ã€‚åƒç´ çº§åˆ«çš„è®¡ç®—é€šå¸¸åŒ…æ‹¬GBufferçš„æ¸²æŸ“ï¼ŒåŠ¨æ€å…‰æºã€é˜´å½±çš„è®¡ç®—ï¼Œå±å¹•ç©ºé—´çš„ç‰¹æ•ˆä¾‹å¦‚å±å¹•ç©ºé—´çŽ¯å¢ƒé®æŒ¡ï¼ˆSSAOï¼‰ã€å±å¹•ç©ºé—´åå°„ï¼ˆSSRï¼‰ï¼Œç”šè‡³å®žæ—¶å…‰çº¿è¿½è¸ªã€‚è¿™äº›è®¡ç®—é€šå¸¸ä¹Ÿæ˜¯ä¸€å¸§é‡Œé¢æœ€è€—è´¹æ€§èƒ½çš„éƒ¨åˆ†ã€æ¯•ç«Ÿå¤§éƒ¨åˆ†çš„ç”»é¢å‡ºè‰²çš„æ¸¸æˆï¼Œåƒç´ è®¡ç®—æ˜¯ç»å¯¹çš„ç“¶é¢ˆï¼ˆpixel boundï¼‰ã€‚\n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/fc5f475f3afa49e6b7a553aa3b5f58c1.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_20,color_FFFFFF,t_70,g_se,x_16\">\n    <br><div style=\"color: #999;\"> å›¾ 25 DLSS æ¸²æŸ“åŠ é€Ÿ</div></center> \n\n\næ‰€ä»¥DLSS 2.0çš„åŠ é€Ÿå¤šå°‘ï¼Œä¹Ÿ**ç›´æŽ¥å–å†³äºŽåƒç´ è®¡ç®—åœ¨å¤šå¤§ç¨‹åº¦ä¸Šæ˜¯æ€§èƒ½ç“¶é¢ˆã€‚**é€šå¸¸æ¥è¯´ï¼Œç”»é¢è¶Šå¥½çš„3Aå¤§ä½œï¼Œè¶Šä¼šç”¨æ›´åŠ è€—è´¹æ€§èƒ½çš„æ¸²æŸ“æŠ€æœ¯ï¼Œåƒç´ ä¹Ÿå°±ä¼šæ›´å¤§ç¨‹åº¦çš„æˆä¸ºç“¶é¢ˆï¼Œè€ŒDLSSåˆ™ä¼šæä¾›æ›´å¤§çš„åŠ é€Ÿï¼\n\nåœ¨çœæŽ‰çš„æ¸²æŸ“è®¡ç®—ä¹‹ä¸Šï¼Œè¿è¡ŒDLSS 2.0è¿™ä¸ªç®—æ³•æœ¬èº«ä¼šå¼•å…¥ä¸€å®šçš„å¼€é”€ï¼Œè¿™ä¸ªå¼€é”€é€šå¸¸æ˜¯å®Œå…¨å–å†³äºŽåˆ†è¾¨çŽ‡å¤§å°çš„ï¼Œä¸éšåœºæ™¯å†…å®¹è€Œæ”¹å˜ã€‚ä¸‹é¢è¿™ä¸ªè¡¨æ ¼å±•ç¤ºäº†DLSS 2.0åœ¨ä¸åŒGPUå’Œä¸åŒåˆ†è¾¨çŽ‡ä¸‹çš„å¼€é”€ã€‚ç›¸æ¯”äºŽDLSS 1.0ï¼Œæˆ‘ä»¬æŠŠè¿™ä¸ªå¼€é”€å‡å°äº†ä¸¤å€ä»¥ä¸Šã€‚åœ¨2080Tiä¸Šï¼Œ4Kåˆ†è¾¨çŽ‡ä¸‹ä¹Ÿåªæœ‰1.5æ¯«ç§’ï¼Œå› ä¸ºæœ‰Tensor Coreçš„åŠ é€Ÿï¼Œè¿™å·²ç»å’Œæ™®é€šçš„TAAéžå¸¸æŽ¥è¿‘äº†ã€‚\n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/1a58dfc0348741b0861f2ebbec493c72.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_20,color_FFFFFF,t_70,g_se,x_16\">\n    <br><div style=\"color: #999;\">å›¾ 26 DLSS cost</div></center> \n\n# **4 DLSSæŒ‘æˆ˜ä¸Žæ€»ç»“**\n\n## 4.1 DLSSæ•ˆæžœå¯¹æ¯”\n\nDLSS 2.0å¯ä»¥å°†540pçš„æ¸²æŸ“å›¾åƒç›´æŽ¥æ”¾å¤§åˆ°1080pï¼Œæˆ–è€…720påˆ°1440pï¼Œ1080påˆ°4Kã€‚å¹¶ä¸”æ”¾å¤§çš„ç”»é¢åœ¨è´¨é‡ä»¥åŠç»†èŠ‚ç¨‹åº¦å®Œå…¨ä¸è¾“åŽŸç”Ÿåˆ†è¾¨çŽ‡æ¸²æŸ“ï¼Œç½‘ä¸Šçš„è®¸å¤šæµ‹è¯„ä¹Ÿéƒ½åæ˜ äº†è¿™ä¸€ç‚¹ã€‚\n\nä¸‹é¢æ”¾å‡ ç»„ä¾‹å­ï¼Œç¬¬ä¸€ä¸ªæ˜¯ä¸€ä¸ªå‡ ä½•éžå¸¸å¯†é›†çš„æ£®æž—åœºæ™¯ï¼Œå¼€å¯å®žæ—¶å…‰çº¿è¿½è¸ªåŽ540påŽŸç”Ÿåˆ†è¾¨çŽ‡æ¸²æŸ“å¤§æ¦‚æœ‰89fpsï¼Œä½†æ˜¯å› ä¸ºåˆ†è¾¨çŽ‡å¤ªä½Žï¼Œç”»é¢éžå¸¸æ¨¡ç³Šã€‚\n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/b571906c1f2a4e6c895085427ef1cae0.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_14,color_FFFFFF,t_70,g_se,x_16\">\n    <br><div style=\"color: #999;\"> å›¾ 27 540p - 89fpsåŽŸç”Ÿæ¸²æŸ“æ•ˆæžœå›¾</div></center> \n\nå¦‚æžœ1080pæ¸²æŸ“ï¼Œç”»é¢åˆ™æ¸…æ™°äº†å¾ˆå¤šï¼Œä½†æ˜¯å¸§çŽ‡ä¹Ÿé™ä½Žåˆ°48fps\n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/df3b877917694856b7f2ef7a16362116.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_14,color_FFFFFF,t_70,g_se,x_16\">\n    <br><div style=\"color: #999;\">å›¾ 28 1080p â€“ 48fpsåŽŸç”Ÿæ¸²æŸ“æ•ˆæžœå›¾</div></center> \n\nä½¿ç”¨DLSS2.0, ç”¨540påˆ†è¾¨çŽ‡æ¸²æŸ“çš„ç”»é¢ä½œä¸ºè¾“å…¥ï¼Œé€šè¿‡æ·±åº¦å­¦ä¹ è¶…é‡‡æ ·è‡³1080pï¼Œå¸§çŽ‡æå‡åˆ°86fpsï¼Œå¹¶ä¸”ç”»è´¨å’ŒåŽŸç”Ÿååˆ†æŽ¥è¿‘ã€‚\n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/8b12156c24f64cd69ead4edacf8a299e.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_14,color_FFFFFF,t_70,g_se,x_16\">\n    <br><div style=\"color: #999;\">å›¾ 29 DLSS 2.0ï¼Œ540pæ¸²æŸ“è¾“å…¥è¶…é‡‡æ ·è‡³1080pï¼Œ86fps</div></center> \n\nä½†å¦‚æžœæ”¾å¤§çœ‹çš„è¯ï¼Œå¯ä»¥å‘çŽ°DLSS2.0çš„ç»“æžœå’ŒåŽŸç”Ÿ1080pè¿˜æ˜¯æœ‰ä¸€äº›å·®åˆ«ï¼Œé‚£ä¹ˆä¸ºäº†éªŒè¯æ­£ç¡®æ€§ï¼Œä¸‹é¢è¿™ä¸ªå¯¹æ¯”çš„å·¦ä¸‹è§’æ˜¯æ¯ä¸ªåƒç´ ç”¨32ä¸ªæ ·æœ¬æ¸²æŸ“çš„ground truthã€‚å¾ˆæ˜Žæ˜¾DLSS 2.0ç”¨åœ¨540pä¸‹æ¸²æŸ“çš„ç»“æžœï¼Œæ¯”1080pçš„åŽŸç”Ÿæ¸²æŸ“æ›´æŽ¥è¿‘ground truthï¼\n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/b7e0d43a924e45fe91b6bbb6810a381c.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_17,color_FFFFFF,t_70,g_se,x_16\">\n    <br><div style=\"color: #999;\">å›¾ 30 200%å¯¹æ¯”</div></center> \n\n## 4.2 DLSS 2.0çš„ä¼˜ç¼ºç‚¹\n\n### 4.2.1  DLSS 2.0 å››å¤§ç‰¹æ€§\n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/a743d2987f5f431e84c11123fceaf66a.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_20,color_FFFFFF,t_70,g_se,x_16\">\n    <br><div style=\"color: #999;\">å›¾ 31 DLSS 2.0å››å¤§ç‰¹æ€§</div></center>\n\n1. ç”»è´¨æžå¤§æå‡ï¼Œç»†èŠ‚å’Œé”åº¦åª²ç¾Žã€ç”šè‡³è¶…è¶ŠåŽŸç”Ÿåˆ†è¾¨çŽ‡\n2. 4å€åƒç´ è¶…é‡‡æ ·ï¼ˆ540påˆ°1080pï¼Œ1080påˆ°4Kï¼Œæ¯4ä¸ªåƒç´ ä¸­æœ‰3ä¸ªæ˜¯é€šè¿‡è¶…é‡‡æ ·ç”Ÿæˆï¼‰\n3. é€šç”¨æ¨¡åž‹ï¼Œä¸€ä¸ªç¥žç»ç½‘ç»œé€‚ç”¨äºŽæ‰€æœ‰æ¸¸æˆï¼ˆä¸åŒå¼•æ“Žï¼Œç€è‰²é£Žæ ¼ï¼Œåˆ†è¾¨çŽ‡éƒ½æœ‰å¾ˆå¼ºçš„é€šç”¨æ€§ï¼‰\n4. Inferenceå¼€é”€å‡åŠ\n\n### 4.2.2 DLSSç¼ºç‚¹\n\n1. DLSSä¼¼ä¹Žä¸èƒ½å¾ˆå¥½åœ°ä¸ŽæŸäº›AAæŠ€æœ¯(å¦‚TSAA)ä¸€èµ·å·¥ä½œï¼Œå½“å¯ç”¨è¿™äº›æŠ€æœ¯æ—¶ï¼ŒDLSSæ€§èƒ½ä¼šå—åˆ°ä¸¥é‡å½±å“ã€‚\n2. æ­¤å¤–ï¼Œç”±äºŽDLSSåªèƒ½å·¥ä½œåœ¨å¼ é‡æ ¸çš„GPUä¸Šï¼Œæ‰€ä»¥CUDA-onlyå’ŒStreamå¤„ç†å™¨çš„gpuä¸èƒ½å®žçŽ°DLSSã€‚\n\n## 4.2.3 DLSSæŠ€æœ¯çš„å±•æœ›\n\n1. **æ›´ç®€å•çš„ SR ç½‘ç»œæž¶æž„**\n\nè™½ç„¶ DLSR æ¨¡åž‹åœ¨å›¾åƒé‡å»ºæ–¹é¢å–å¾—äº†å¾ˆé«˜çš„å‡†ç¡®çŽ‡å’Œä¿çœŸåº¦ï¼Œä½†åœ¨æœ¬åœ°éƒ¨ç½²ä»ç„¶æ˜¯æžå…¶å›°éš¾ä¸”è€—æ—¶çš„ã€‚ç”±äºŽæ‹¥æœ‰å¤§é‡çš„è®¡ç®—æˆæœ¬å’Œç½‘ç»œè®­ç»ƒç›¸å…³å‚æ•°ï¼ŒDLSRéœ€è¦å¤§åž‹æ•°æ®ä¸­å¿ƒæˆ–è¶…çº§è®¡ç®—æœºã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œéœ€è¦é™ä½Žç©ºé—´å’Œæ—¶é—´å¤æ‚åº¦ï¼Œé™ä½Žè®¡ç®—æˆæœ¬ï¼Œå‡å°‘å‚æ•°æ•°é‡ï¼Œè¿˜éœ€è¦å°†å›¾åƒè´¨é‡ä¿æŒåœ¨å¯æŽ¥å—èŒƒå›´å†…ã€‚\n\n2. **æ›´æœ‰æ•ˆçš„ç®—æ³•æ¥è¡¥å¿ä¿¡æ¯ä¸¢å¤±**\n\nDLSR æ¨¡åž‹çš„ä¸»è¦å·¥ä½œåŽŸç†æ˜¯ä»Žä½Žåˆ†è¾¨çŽ‡è¾“å…¥é‡å»ºé«˜åˆ†è¾¨çŽ‡å›¾åƒã€‚ä½†æ˜¯ï¼Œä»¥éžå¸¸ä½Žçš„å†…éƒ¨åˆ†è¾¨çŽ‡ï¼ˆä¾‹å¦‚ 540pï¼‰è¿›è¡Œæ¸²æŸ“ï¼Œæˆ–è€…è¿è¡Œéžå¸¸é«˜çš„æ”¾å¤§æ“ä½œæ—¶ï¼ˆä¾‹å¦‚ä»Ž 1080p ç¼©æ”¾åˆ° 8Kï¼‰ï¼Œå¯¹äºŽé‡å»ºè€Œè¨€ï¼Œæ­¤æ—¶çš„ç¼ºå¤±æ•°æ®é‡å˜å¾—å¤ªå¤§ã€‚è¿™é€šå¸¸ä¼šå¯¼è‡´é”™è¯¯è¡¨è¾¾æˆ–ä¸å‡†ç¡®çš„è§†è§‰æ•°æ®ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œéœ€è¦åŠªåŠ›ä½¿é¢„æµ‹ç®—æ³•ä»Žæ›´å°‘çš„åƒç´ ä¸­æ›´æœ‰æ•ˆåœ°æå–è§†è§‰ä¿¡æ¯ã€‚\n\n3. **ä¸“æ³¨äºŽæ›´å¹¿æ³›çš„å®žæ–½å’Œæ”¯æŒ**\n\nDLSR æ˜¯ä¸€é¡¹éžå¸¸æ–°çš„æŠ€æœ¯ï¼Œä»å¤„äºŽèµ·æ­¥é˜¶æ®µã€‚å› æ­¤ï¼Œç›®å‰åªæœ‰å°‘æ•°åº”ç”¨ç¨‹åºæä¾›å¯¹ DLSR çš„å®žçŽ°æ”¯æŒã€‚éœ€è¦å®Œæˆå·¥ä½œå¹¶éœ€è¦æž„å»ºç›¸å…³çš„ API ä»¥æ‰©å±•å¯¹æ›´å¤šåº”ç”¨ç¨‹åºçš„æ”¯æŒï¼Œå¹¶ä¸”å¿…é¡»ä¸ºå¼€å‘äººå‘˜åˆ›å»ºè½¯ä»¶å·¥å…·ä»¥å®žçŽ°æ›´å¿«çš„å¢žé•¿ã€‚\n\n# **Postscriptï¼šæ€»ç»“**\n\nè°ƒæŸ¥å’Œæ”¥å†™æŠ¥å‘Šå‰å‰åŽåŽèŠ±äº†ä¸€å‘¨æ—¶é—´ã€‚å‰æœŸä¹Ÿæ˜¯éšå¿ƒæ‰€æ¬²çš„æŸ¥æ–‡çŒ®çœ‹èµ„æ–™å†™mdï¼ŒåŽæœŸä¸‰å¤©ç‹‚è‚å‡ºå°20é¡µçš„æŠ¥å‘Šè¿˜æ˜¯ç›¸å½“ç—›è‹¦çš„ä¸€ä»¶äº‹â€¦â€¦ç¡®å®žæ—¶é—´å®‰æŽ’ç›¸å½“ä¸åˆç†ï¼ˆæ‹–å»¶ç—‡ä½ åˆæ¥å•¦ï¼‰ï¼Œä½†æ›´å¤šçš„æ—¶é—´æˆ‘è®¤ä¸ºæ˜¯èŠ±è´¹åœ¨æ‹æ€è·¯ä¸Šã€‚åœ¨æŸ¥é˜…èµ„æ–™è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä¸€ç›´åœ¨å°è¯•æ‘¸æ¸…å†…åœ¨çš„é€»è¾‘é“¾â€”â€”å¯æƒœæ€è·¯ä¸€ç›´è¢«æŽ¨ç¿»é‡å»ºï¼Œæ–‡ç« ç»“æž„ä¹Ÿä¸€æ”¹å†æ”¹ï¼Œç›´åˆ°ddlå‰çš„æœ€åŽä¸€å¤©çš„æˆ‘åŸºæœ¬æ»¡æ„ã€‚\n\næ€»ä½“æ¥è¯´ï¼Œæ”¶èŽ·è¿˜æ˜¯ç›¸å½“å¤§çš„ï¼Œè°ƒç ”è°ƒæŸ¥æœ¬èº«å°±æ˜¯ä¸€ä»¶ä»¤äººå…´å¥‹çš„äº‹â€”â€”å¯ä»¥è‡ªå·±æ€è€ƒå†…åœ¨è”ç³»ï¼Œæå‡ºé—®é¢˜ï¼Œå†åŽ»è§£å†³â€¦â€¦è¿™ä¹Ÿæ˜¯æˆ‘ç¬¬ä¸€æ¬¡çœ‹è¿™ä¹ˆå¤šçš„å¤–æ–‡ææ–™ï¼ˆåŽŸå…ˆå¯¹è‹±æ–‡æ–‡çŒ®æœ‰äº›ç•æƒ§å¿ƒç†ï¼Œä¸€ç›´æ²¡æœ‰è¿ˆå‡ºè¿™ä¸€æ­¥ï¼Œæˆ–è€…è¯´æ²¡è¿ˆå‡ æ­¥ï¼‰ã€‚\n\næˆ‘æœ€æ„Ÿå…´è¶£æˆ–è€…æœ€æƒ³ä»Žäº‹çš„æ–¹å‘ä¹‹ä¸€å°±æ˜¯æ¸¸æˆå¼€å‘ï¼ˆä¸€ä¸ªçˆ±çŽ©æ¸¸æˆçš„äººä¹Ÿæƒ³åŽ»åšæ¸¸æˆï¼‰ã€‚åŽŸå…ˆæˆ‘å¤šå¤šå°‘å°‘ä½Žä¼°äº†è¿™ä¸ªé¢†åŸŸçš„éš¾åº¦ï¼Œä½†çŽ°åœ¨æ›´å¤šåœ°æˆ‘å¯¹è¿™ä¸ªæ–¹å‘äº§ç”Ÿäº†æ•¬ç•ä¹‹æƒ…ã€‚å½“ç„¶æ‰“åŠ¨æˆ‘çš„è¿˜æœ‰å¾ˆå¤šè¿™ä¸ªé¢†åŸŸçš„å‰è¾ˆä»¬ã€‚å½“æˆ‘ç ”ç©¶ã€ŠReal time renderingã€‹ï¼Œç¿»çœ‹æ¯›æ˜Ÿäº‘çš„æ€»ç»“æç‚¼æ—¶ï¼Œæˆ‘ä½©æœç€ä»–çš„çƒ­çˆ±ï¼Œä¹Ÿæƒ‹æƒœç€ä»–çš„ç¦»å¼€ï¼›å½“æˆ‘æµè§ˆæ–‡åˆ€ç§‹äºŒçš„[çŸ¥ä¹Žå›žç­”](https://www.zhihu.com/question/29504480/answer/44764493)æ—¶ï¼Œæˆ‘ä¹Ÿæ˜¯æ·±æ·±ä½©æœçš„â€”â€”æˆ‘è®¤ä¸ºæˆ‘å¯¹codingæ˜¯å¾ˆæ„Ÿå…´è¶£çš„ï¼Œä»¥è‡³äºŽæ¯æ¬¡è¯¾è®¾å’Œé¡¹ç›®æˆ‘å¾ˆå¤šæ—¶é—´éƒ½èŠ±åœ¨å­¦ä¹ æ–°ä¸œè¥¿åŠ ä¸ŠåŽ»ï¼Œä»¥åšå¾—å¥½äº›ï¼Œæ›´å¥½äº›ã€‚ä½†å½“æˆ‘å‘çŽ°å¤§ç¥žçš„\"å…´è¶£\"åŽï¼Œæˆ‘ä¹Ÿæ„è¯†åˆ°ï¼Œæˆ‘è¿˜æœ‰å¾ˆé•¿çš„ä¸€æ®µè·¯è¦èµ°ï¼š\n\n <center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https://img-blog.csdnimg.cn/41c45a32be944723964575606a6fb82d.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6Zi_6bKr5piv5p2h5ZK46bG8,size_20,color_FFFFFF,t_70,g_se,x_16\"\n    width=\"50%\">\n</center>\n\næœ€åŽå†æ¥è°ˆè°ˆè¿™ç¯‡æ–‡ç« ï¼Œä¸ªäººæ„Ÿè§‰æ›´å¤šçš„è¿˜æ˜¯åœ¨å­¦ä¹ æ‘˜æŠ„å€Ÿé‰´ã€‚å†™ç»ªè®ºçš„æ—¶å€™è¿˜åœ¨æ‰‹æ•²ï¼Œé™„å¼•ç”¨ï¼›å†™æŠ—é”¯é½¿éƒ¨åˆ†çš„æ—¶å€™è¿˜èƒ½æ•´åˆå¤šä»½èµ„æ–™ï¼Œç¿»è¯‘ç½‘ç«™æ–‡çŒ®ï¼Œå†åšæ ¡å¯¹ï¼›ç­‰å†™åˆ°DLSSæ—¶å°±å¼€å§‹å¤åˆ¶ç²˜è´´äº†â€¦â€¦æ€»ä½“è€Œè¨€ï¼Œæ›´åƒæ˜¯å¤šå®¶èµ„æ–™çš„å¤§åˆé›†ï¼Œå‹‰å‹‰å¼ºå¼ºä¸€ä¸ªä¼˜ç‚¹å°±æ˜¯æœ‰è‡ªå·±çš„æ€è€ƒåœ¨å†…ã€‚å½“ç„¶ï¼Œå¦‚æœ‰é”™è¯¯å’Œé—®é¢˜ï¼Œæ¬¢è¿ŽæŽ¢è®¨ã€‚é“é˜»ä¸”é•¿ï¼Œè¿™æ˜¯ä¸€æ¬¡æŠŠæŠ¥å‘Šæ•´ç†è‡³CSDNä¸Šï¼Œç›¸ä¿¡ä¸ä¼šæ˜¯æœ€åŽä¸€æ¬¡ï¼\n","tags":["DL","CG"],"categories":["Other"]}]