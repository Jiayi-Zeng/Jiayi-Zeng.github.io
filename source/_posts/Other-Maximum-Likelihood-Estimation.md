---
title: "Parameter Estimation: Maximum Likelihood Estimation"

date: 2023-03-09 10:02:11

tags: [ML]

categories: Other

cover_image: https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/202303100212.png
---

我们以一个赌博的例子来模拟机器学习的概率推理过程。假如你参与了一场赌博，你会被告知一个硬币抛掷10次的正反情况，接下来由你下注，而你只有一次机会，猜对硬币下一次正反情况则赢得100元，猜错则损失100元。这时，你会如何决策？

# **1 概率和似然**

一般地，硬币有正反两面，如果硬币正反两面是均匀的，即每次抛掷后硬币为正的概率是0.5。使用这个硬币，很可能抛10次，有5次是正面。但是假如有人对硬币做了手脚，比如提前对硬币做了修改，硬币每次都会正面朝上，现在抛10次，10次都是正面，那么下次你绝对不会猜它是反面，因为前面的10次结果摆在那里，直觉上你不会相信这是一个普通的硬币。现在有一人抛了10次硬币，得到6正4反的结果，如何估算下次硬币为正的概率呢？

因为硬币并不是我们制作的，我们不了解硬币是否是完全均匀的，只能根据现在的观察结果来反推硬币的情况。假设硬币上有个参数 $\theta$，它决定了硬币的正反均匀程度，$\theta=0.5$表示正反均匀，每次抛硬币为正的概率为 0.5，$\theta=1$表示硬币只有正面，每次抛硬币为正的概率为1。那么，从观察到的正反结果，反推硬币的构造参数 $\theta$ 的过程，就是一个参数估计的过程。

## 1.1 概率

抛掷10次硬币可能出现不同的情况，可以是“5正5反”、“4正6反”，“10正0反”等。设“6正4反为事件A”, 假如我们知道硬币是如何构造的，即已知硬币的参数 $\theta$，那么出现事件A的概率为：

$$
P(A|\theta=0.5)=C_{10}^6 \times 0.5^6 \times (1-0.5)^4 \approx 0.2051 \\
P(A|\theta=0.6)=C_{10}^6 \times 0.6^6 \times (1-0.6)^4 \approx 0.2508 \\
P(A|\theta=0.9)=C_{10}^6 \times 0.9^6 \times (1-0.9)^4 \approx 0.0112 \\
$$

概率反映的是：**已知背后原因，推测某个结果发生的概率**。

## 1.2 似然

与概率不同，似然反映的是：**已知结果，反推原因**。具体而言，似然（Likelihood）函数表示的是基于观察的数据，取不同的参数 $\theta$ 时，统计模型以多大的可能性接近真实观察数据。这就很像开篇提到的赌局，已经给你了一系列硬币正反情况，但你并不知道硬币的构造，下次下注时你要根据已有事实，反推硬币的构造。例如，当观察到硬币“10正0反”的事实，猜测硬币极有可能每次都是正面；当观察到硬币“6正4反”的事实，猜测硬币有可能不是正反均匀的，每次出现正面的可能性是0.6。

似然函数与前面的概率函数的计算方式极其相似，与概率函数不同的是，似然函数是 的函数，即 $\theta$ 是未知的。似然函数衡量的是在不同参数 $\theta$ 下，真实观察数据发生的可能性。似然函数通常是多个观测数据发生的概率的联合概率，即多个观测数据都发生的概率。在机器学习里可以这样理解，目标 $y$ 和特征 $x$ 时发生，这些数值被观测到的概率。单个观测数据发生的可能性为$P(\theta)$，如果各个观测之间是相互独立的，那么多个观测数据都发生的概率可表示为**各个样本发生的概率的乘积**。

似然函数通常用 $\theta$ 表示，对应英文Likelihood。观察到抛硬币“6正4反”的事实，硬币参数 $\theta$ 取不同值时，似然函数表示为：

$$
L(\theta;A)=C_{10}^6 \times \theta^6 \times(1-\theta)^4
$$

这个公式的图形如下图所示。从图中可以看出：参数 $\theta$ 为0.6时，似然函数最大，参数为其他值时，“6正4反”发生的概率都相对更小。在这个赌局中，我会猜测下次硬币为正，因为根据已有观察，硬币很可能以0.6的概率为正。

![img](https://raw.githubusercontent.com/Jiayi-Zeng/Jiayi-Zeng.github.io/pic/img/v2-cd59525e2e226638741ff8b97139e912_1440w.webp)

推广到更为一般的场景，似然函数的一般形式可以用下面公式来表示，也就是之前提到的，各个样本发生的概率的乘积。

$$
L(\theta; \mathbf{X})=P_1(\theta; \mathbf{X}_1)\times P_2(\theta; \mathbf{X}_2)\times ... \times P_n(\theta; \mathbf{X}_n) = \prod P_i(\theta; \mathbf{X}_i)
$$

# **2 最大似然估计**

理解了似然函数的含义，就很容易理解最大似然估计的机制。似然函数是关于模型参数的函数，是描述观察到的真实数据在不同参数下发生的概率。最大似然估计要寻找最优参数，让似然函数最大化。或者说，使用最优参数时观测数据发生的概率最大。

## 2.1 线性回归的最大似然估计

线性回归的误差项ε是预测值与真实值之间的差异，如下面公式所示。它可能是一些随机噪音，也可能是线性回归模型没考虑到的一些其他影响因素。

$$
y_i= \epsilon_i + \sum_{j=1}^nw_jx_i,j = \epsilon_i + \mathbf{W}^T\mathbf{x_i}
$$

线性回归的一大假设是：误差服从均值为0的正态分布，且多个观测数据之间互不影响，相互独立。根据正态分布的公式，可以得到 $\epsilon$ 的概率密度。假设 $x$ 服从正态分布，它的均值为 $\mu$ ，方差为 $\sigma$ ，它的概率密度公式如下。公式左侧的 $P(x;\mu,\sigma)$ 表示 $x$ 是随机变量，$;$ 分号强调 $\mu$ 和 $\sigma$ 不是随机变量，而是这个概率密度函数的参数。

$$
P(x;\mu,\sigma)=\frac{1}{\sqrt{2\pi\sigma^2}}exp({-\frac{(x-\mu)^2}{2\sigma^2}})
$$

既然误差项服从正态分布，那么：

$$
P(\epsilon_i)=\frac{1}{\sqrt{2\pi\sigma^2}}exp({-\frac{\epsilon_i^2}{2\sigma^2}})
$$

由于 $\epsilon_i=y_i-\mathbf{w}^T\mathbf{x_i}$, 并取均值 $\mu$ 为0，可得到：

$$
P(y_i|\mathbf{x_i};\mathbf{w})=\frac{1}{\sqrt{2\pi\sigma^2}}exp({-\frac{(y_i-\mathbf{w}^T\mathbf{x_i})^2}{2\sigma^2}})
$$

前文提到，似然函数是所观察到的各个样本发生的概率的乘积。一组样本有 $m$ 个观测数据，其中单个观测数据发生的概率为刚刚得到的公式，$m$ 个观测数据的乘积如下所示。

$$
L(\mathbf{w})=L(\mathbf{w};\mathbf{X},\mathbf{y})= \prod _{i=1}^NPr(y_i|\mathbf{x_1};\mathbf{w})= \prod_{i=1}^m \frac{1}{\sqrt{2\pi\sigma^2}}exp({-\frac{(y_i-\mathbf{w}^T\mathbf{x_i})^2}{2\sigma^2}})
$$

其中，$\mathbf{x_i}$ 和 $y_i$都是观测到的真实数据，是已知的，$\mathbf{w}$ 是需要去求解的模型参数。

给定一组观测数据 $\mathbf{X}$ 和 $\mathbf{y}$ ，如何选择参数 $\mathbf{w}$ 来使模型达到最优的效果？最大似然估计法告诉我们应该选择一个 $\mathbf{w}$，使得似然函数 $L$ 最大。$L$ 中的乘积符号和 $exp$ 运算看起来就非常复杂，直接用 $L$ 来计算十分不太方便，于是统计学家在原来的似然函数基础上，取 $log$ 对数。$log$的一些性质能大大化简计算复杂程度，且对原来的似然函数增加 $log$ 并不影响参数 $\mathbf{w}$ 的最优值。通常使用花体的 $\mathscr{l}$ 来表示损失函数的对数似然函数。

$$
\begin{align}
\mathscr{l}(\mathbf{w}) &= log L(\mathbf{w})\\
&= log \prod_{i=1}^m \frac{1}{\sqrt{2\pi\sigma^2}}exp({-\frac{(y_i-\mathbf{w}^T\mathbf{x_i})^2}{2\sigma^2}})\\
&= \sum_{i=1}^mlog[\frac{1}{\sqrt{2\pi\sigma^2}}exp({-\frac{(y_i-\mathbf{w}^T\mathbf{x_i})^2}{2\sigma^2}})]\\
&= \sum_{i=1}^mlog[\frac{1}{\sqrt{2\pi\sigma^2}}]+\sum_{i=1}^mlog[exp({-\frac{(y_i-\mathbf{w}^T\mathbf{x_i})^2}{2\sigma^2}})]\\
=& mlog\frac{1}{\sqrt{2\pi\sigma^2}}-\frac{1}{2\sigma^2}\sum_{i=1}^m(y_i-\mathbf{w}^T\mathbf{x_i})^2
\end{align}
$$

由于我们只关心参数 $\mathbf{w}$ 取何值时，似然函数最大，标准差 $\sigma$ 并不会影响 $\mathbf{w}$ 取何值时似然函数最大，所以可以忽略掉带有标准差 $\sigma$ 的项 $mlog\frac{1}{\sqrt{2\pi\sigma^2}}$。再在 $-\frac{1}{2\sigma^2}\sum_{i=1}^m(y_i-\mathbf{w}^T\mathbf{x_i})^2$ 加个负号，负负得正，原来似然函数ℓ最大化问题就变成了最小化问题，其实最后还是最小化：

$$
\sum_{i=1}^m(y_i-\mathbf{w}^T\mathbf{x_i})^2
$$
这与最小二乘法所优化的损失函数几乎一样，都是“真实值 - 预测值”的平方和，可以说是殊途同归。

接下来对公式参数求解，可以求导方法，让导数为0，得到一个矩阵方程，矩阵方程的解即为模型的最优解；也可以使用梯度下降法，求模型的最优解。

### 最小二乘与最大似然

前面的推导中发现，最小二乘与最大似然的公式几乎一样。直观上来说，最小二乘法是在寻找观测数据与回归超平面之间的误差距离最小的参数。最大似然估计是最大化观测数据发生的概率。当我们假设误差是正态分布的，所有误差项越接近均值0，概率越大。正态分布是在均值两侧对称的，误差项接近均值的过程等同于距离最小化的过程。

# **Reference** 

https://zhuanlan.zhihu.com/p/143416436

https://www.jiqizhixin.com/articles/2018-07-12-5